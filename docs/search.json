[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Experienced Supply Chain Manager\nData Scientist\nSamsung Electronics (Jul 2019 ~ )\nMS in Data Science\nBS in Industrial Engineering\nBA in Sports Industry Management"
  },
  {
    "objectID": "about.html#jinwook-chang-張鎭旭",
    "href": "about.html#jinwook-chang-張鎭旭",
    "title": "About",
    "section": "",
    "text": "Experienced Supply Chain Manager\nData Scientist\nSamsung Electronics (Jul 2019 ~ )\nMS in Data Science\nBS in Industrial Engineering\nBA in Sports Industry Management"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Note of myself",
    "section": "",
    "text": "Summarise using GPT\n\n\n\nAI\n\n\nGPT\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIterative Prompt Development\n\n\n\nAI\n\n\nGPT\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Network\n\n\n\nDataScience\n\n\nStatistics\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPrinciples of Prompting 2\n\n\n\nAI\n\n\nGPT\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPrinciples of Prompting 1\n\n\n\nAI\n\n\nGPT\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRegularization (정규화 기법)\n\n\n\nDataScience\n\n\nStatistics\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nKullback-Leibler Divergence 와 Chi-square 검정\n\n\n\nDataScience\n\n\nStatistics\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2023 MIT 6.S191 04 Diagnosing Bias\n\n\n\nDataScience\n\n\nDeepLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nKullback-Leibler Divergence 쉽게 이해하기\n\n\n\nDataScience\n\n\nMath\n\n\nStatistics\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2023 MIT 6.S191 03 MNIST\n\n\n\nDataScience\n\n\nDeepLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2023 MIT 6.S191 02 Music Generation with RNNs\n\n\n\nDataScience\n\n\nDeepLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2023 MIT 6.S191 01 Intro to TensorFlow\n\n\n\nDataScience\n\n\nDeepLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2023 MIT 6.S191 Deep Learning\n\n\n\nDataScience\n\n\nDeepLearning\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n켈리 기준\n\n\n\nDataScience\n\n\nScript\n\n\nOptimization\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n딥러닝을 위한 Multi variables Linear Regression\n\n\n\nDataScience\n\n\nRegression\n\n\nDeepLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n딥러닝을 위한 Simple Linear Regression\n\n\n\nDataScience\n\n\nRegression\n\n\nDeepLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n배낭 문제 (여러 배낭)\n\n\n\nDataScience\n\n\nOptimization\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n배낭 문제\n\n\n\nDataScience\n\n\nOptimization\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n생산계획\n\n\n\nDataScience\n\n\nOptimization\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1종 오류 2종 오류 헷갈리지 않기\n\n\n\nDataScience\n\n\nStatistics\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGPT 템플릿\n\n\n\nAI\n\n\nGPT\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEntropy 쉽게 이해하기\n\n\n\nDataScience\n\n\nMath\n\n\nStatistics\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n상호 정보량 (Mutual Informaion)\n\n\n\nDataScience\n\n\nMetric\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embedding과 Word2Vec\n\n\n\nDataScience\n\n\nNLP\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n이원수와 전진자동미분\n\n\n\nDataScience\n\n\nOptimization\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n인과관계의 조건과 탐색\n\n\n\nDataScience\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDBSCAN\n\n\n\nDataScience\n\n\nClustering\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Boosting\n\n\n\nDataScience\n\n\nEnsembleLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest\n\n\n\nDataScience\n\n\nEnsembleLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEnsemble Learning\n\n\n\nDataScience\n\n\nMainTheme\n\n\nEnsembleLearning\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nK-means clustering\n\n\n\nDataScience\n\n\nClustering\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nt-sne\n\n\n\nDataScience\n\n\nDimensionReduction\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUMAP\n\n\n\nDataScience\n\n\nDimensionReduction\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\nMainTheme\n\n\nDataScience\n\n\nClustering\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDimension Reduction (차원 축소)\n\n\n\nMainTheme\n\n\nDataScience\n\n\nDimensionReduction\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nN-1 더미 변수를 활용하는 이유\n\n\n\nDataScience\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCovariance Matrix\n\n\n\nDataScience\n\n\nMath\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHierarchical Clustering\n\n\n\nDataScience\n\n\nClustering\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis\n\n\n\nDataScience\n\n\nDimensionReduction\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAlpha max plus Beta min (Fast 2D Eculedian Distance)\n\n\n\nDataScience\n\n\nOptimization\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBenford’s Law\n\n\n\nDataScience\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nZipf’s Law (단어 사용의 분포)\n\n\n\nDataScience\n\n\nNLP\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 9, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/alpha-max-plus-beta-min/index.html",
    "href": "posts/alpha-max-plus-beta-min/index.html",
    "title": "Alpha max plus Beta min (Fast 2D Eculedian Distance)",
    "section": "",
    "text": "Alpha max plus Beta min 알고리즘은 두 제곱의 합의 제곱근을 고속으로 근사하는 방법입니다. 이 계산은 직각 삼각형의 빗변 길이를 구하거나, 2차원 벡터의 놈, 또는 실수와 허수부를 가진 복소수 z = a + bi의 크기를 구하는 데 사용되는 유용한 기능입니다.\n이 알고리즘은 계산 비용이 높은 제곱 및 제곱근 연산을 수행하는 대신 비교, 곱셈 및 덧셈과 같은 간단한 연산을 사용합니다. α와 β 매개변수를 적절하게 선택함으로써 곱셈 연산을 이진 자릿수의 단순한 시프트로 줄일 수 있습니다. 이는 고속 디지털 회로 구현에 특히 적합합니다.\n알파 맥스 플러스 베타 민 알고리즘은 다음과 같이 표현할 수 있습니다:\n\\[\n\\sqrt{A^2 + B^2} ≈ α \\cdot max(A, B) + β \\cdot min(A, B)\n\\]\n여기서 A와 B는 주어진 두 수이고, max(A, B)는 A와 B 중 큰 값을 나타내며, min(A, B)는 작은 값을 나타냅니다. α와 β는 근사치를 최적화하는 선택된 매개변수입니다.\n이 알고리즘은 두 제곱의 합의 제곱근을 직접 계산하는 것에 비해 빠르고 계산적으로 효율적인 대안을 제공하면서도 합리적인 정확도를 유지합니다.\n\n\n\nUntitled"
  },
  {
    "objectID": "posts/alpha-max-plus-beta-min/index.html#benchmark-of-alpha-max-plus-beta-min",
    "href": "posts/alpha-max-plus-beta-min/index.html#benchmark-of-alpha-max-plus-beta-min",
    "title": "Alpha max plus Beta min (Fast 2D Eculedian Distance)",
    "section": "Benchmark of Alpha max plus Beta min",
    "text": "Benchmark of Alpha max plus Beta min\n# 기본 제곱근 계산 함수\nsqrt_sum_of_squares &lt;- function(a, b) {\n  return(sqrt(a^2 + b^2))\n}\n\n# 알파 맥스 플러스 베타 민 근사 함수\nalpha_max_plus_beta_min &lt;- function(a, b, alpha, beta) {\n  return(alpha * max(a, b) + beta * min(a, b))\n}\n\n# 테스트 케이스 생성\nnum_tests &lt;- 10000\ntest_cases &lt;- data.frame(a = runif(num_tests), b = runif(num_tests))\n\n# 파라미터 설정\nalpha &lt;- 0.960\nbeta &lt;- 0.397\n\n# 기본 제곱근 계산 시간 측정\nstart_time &lt;- Sys.time()\nfor (i in 1:num_tests) {\n  sqrt_sum_of_squares(test_cases$a[i], test_cases$b[i])\n}\nend_time &lt;- Sys.time()\nsqrt_time &lt;- end_time - start_time\n\n# 알파 맥스 플러스 베타 민 계산 시간 측정\nstart_time &lt;- Sys.time()\nfor (i in 1:num_tests) {\n  alpha_max_plus_beta_min(test_cases$a[i], test_cases$b[i], alpha, beta)\n}\nend_time &lt;- Sys.time()\napprox_time &lt;- end_time - start_time\n\n# 결과 출력\ncat(\"기본 제곱근 계산 시간: \", sqrt_time, \"\\n\")\ncat(\"알파 맥스 플러스 베타 민 근사 시간: \", approx_time, \"\\n\")\n\n## 기본 제곱근 계산 시간:  0.09882092\n## 알파 맥스 플러스 베타 민 근사 시간:  0.03761482"
  },
  {
    "objectID": "posts/bayesian-network/index.html",
    "href": "posts/bayesian-network/index.html",
    "title": "Bayesian Network",
    "section": "",
    "text": "베이지언 네트워크란…\n베이지언 네트워크는 확률적 그래픽 모델의 한 종류로, 변수 사이의 조건부 확률 관계를 그래픽으로 나타낸 것입니다. 이 모델은 주로 불확실성이 있는 상황에서 의사 결정을 지원하거나, 복잡한 데이터셋에서 의미 있는 패턴을 발견하는 데 사용됩니다.\n베이지언 네트워크는 방향성 비순환 그래프(Directed Acyclic Graph, DAG)로 표현되며, 그래프의 노드는 확률 변수를, 간선은 확률 변수들 사이의 확률적 의존성을 나타냅니다. 그래프의 간선이 화살표로 표시되는 이유는 한 변수에서 다른 변수로의 ’방향성’을 나타내기 위해서입니다.\n베이지언 네트워크의 핵심 아이디어는 조건부 독립성을 이용하는 것입니다. 즉, 어떤 변수가 주어졌을 때 다른 변수들 사이의 독립성을 모델링합니다. 이러한 독립성 가정은 모델의 복잡성을 크게 줄여주며, 그 결과로 효과적인 추론과 학습이 가능해집니다.\n이러한 특성 때문에 베이지언 네트워크는 다양한 분야에서 활용됩니다. 예를 들어, 의료 분야에서는 질병의 원인을 파악하거나 증상으로부터 진단을 내리는 데 사용되며, 인공지능 분야에서는 로봇의 추론 시스템이나 자연어 처리 등에서 사용됩니다.\n\n위의 예시와 같이 각 노드들에 대해서 Parents 노드가 Given 되었을 때 Child 노드의 확률과 같이 표현됩니다.\n조금 더 구체적인 사례로 베이지언 네트워크의 구조에 대해 알아봅시다.\n\n\n베이지언 네트워크의 구조\n\n다음과 같은 베이지언 네트워크를 고려해봅시다. 알람은 도둑/지진에 영향을 받고, 존과 메리의 전화는 알람에 받는 베이지언 네트워크 구조입니다. 간단하게 보이는 네트워크이지만, 이 네트워크는 베이지언 네트워크의 모든 기본 구조를 가지고 있습니다.\n베이지언 네트워크의 기본 구조는 다음과 같습니다.\n\nCommon Parent : 존/메리의 전화가 알람의 영향을 받는 부분입니다. 알람이 given일 때, 존과 메리의 전화는 독립입니다.\nCascading : 도둑 -&gt; 알람 -&gt; 존의 전화로 연결되는 부분입니다. 알람이 given일 때, 도둑과 존의 전화는 독립입니다.\nV-Structure: 도둑/지진과 알람의 관계입니다. 알람이 given일 때, 도둑과 지진은 의존적입니다.\n\n좀 더 복잡한 문제로 독립/의존을 판별해 봅시다.\n\n위의 사례에서, X2가 주어질 때, X1과 X4는 독립입니다. X1이 주어질 때, X2와 X5는 독립입니다.\nMarkov Blanket은 특정 노드가 독립일 조건을 만족시키는 노드의 집합입니다.\n가령, A 노드가 독립이기 위해서는 A노드의 부모, 자식, 자식들의 다른 부모들이 Markov Blanket에 해당됩니다.\n부모의 경우 Common parent 구조에 의해, 자식의 경우 Cascading 구조에 의해, 자식들의 다른 부모들의 경우, V-structure에 의해 이를 만족하는 것을 확인할 수 있습니다.\n\n\n베이지언 네트워크의 Factorization\n처음 베이지언 네트워크를 설명할 때, 각 노드들은 Parents 노드가 Given 되었을 때 Child 노드의 확률이라고 했습니다.\n일반적인 경우에서 \\(P(X1,X2,X3,X4,X5,X6,X7,X8)\\)을 구하려면 많은 parameter가 필요하나, 네트워크를 활용하여 아래와 같이 간략히 표현할 수 있습니다."
  },
  {
    "objectID": "posts/benfords-law/index.html",
    "href": "posts/benfords-law/index.html",
    "title": "Benford’s Law",
    "section": "",
    "text": "벤포드의 법칙(Benford’s Law), 또는 첫 번째 숫자 법칙(First-Digit Law)은 다양한 실생활 데이터 집합에서 특정 숫자가 처음 자리에 나타날 확률에 관한 경험적 법칙입니다. 이 법칙은 1881년에 관찰되었으나, 물리학자 프랭크 벤포드(Frank Benford)가 1938년에 이 법칙을 더 널리 알렸기 때문에 그의 이름이 붙었습니다.\n벤포드의 법칙에 따르면, 첫 번째 숫자가 n인 데이터의 비율은 다음 공식으로 계산할 수 있습니다:\nP(n) = log10(n + 1) - log10(n) = log10(1 + 1/n)\n이 공식에 따르면, 첫 번째 숫자가 1일 확률은 약 30.1%, 2일 확률은 약 17.6%, 9일 확률은 약 4.6%입니다.\n벤포드의 법칙은 인구, 경제, 과학, 지리 등 다양한 분야의 데이터에 적용됩니다. 벤포드의 법칙은 데이터의 숫자 분포를 분석하는 데 도움이 되며, 이상치 탐지, 부정 행위 감지, 데이터 오류 확인 등에 활용됩니다.\n예를 들어 회계 분야에서 벤포드의 법칙을 이용해 부정 회계를 찾아낼 수 있습니다. 벤포드의 법칙에 따르면, 금융 데이터의 첫 자리 숫자 분포는 특정한 패턴을 따르는데, 이 패턴과 크게 벗어난 데이터가 발견되면 부정 행위의 가능성을 의심할 수 있습니다.\n[1 ≤ k ≤ 9 에서의 k의 분포 P_k %의 그래프]"
  },
  {
    "objectID": "posts/benfords-law/index.html#benfords-law-예제",
    "href": "posts/benfords-law/index.html#benfords-law-예제",
    "title": "Benford’s Law",
    "section": "Benford’s Law 예제",
    "text": "Benford’s Law 예제\n# Load required packages\nlibrary(gapminder)\nlibrary(tidyverse)\n\n# Load data from the gapminder package\ndata &lt;- gapminder\n\n# Filter the dataset to get the most recent population data for each country\nlatest_data &lt;- data |&gt; filter(year == max(year))\n\n# Extract the first digits from the population data\nfirst_digits &lt;- as.character(latest_data$pop) |&gt; str_sub(1,1) |&gt; as.integer()\n\n# Calculate the frequencies of the first digits\nobserved_freq &lt;- as.vector(table(first_digits) / length(first_digits))\n\n# Compute the expected frequencies according to Benford's Law\ndigits &lt;- 1:9\nbenford_freq &lt;- log10(1 + 1/digits)\n\n# Compare the observed and expected frequencies\ncomparison &lt;- tibble(\n  Digit = digits,\n  Observed_Frequency = observed_freq,\n  Benford_Frequency = benford_freq\n)\n\n# Reshape the data into a tidy format\ncomparison_tidy &lt;- comparison |&gt;\n  pivot_longer(cols = c(Observed_Frequency, Benford_Frequency), names_to = \"Type\", values_to = \"Frequency\")\n\n# Create a bar plot with ggplot2\nggplot(comparison_tidy, aes(x = factor(Digit), y = Frequency, fill = Type)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Comparing Observed and Benford's Law Frequencies\",\n       x = \"First Digit\",\n       y = \"Frequency\",\n       fill = \"Frequency Type\") +\n  theme_minimal()\n\n# Perform a chi-squared goodness-of-fit test\nobserved_counts &lt;- table(first_digits)\nchisq_test &lt;- chisq.test(observed_counts, p = benford_freq)\n\n# Print the results of the chi-squared test\ncat(\"Chi-squared test statistic:\", chisq_test$statistic, \"\\n\")\ncat(\"Degrees of freedom:\", chisq_test$parameter, \"\\n\")\ncat(\"p-value:\", chisq_test$p.value, \"\\n\")\n\n# Check if the observed data follows Benford's Law (using a significance level of 0.05)\nif (chisq_test$p.value &gt; 0.05) {\n  cat(\"The data follows Benford's Law.\\n\")\n} else {\n  cat(\"The data does not follow Benford's Law.\\n\")\n}\n\n\n\nfigure_1.png\n\n\nChi-squared test for given probabilities\n\ndata:  observed_counts\nX-squared = 4.9697, df = 8, p-value = 0.7608\n\n# The data follows Benford's Law."
  },
  {
    "objectID": "posts/bias-diagnose/index.html",
    "href": "posts/bias-diagnose/index.html",
    "title": "2023 MIT 6.S191 04 Diagnosing Bias",
    "section": "",
    "text": "2023년 MIT에서 강의된 6.S191 강좌의 Lab의이해를 돕기위해 작성되었습니다.\nLab에 대해서는 이곳에서 확인가능하며, 해당 강좌는 여기서 확인가능합니다.\n\nLab1-Part1 Intro to Tensorflow\nLab1-Part2 Music Generation\nLab2-Part1 MNIST\nLab2-Part2 Diagnosing Bias\nLab3-Part1 Introduction Caspa\nLab3-Part2 Bias and Uncertainty\n\n이번 과제에서는 Facial Detection System을 구성하고 이 모델에 Bias가 어떻게 구성되어있는지 확인해보겠습니다.\n# Import Tensorflow 2.0\n%tensorflow_version 2.x\nimport tensorflow as tf\n\nimport IPython\nimport functools\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\n\n\n# Download and import the MIT Introduction to Deep Learning package\n!pip install mitdeeplearning\nimport mitdeeplearning as mdl\n\n4.1 Dataset\n얼굴 인식 모델을 만들기 위해, 두가지 데이터를 사용할 것입니다:\n\nPositive Train : CelebA Dataset\nNegative Train : ImageNet\n\n이 데이터를 통해 이미지가 얼굴인지 아닌지 맞추는 binary classification 모델을 만들도록 하겠습니다.\n먼저 데이터를 다운받고 확인해봅시다.\n# Get the training data: both images from CelebA and ImageNet\npath_to_training_data = tf.keras.utils.get_file('train_face.h5', 'https://www.dropbox.com/s/hlz8atheyozp1yx/train_face.h5?dl=1')\n# Instantiate a TrainingDatasetLoader using the downloaded dataset\nloader = mdl.lab2.TrainingDatasetLoader(path_to_training_data)\n\n얼굴 인식 모델에서 Bias라 하면 어떤 것을 뜻할까요?\n특정 성별 인종에 대한 분류기의 작동이 미흡하거나, 안경과 같은 악세서리를 착용했을 때 작동이 미흡한 경우를 뜻할 것입니다.\n이는 잠재 공간에서 특정 부분이 희박하다는 것으로도 이해할 수 있습니다.\n\n\n4.2 CNN Model\n먼저 CNN 구조로 얼굴 인식 모델을 작성하겠습니다.\n### Define the CNN model ###\n\nn_filters = 12 # base number of convolutional filters\n\n'''Function to define a standard CNN model'''\ndef make_standard_classifier(n_outputs=1):\n  Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n  BatchNormalization = tf.keras.layers.BatchNormalization\n  Flatten = tf.keras.layers.Flatten\n  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n\n  model = tf.keras.Sequential([ \n    Conv2D(filters=1*n_filters, kernel_size=5,  strides=2),\n    BatchNormalization(),\n    \n    Conv2D(filters=2*n_filters, kernel_size=5,  strides=2),\n    BatchNormalization(),\n\n    Conv2D(filters=4*n_filters, kernel_size=3,  strides=2),\n    BatchNormalization(),\n\n    Conv2D(filters=6*n_filters, kernel_size=3,  strides=2),\n    BatchNormalization(),\n\n    Flatten(),\n    Dense(512),\n    Dense(n_outputs, activation=None),\n  ])\n  return model\n\nstandard_classifier = make_standard_classifier()\n### Train the standard CNN ###\n\n# Training hyperparameters\nbatch_size = 32\nnum_epochs = 2  # keep small to run faster\nlearning_rate = 5e-4\n\noptimizer = tf.keras.optimizers.Adam(learning_rate) # define our optimizer\nloss_history = mdl.util.LossHistory(smoothing_factor=0.99) # to record loss evolution\nplotter = mdl.util.PeriodicPlotter(sec=2, scale='semilogy')\nif hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n\n@tf.function\ndef standard_train_step(x, y):\n  with tf.GradientTape() as tape:\n    # feed the images into the model\n    logits = standard_classifier(x) \n    # Compute the loss\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n\n  # Backpropagation\n  grads = tape.gradient(loss, standard_classifier.trainable_variables)\n  optimizer.apply_gradients(zip(grads, standard_classifier.trainable_variables))\n  return loss\n\n# The training loop!\nfor epoch in range(num_epochs):\n  for idx in tqdm(range(loader.get_train_size()//batch_size)):\n    # Grab a batch of training data and propagate through the network\n    x, y = loader.get_batch(batch_size)\n    loss = standard_train_step(x, y)\n\n    # Record the loss and plot the evolution of the loss as a function of training\n    loss_history.append(loss.numpy().mean())\n    plotter.plot(loss_history.get())\n\n이제 모델을 평가해보겠습니다.\n### Evaluation of standard CNN ###\n\n# TRAINING DATA\n# Evaluate on a subset of CelebA+Imagenet\n(batch_x, batch_y) = loader.get_batch(5000)\ny_pred_standard = tf.round(tf.nn.sigmoid(standard_classifier.predict(batch_x)))\nacc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n\nprint(\"Standard CNN accuracy on (potentially biased) training set: {:.4f}\".format(acc_standard.numpy()))\nStandard CNN accuracy on (potentially biased) training set: 0.9976\n\n\nBias 조사하기\nCNN은 얼굴 인식과 같은 computer vision 문제에서 가장 많이 사용되는 구조일 것입니다.\n이 모델은 광범위하게 사용됨에도 불구하고 빅테크 회사들 마저 구현한 모델의 편향으로 인해 어려움을 겪고 있습니다.\nBuolamwini와 Gebru는 얼굴 인식 모델에서 피부색과 성별에 따라 정확도 차이가 매우 큼을 밝혀내기도 했습니다.\n이러한 문제를 어떻게 해결할 수 있을까요?\n가장 쉬운 방법 중 하나는 subclass를 생성하는 것일 겁니다. 훈련할 얼굴 이미지에 잠재적으로 생길만한 편향(인종 성별 모자 안경 등)을 고려하여 클래스를 부여하고 훈련시키는 것입니다.\n그러나 이 방법은 매우 비효율적입니다. 그리고 예기치 못한 bias에 대해 처리할 방법이 없습니다.\n조금 더 나은 방법이 있을까요?\n우리는 이번 실습에서 VAE를 통해 더 나은 방법을 알아볼 것입니다.\n\n\nVAE를 통해 잠재 공간 학습하기\nVAE를 통해 이루고자 하는 목표는 얼굴 훈련 데이터에 대한 기본 잠재 공간의 표현을 학습하는 모델을 훈련하는 것입니다.\n이렇게 학습된 표현은 데이터에서 어떤 특징이 과소 표현되거나 과대 표현되는지에 대한 정보를 제공합니다.\nVAE는 인코더-디코더 구조를 사용하여 입력 데이터의 잠재적 표현을 학습합니다. 컴퓨터 비전의 맥락에서 인코더 네트워크는 입력 이미지를 받아 평균과 표준 편차로 정의된 일련의 변수로 인코딩한 다음 이러한 매개 변수로 정의된 분포에서 샘플링된 잠재 변수 집합을 생성합니다. 그런 다음 디코더 네트워크는 이러한 변수를 ’디코딩’하여 원본 이미지의 재구성을 생성하고, 이 재구성은 학습 중에 모델이 어떤 잠재 변수를 학습하는 것이 중요한지 식별하는 데 사용됩니다.\n\n먼저 VAE의 Loss 함수에 대해 생각해 봅시다. 위의 그림에서와 같이 VAE는 정규 분포로 가정한 평균과 표준 편차를 학습하고, 이를 통해 기존의 이미지와 흡사한 이미지를 출력합니다. Loss는 이 두가지를 만족하기위해 Latent Loss와 Reconstruction Loss를 함께 최적화합니다.\nLatent Loss는 VAE에서 학습된 분포가 정규분포에 가깝도록 학습합니다. 정규 분포와의 차이를 구하기 위해 Kullback-Leibler Divergence를 이용합니다. 쿨백-라이블러에 대한 내용은 링크를 참조 부탁드립니다.\n\\[L_{KL}(\\mu, \\sigma) = \\frac{1}{2}\\sum_{j=0}^{k-1} (\\sigma_j + \\mu_j^2 - 1 - \\log{\\sigma_j})\\]\nReconsruction Loss는 단순히 만들어진 결과 이미지와 인풋 이미지의 차이의 절대값으로 정의합니다.\n\\[L_{x}{(x,\\hat{x})} = ||x-\\hat{x}||_1\\]\n\\[L_{VAE} = c\\cdot L_{KL} + L_{x}{(x,\\hat{x})}\\]\n### Defining the VAE loss function ###\n\n''' Function to calculate VAE loss given:\n      an input x, \n      reconstructed output x_recon, \n      encoded means mu, \n      encoded log of standard deviation logsigma, \n      weight parameter for the latent loss kl_weight\n'''\ndef vae_loss_function(x, x_recon, mu, logsigma, kl_weight=0.0005):\n  # TODO: Define the latent loss. Note this is given in the equation for L_{KL}\n  # in the text block directly above\n  latent_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1.0 - logsigma, axis=1)\n\n  # TODO: Define the reconstruction loss as the mean absolute pixel-wise \n  # difference between the input and reconstruction. Hint: you'll need to \n  # use tf.reduce_mean, and supply an axis argument which specifies which \n  # dimensions to reduce over. For example, reconstruction loss needs to average \n  # over the height, width, and channel image dimensions.\n  # https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean\n  reconstruction_loss = tf.reduce_mean(tf.abs(x-x_recon), axis=(1,2,3))\n\n  # TODO: Define the VAE loss. Note this is given in the equation for L_{VAE}\n  # in the text block directly above\n  vae_loss = kl_weight * latent_loss + reconstruction_loss\n  \n  return vae_loss\n이제 샘플링과 reparameterization에 대해 알아보겠습니다.\nVAE는 학습된 잠재 변수를 샘플링하기 위해 “재파라미터화 트릭”을 사용합니다.\nVAE 인코더는 각 잠재 변수에 대해 단일 실수 벡터를 생성하는 대신 가우스 분포를 대략 따르도록 제약된 평균 벡터와 표준편차 벡터를 생성합니다. 그런 다음 가우스 분포에서 노이즈 값 ϵ를 샘플링한 다음 표준 편차에 따라 스케일링하고 평균을 다시 추가하여 결과를 샘플링된 잠재 벡터로 출력합니다. 이를 ϵ∼N(0,(I))을 샘플링하는 잠재 변수 z에 대해 공식화하면 다음과 같습니다:\n\\[z = \\mu + e^{\\left(\\frac{1}{2} \\cdot \\log{\\Sigma}\\right)}\\circ \\epsilon\\]\n여기서 μ는 평균이고 Σ는 공분산 행렬입니다. 이를 통해 VAE의 손실 함수를 깔끔하게 정의하고, 무작위로 샘플링된 잠재 변수를 생성하고, 역전파를 통해 학습할 수 있습니다\n### VAE Sampling ###\n\n\"\"\"Sample latent variables via reparameterization with an isotropic unit Gaussian.\n# Arguments\n    z_mean, z_logsigma (tensor): mean and log of standard deviation of latent distribution (Q(z|X))\n# Returns\n    z (tensor): sampled latent vector\n\"\"\"\ndef sampling(z_mean, z_logsigma):\n  # By default, random.normal is \"standard\" (ie. mean=0 and std=1.0)\n  batch, latent_dim = z_mean.shape\n  epsilon = tf.random.normal(shape=(batch, latent_dim))\n\n  # TODO: Define the reparameterization computation!\n  # Note the equation is given in the text block immediately above.\n  z = z_mean + tf.math.exp(0.5 * z_logsigma) * epsilon\n  \n  return z\n\n\nSemi-Supervised Variational Autoencoder (SS-VAE)\n이제 CNN과 VAE에 대해 어느정도 이해가 되었을 것이라 생각합니다.\n얼굴 인식 작업에 대한 분류 결정을 출력하고 모델의 편향이 어디에서 비롯되었는지 분석하기 위해 지도 구성 요소가 있는 VAE를 개발할 것입니다. 부올람위니와 게브루의 연구와 같은 이전 연구에서는 얼굴 인식 모델에 편향이 발생할 수 있는 두 가지 범주로 피부색과 성별에 초점을 맞췄지만, 라벨이 지정되지 않은 다른 특징들도 편향되어 분류 성능이 저하될 수 있습니다. 이러한 잠재적 특징을 학습할 수 있도록 반지도형 VAE(SS-VAE)를 구축할 예정입니다.\n\nSS-VAE를 지도 분류 문제, 즉 얼굴 감지 작업에 적용해 보겠습니다. 중요한 점은 SS-VAE 아키텍처의 인코더 부분이 클래스 예측(얼굴 또는 얼굴이 아님)에 해당하는 단일 감독 변수인 \\(z_o\\) 를 출력한다는 점입니다. 일반적으로 VAE는 클래스 예측과 같은 감독 변수를 출력하도록 훈련되지 않습니다. 이것이 SS-VAE와 기존 VAE의 주요 차이점입니다.\n이진 분류 문제에 대해 모델을 훈련하더라도 잠재적인 편견을 발견하는 데 관심이 있기 때문에 얼굴의 잠재적 표현만 학습하고자 한다는 점을 기억해야합니다. 즉, 이미지가 얼굴일 때는 잠재적 표현을 학습하고, 얼굴이 아닐 때에는 \\(z_o\\)를 출력한 이후 종료되어야 합니다.\nSS-VAE의 Loss 함수를 정의해보겠습니다. 기존 VAE와 달리 Classificaion이 추가되었기에 이에 대한 Loss 함수가 추가되어야 합니다. 또한 이미지가 얼굴일 때만 VAE의 Loss가 계산되어야 할 것입니다.\n즉, classification loss \\(L_y\\), VAE loss \\({I}_f(y)\\Big[L_{VAE}\\Big]\\)가 필요하며, \\({I}_f(y)\\) 는 1 일 때 얼굴, 0일 때 얼굴이 아는 다른 이미지여야 합니다.\n\\[L_{total} = L_y(y,\\hat{y}) + {I}_f(y)\\Big[L_{VAE}\\Big]\\]\n### Loss function for SS-VAE ###\n\n\"\"\"Loss function for SS-VAE.\n# Arguments\n    x: true input x\n    x_pred: reconstructed x\n    y: true label (face or not face)\n    y_logit: predicted labels\n    mu: mean of latent distribution (Q(z|X))\n    logsigma: log of standard deviation of latent distribution (Q(z|X))\n# Returns\n    total_loss: SS-VAE total loss\n    classification_loss: SS-VAE classification loss\n\"\"\"\ndef ss_vae_loss_function(x, x_pred, y, y_logit, mu, logsigma):\n\n  # TODO: call the relevant function to obtain VAE loss, defined earlier in the lab\n  vae_loss = vae_loss_function(x, x_pred, mu, logsigma) # TODO\n\n  # TODO: define the classification loss using sigmoid_cross_entropy\n  # https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n  classification_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_logit) # TODO\n\n  # Use the training data labels to create variable face_indicator:\n  #   indicator that reflects which training data are images of faces\n  face_indicator = tf.cast(tf.equal(y, 1), tf.float32)\n\n  # TODO: define the SS-VAE total loss! Use tf.reduce_mean to average over all\n  # samples\n  total_loss = tf.reduce_mean(\n      classification_loss + \n      face_indicator * vae_loss\n  )\n  return total_loss, classification_loss, vae_loss\n이제 모델을 구성해보도록 합시다\n### Define the decoder portion of the SS-VAE ###\n\ndef make_face_decoder_network(n_filters=12):\n\n  # Functionally define the different layer types we will use\n  Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu')\n  BatchNormalization = tf.keras.layers.BatchNormalization\n  Flatten = tf.keras.layers.Flatten\n  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n  Reshape = tf.keras.layers.Reshape\n\n  # Build the decoder network using the Sequential API\n  decoder = tf.keras.Sequential([\n    # Transform to pre-convolutional generation\n    Dense(units=4*4*6*n_filters),  # 4x4 feature maps (with 6N occurances)\n    Reshape(target_shape=(4, 4, 6*n_filters)),\n\n    # Upscaling convolutions (inverse of encoder)\n    Conv2DTranspose(filters=4*n_filters, kernel_size=3,  strides=2),\n    Conv2DTranspose(filters=2*n_filters, kernel_size=3,  strides=2),\n    Conv2DTranspose(filters=1*n_filters, kernel_size=5,  strides=2),\n    Conv2DTranspose(filters=3, kernel_size=5,  strides=2),\n  ])\n\n  return decoder\n### Defining and creating the SS-VAE ###\n\nclass SS_VAE(tf.keras.Model):\ndef __init__(self, latent_dim):\n  super(SS_VAE, self).__init__()\n  self.latent_dim = latent_dim\n\n  # Define the number of outputs for the encoder. Recall that we have \n  # `latent_dim` latent variables, as well as a supervised output for the \n  # classification.\n  num_encoder_dims = 2*self.latent_dim + 1\n\n  self.encoder = make_standard_classifier(num_encoder_dims)\n  self.decoder = make_face_decoder_network()\n\n# function to feed images into encoder, encode the latent space, and output\n#   classification probability \ndef encode(self, x):\n  # encoder output\n  encoder_output = self.encoder(x)\n\n  # classification prediction\n  y_logit = tf.expand_dims(encoder_output[:, 0], -1)\n  # latent variable distribution parameters\n  z_mean = encoder_output[:, 1:self.latent_dim+1] \n  z_logsigma = encoder_output[:, self.latent_dim+1:]\n\n  return y_logit, z_mean, z_logsigma\n\n# Decode the latent space and output reconstruction\ndef decode(self, z):\n  # TODO: use the decoder (self.decoder) to output the reconstruction\n  reconstruction = self.decoder(z) # TODO\n  return reconstruction\n\n# The call function will be used to pass inputs x through the core VAE\ndef call(self, x): \n  # Encode input to a prediction and latent space\n  y_logit, z_mean, z_logsigma = self.encode(x)\n\n  # TODO: call the sampling function that you created above using \n  #       z_mean and z_logsigma\n  z = sampling(z_mean, z_logsigma) # TODO\n\n  # TODO: reconstruction\n  recon = self.decode(z) # TODO\n  \n  return y_logit, z_mean, z_logsigma, recon\n\n# Predict face or not face logit for given input x\ndef predict(self, x):\n  y_logit, z_mean, z_logsigma = self.encode(x)\n  return y_logit\n\nss_vae = SS_VAE(latent_dim=32)\n### Training the SS-VAE ###\n\n# Hyperparameters\nbatch_size = 32\nlearning_rate = 5e-4\nlatent_dim = 32\n\n# SS-VAE needs slightly more epochs to train since its more complex than \n# the standard classifier so we use 6 instead of 2\nnum_epochs = 6\n\n# instantiate a new SS-VAE model and optimizer\nss_vae = SS_VAE(latent_dim)\noptimizer = tf.keras.optimizers.Adam(learning_rate)\n\n# To define the training operation, we will use tf.function which is a powerful tool \n#   that lets us turn a Python function into a TensorFlow computation graph.\n@tf.function\ndef ss_vae_train_step(x, y):\n\n  with tf.GradientTape() as tape:\n    # Feed input x into ss_vae. Note that this is using the SS_VAE call function!\n    y_logit, z_mean, z_logsigma, x_recon = ss_vae(x)\n\n    '''TODO: call the SS_VAE loss function to compute the loss'''\n    loss, class_loss, _ = ss_vae_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma) # TODO\n  \n  '''TODO: use the GradientTape.gradient method to compute the gradients.\n     Hint: this is with respect to the trainable_variables of the SS_VAE.'''\n  grads = tape.gradient(loss, ss_vae.trainable_variables) # TODO\n\n  # apply gradients to variables\n  optimizer.apply_gradients(zip(grads, ss_vae.trainable_variables))\n  return loss\n\n# get training faces from data loader\nall_faces = loader.get_all_train_faces()\n\nif hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n\n# The training loop -- outer loop iterates over the number of epochs\nfor i in range(num_epochs):\n\n  IPython.display.clear_output(wait=True)\n  print(\"Starting epoch {}/{}\".format(i+1, num_epochs))\n  \n  # get a batch of training data and compute the training step\n  for j in tqdm(range(loader.get_train_size() // batch_size)):\n    # load a batch of data\n    (x, y) = loader.get_batch(batch_size)\n    # loss optimization\n    loss = ss_vae_train_step(x, y)\n    \n    # plot the progress every 200 steps\n    if j % 500 == 0: \n      mdl.util.plot_sample(x, y, ss_vae)\n\n\nSS-VAE를 활용하여 bias 발견하기\n이제 SS-VAE의 학습을 모두 마쳤으니, bias를 발견해봅시다.\nSS-VAE를 훈련하려면 VAE 재구성 손실과 지도 분류 손실이 모두 필요했습니다. VAE 재구성 손실은 모델이 특정 입력 데이터를 얼마나 잘 처리할 수 있는지를 반영하며, 재구성 손실이 높을수록 모델이 특정 예제를 학습하기가 더 어려워집니다.\n그렇다면 재구성 하기 힘든 결과일수록 학습이 덜 된 이미지라고 할 수 있겠죠. 즉, 재구성 손실이 높은 이미지들의 특징을 확인하여 우리가 훈련한 모델의 어떠한 부분이 부족한지 알 수 있습니다.\n### Linking model performance to uncertainty and bias\n\n# Load a random sample of 5000 faces from our dataset and compute the model performance on them\n(x, y) = loader.get_batch(5000, only_faces=True)\ny_logit, z_mean, z_logsigma, x_recon = ss_vae(x)\nloss, class_loss, vae_loss = ss_vae_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma)\n\n# Sort the results by the vae loss scores\nvae_loss = vae_loss.numpy()\nind = np.argsort(vae_loss, axis=None)\n\n# Plot the 25 samples with the highest and lowest reconstruction losses\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\nax[0].imshow(mdl.util.create_grid_of_images(x[ind[:25]]))\nax[0].set_title(\"Samples with the lowest reconstruction loss \\n\" + \n                f\"Average recon loss: {np.mean(vae_loss[ind[:25]]):.2f}\")\n\nax[1].imshow(mdl.util.create_grid_of_images(x[ind[-25:]]))\nax[1].set_title(\"Samples with the highest reconstruction loss \\n\" + \n                f\"Average recon loss: {np.mean(vae_loss[ind[-25:]]):.2f}\");\n\n이제 우리는 어떠한 이미지가 재구성 손실이 높은지, 낮은지 확인할 수 있습니다.\n각 경우에 대해 특징을 발견하실 수 있나요?\n제가 보았을 때 재구성 손실이 낮은 예시의 경우 대부분 정면을 바라보고, 안경을 쓰지 않았고, 백인인 반면에\n재구성 손일이 높은 예시의 경우 측면을 바라보고, 안경을 쓰고, 타 인종인 경우가 많다고 생각하였습니다."
  },
  {
    "objectID": "posts/causal-relationship/index.html",
    "href": "posts/causal-relationship/index.html",
    "title": "인과관계의 조건과 탐색",
    "section": "",
    "text": "영국의 철학자 데이비드 흄이 그의 저서 “인간 지성의 조사”에서 인간 지식의 인과관계를 인식하는데 있어 필요한 세 가지 조건을 제시하였습니다. 조건은 아래와 같습니다 :\n\n인접성(contiguity) : 인과 관계에 있는 두 사건이 공간적으로나 시간적으로 가까워야 함을 의미합니다. 예를 들어, 어떤 원인이 결과를 초래하려면, 그 원인과 결과는 공간적으로 인접해 있어야 하며, 시간적으로도 가까운 시점에서 발생해야 합니다. 이 개념은 인과 관계를 분석할 때 중요한 역할을 합니다.\n시간적 선행(temporal precedence) : 인과 관계에 있는 두 사건이 시간 순서에 따라 발생해야 함을 의미합니다. 원인이 항상 결과에 선행해야 합니다. 즉, 원인이 먼저 발생하고 그 다음에 결과가 발생합니다.\n지속적 연관(constant conjunction) : 지속적 연관은 인과 관계에 있는 두 사건이 일정하게 발생한다는 것을 의미합니다. 이 개념은 두 사건 사이에 인과 관계가 있다고 추론할 때 중요한 역할을 합니다. 예를 들어, 어떤 원인이 항상 특정한 결과와 함께 발생하면, 그 두 사건 사이에 인과 관계가 있다고 추론할 수 있습니다.\n\n마찬가지로 영국의 철학자인 존 스튜어트 밀의 경우, 인과관계를 탐색하는 데에 있어 5가지 방법을 제시하였습니다. 이는 ’Mill’s Method’라고도 불립니다. 다섯 가지 방법은 하기와 같습니다:\n\n동시 발생의 방법 (Method of Agreement): 서로 다른 상황에서 결과가 발생할 때 공통적으로 존재하는 요소를 인과 요인으로 간주합니다.\n차이의 방법 (Method of Difference): 결과가 발생하는 상황과 발생하지 않는 상황에서의 유일한 차이점을 인과 요인으로 간주합니다.\n공변의 방법 (Method of Concomitant Variation): 인과 요인과 결과가 함께 변하는 정도를 관찰하여 인과 관계를 찾습니다.\n잔여의 방법 (Method of Residues): 이미 알려진 인과 요인을 제외한 후, 남아있는 결과와 요인 사이의 인과 관계를 찾습니다.\n역설적 합리화의 방법 (Method of Contra-positive Reasoning): 인과 요인이 없을 때 결과도 발생하지 않는 것을 관찰하여 인과 관계를 확인합니다."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "front_image.jpg\n\n\n\nClustering은..\n클러스터링(clustering)은 데이터 마이닝과 머신러닝에서 사용되는 비지도 학습(unsupervised learning) 방법 중 하나입니다. 클러스터링의 목적은 주어진 데이터셋에서 유사한 특성을 가진 데이터 포인트들을 그룹화하는 것입니다. 이렇게 그룹화된 데이터 포인트들의 집합을 클러스터(cluster)라고 합니다. 클러스터링은 고객 세분화, 이미지 분류, 문서 군집화, 이상치 탐지, 추천 시스템 등 다양한 분야에서 활용됩니다. 클러스터링을 통해 데이터의 구조와 패턴을 발견하고, 새로운 인사이트를 얻거나 의사 결정을 돕는 데 도움이 됩니다. 주요 기법은 다음과 같습니다:\n\n중심 기반 클러스터링(Centroid based clustering): 각 클러스터의 중심을 정의하고, 각 데이터 포인트를 가장 가까운 중심에 할당하는 방식으로 클러스터를 형성합니다. 중심과 데이터 포인트 간 거리의 제곱합을 최소화하는 방식으로 반복적으로 최적화합니다. K-means clustering\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): 데이터 포인트의 밀도에 기반한 클러스터링 방법으로, 밀도가 높은 영역을 클러스터로 인식하고, 밀도가 낮은 영역은 노이즈로 처리합니다. 이 방법은 클러스터의 개수를 미리 지정할 필요가 없으며, 노이즈에 대한 처리가 가능합니다. DBSCAN\n계층적 클러스터링(Hierarchical clustering): 데이터 포인트 간의 거리나 유사도를 기반으로 가장 가까운 데이터 포인트나 클러스터를 병합하는 방식으로 진행됩니다. 이 방법은 덴드로그램(dendrogram)이라는 나무 형태의 구조로 클러스터링 과정을 시각화할 수 있습니다. Hierarchical Clustering\n스펙트럼 클러스터링(Spectral clustering): 데이터의 유사도 행렬을 사용하여 그래프를 구성하고, 그래프의 스펙트럼(고유값과 고유벡터)을 기반으로 클러스터를 형성합니다. 이 방법은 비선형 구조를 가진 데이터에 적합하며, 클러스터의 모양이 복잡한 경우에도 잘 작동합니다.\n\n\n\n데이터 포인트 간의 거리는..\n\n유클리디언 거리 (Euclidean Distance): 두 점 사이의 직선 거리를 계산하는 가장 기본적인 거리 메트릭입니다. 이 거리는 기하학적 공간에서 두 점 사이의 거리를 직관적으로 이해하기 쉽습니다. 유클리디안 거리는 L2 노름(norm)으로도 알려져 있습니다.\n맨하탄 거리 (Manhattan Distance): 각 축에 따라 수직으로 이동하여 두 점 사이의 거리를 계산하는 방법으로, L1 노름(norm)으로도 알려져 있습니다. 이 거리 메트릭은 그리드 기반의 데이터에서 종종 사용됩니다.\n코사인 유사도 (Cosine Similarity): 두 벡터 간의 코사인 각도를 사용하여 유사성을 측정하는 방법입니다. 값의 범위는 -1에서 1까지이며, 1에 가까울수록 벡터 간의 방향이 유사함을 나타냅니다. 이 메트릭은 텍스트 문서와 같이 고차원 데이터에서 유용하게 사용됩니다.\nMahalanobis distance(마할라노비스 거리): 다차원 공간에서 두 데이터 포인트 간의 거리를 측정하는 방법 중 하나입니다. 이 거리 측정 방법은 각 차원의 스케일(scale)과 상호간의 공분산(covariance)을 고려하여 거리를 계산합니다. 즉, 각 차원의 중요성을 고려하여 거리를 계산하기 때문에 특이한 데이터나 이상치(outlier)에 덜 민감하다는 장점이 있습니다."
  },
  {
    "objectID": "posts/compare-kld-chi/index.html",
    "href": "posts/compare-kld-chi/index.html",
    "title": "Kullback-Leibler Divergence 와 Chi-square 검정",
    "section": "",
    "text": "최근 쿨백-라이블러 발산의 글을 작성한 이후, \\(\\chi^2\\) 검정에도 goodness of fit을 통해 관측 데이터가 분포를 따르는지 확인 할 수 있는데,\n이 두가지 방식에 어떠한 연관점이 있나 확인해보았습니다. 유감스럽게도 두 방식 모두 분포간의 차이를 측정할 수 있는 방법이지만, 둘 사이에 직접적인 수학 관계는 없다고 합니다.\n분명 관계가 있을 것 같은데, 없다고 하니.. 부족한 수학 실력을 뒤로하고 찜찜한 마음으로 두 방식에 대해 시뮬레이션해보았습니다.\n# 동전 던지기 결과를 관찰한 데이터를 생성합니다. 앞면은 1, 뒷면은 0으로 표현합니다.\n\nobserved &lt;- c(rep(1, 55), rep(0, 45))  \nobserved2 &lt;- c(rep(1, 80), rep(0, 200))\n\n# 카이 제곱 적합도 검정을 수행합니다.\n# 앞면과 뒷면이 나올 이론적 확률은 각각 0.5입니다.\n\nchisq.test(table(observed), p = c(0.5, 0.5))\nchisq.test(table(observed2), p = c(0.5, 0.5))\n\n# KL 발산을 계산합니다.\n# 베르누이 분포의 이론적 확률을 설정합니다.\nexpected_prob &lt;- c(0.5, 0.5)\n\n# 관찰된 데이터의 확률을 계산합니다.\nobserved_prob &lt;- table(observed) / length(observed)\nobserved_prob2 &lt;- table(observed2) / length(observed2\n                                            )\n# KL 발산을 계산합니다.\nKL_divergence &lt;- sum(observed_prob * log(observed_prob / expected_prob))\nKL_divergence2 &lt;- sum(observed_prob2 * log(observed_prob2 / expected_prob))\n첫번째 코인의 경우, Chi-squared test의 경우 p-value 0.32, KL-divergence의 경우 0.005\n두번째 코인의 경우, Chi-squared test의 경우 p-value 7.4e-13, KL-divergence의 경우 0.095\n로 계산됩니다.\nchi_arr &lt;- numeric(99)\nkld_arr &lt;- numeric(99)\n\n\nfor(i in 1:99) {\n  observed &lt;- c(rep(1,i), rep(0,(100 - i)))\n  expected_prob &lt;- c(0.5, 0.5)\n  observed_prob &lt;- table(observed) / length(observed)\n  test &lt;- chisq.test(table(observed), p = expected_prob)\n  chi &lt;- test$p.value\n  kld &lt;- sum(observed_prob * log(observed_prob / expected_prob))\n  \n  chi_arr[i] &lt;- chi\n  kld_arr[i] &lt;- kld\n  \n}\n\nlibrary(tidyverse)\n\nprob_arr &lt;- (1:99)/100\nplot_dt &lt;- tibble(prob = prob_arr, pvalue = chi_arr, kld = kld_arr)\nplot_dt &lt;- pivot_longer(plot_dt,cols = c(\"pvalue\", \"kld\"), names_to = \"method\")\nplot_dt |&gt; ggplot(aes(x = prob, y = value, color = method)) + geom_line()\np(H) 가 0.01 ~ 0.99 까지의 p-value와 KL-divergence의 값을 시각화한 값입니다.\n\nChi-square 검정의 경우 0.4 부근에서 급격하게 치솟는 모습을 볼 수 있으나, KL-divergence의 경우 2차 곡선의 형태를 보입니다.\nChi-square 검정은 검정이라는 이름답게 다소 보수적인 모습을 볼 수 있고\nKL-divergence의 경우 주로 데이터가 얼마나 분포에 가까운지에 대한 거리 metric으로 주로 사용되는 만큼 완만함을 알 수 있습니다."
  },
  {
    "objectID": "posts/covariance/index.html",
    "href": "posts/covariance/index.html",
    "title": "Covariance Matrix",
    "section": "",
    "text": "공분산 행렬은…\n공분산 행렬(covariance matrix)은 변수들 간의 공분산을 요소로 갖는 정방행렬(square matrix)입니다. 공분산은 두 변수가 함께 변하는 정도를 측정하는 값으로, 하나의 변수가 증가할 때 다른 변수가 어떻게 변하는지를 나타냅니다.\n공분산 행렬에서 대각선 요소는 각 변수의 분산(variance)을 나타내며, 이는 해당 변수가 얼마나 퍼져 있는지를 측정합니다. 비대각선 요소는 서로 다른 두 변수 간의 공분산을 나타냅니다.\nn개의 변수를 가진 데이터 세트의 공분산 행렬은 n x n 크기의 정방행렬이며, 행렬의 (i, j) 위치에 있는 요소는 변수 i와 변수 j의 공분산을 나타냅니다.\n예를 들어, 데이터 세트에 변수 X와 변수 Y가 있다고 가정하면 공분산 행렬은 다음과 같이 나타낼 수 있습니다.\n\nCov(X, X)   Cov(X, Y)\nCov(Y, X)   Cov(Y, Y)\n\n여기서 Cov(X, X)는 변수 X의 분산, Cov(Y, Y)는 변수 Y의 분산, Cov(X, Y)와 Cov(Y, X)는 변수 X와 변수 Y의 공분산을 나타냅니다.\n공분산 행렬은 데이터의 선형 관계를 나타내는 정보를 포함하며, 주성분 분석(PCA)과 같은 통계 및 기계 학습 기법에서 중요한 역할을 합니다. 공분산 행렬을 사용하면 데이터의 구조를 파악하고 변수 간의 상관 관계를 분석할 수 있습니다."
  },
  {
    "objectID": "posts/dbscan/index.html",
    "href": "posts/dbscan/index.html",
    "title": "DBSCAN",
    "section": "",
    "text": "Result of DBSCAN\n\n\n\nDBSCAN은..\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)은 밀도 기반의 클러스터링 알고리즘 중 하나로, 1996년에 Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu에 의해 제안되었습니다. DBSCAN은 밀도 기반의 클러스터링을 수행하기 때문에 클러스터의 모양이 원형이 아닌 경우에도 유연하게 적용할 수 있으며, 노이즈를 포함한 데이터에 대해서도 강인한 성능을 보입니다.\nDBSCAN 알고리즘은 다음과 같은 주요 개념을 활용합니다:\n\n이웃 반경(epsilon, ε): 데이터 포인트에서 주어진 거리 내에 있는 이웃 포인트를 찾는 데 사용되는 거리입니다.\n최소 포인트(MinPts): 밀집 영역이라고 간주되기 위해 주어진 이웃 반경 내에 존재해야 하는 포인트의 최소 개수입니다.\n\n알고리즘의 동작 방식은 다음과 같습니다:\n\n데이터셋 내의 모든 포인트를 순회하며, 아직 클러스터에 할당되지 않은 포인트를 선택합니다.\n선택한 포인트에서 epsilon 거리 이내의 이웃 포인트를 찾습니다.\n이웃 포인트의 수가 MinPts 이상이면, 새로운 클러스터를 생성하고 해당 포인트와 그 이웃들을 해당 클러스터에 할당합니다.\n이웃 포인트의 이웃들을 또한 순회하며, MinPts 이상의 이웃을 가지고 있는 포인트를 발견하면 해당 클러스터에 추가합니다. 이 과정을 반복하여 클러스터가 더 이상 확장되지 않을 때까지 수행합니다.\n모든 포인트를 순회할 때까지 1-4 단계를 반복합니다. 이 과정이 끝나면 클러스터가 생성되며, 어떠한 클러스터에도 속하지 않는 포인트는 노이즈로 간주됩니다.\n\nDBSCAN의 주요 장점은 클러스터의 개수를 사전에 지정할 필요가 없으며, 클러스터의 모양에 대한 가정이 없어 다양한 형태의 클러스터를 찾을 수 있다는 점입니다. 또한, 이 알고리즘은 노이즈를 구분하여 클러스터링에 영향을 미치지 않게 처리할 수 있습니다. 단점으로는 높은 차원의 데이터에 대한 성능 저하가 있으며, epsilon과 MinPts와 같은 하이퍼파라미터를 선택하는 것이 어려울 수 있다는 점입니다.\n\n\nDBSCAN 예시\nlibrary(tidyverse)\nlibrary(reticulate)\n\n# Get Smile Data\ncircle_df &lt;- data.frame(\n  t = seq(0, 2 * pi, length.out = 100),\n  x = 0,\n  y = 0,\n  r = 1\n)\ncircle_df$x &lt;- circle_df$r * cos(circle_df$t) + rnorm(100,0,0.03)\ncircle_df$y &lt;- circle_df$r * sin(circle_df$t) + rnorm(100,0,0.03)\n\neyes_df &lt;- data.frame(\n  x = rep(c(-0.3, 0.3),50) + rnorm(50,0,0.03),\n  y = rep(c(0.4, 0.4),50) + rnorm(50,0,0.03)\n)\n\nmouth_df &lt;- data.frame(\n  t = seq(-pi, 0, length.out = 100),\n  x = 0,\n  y = -0.3,\n  r = 0.5\n)\nmouth_df$x &lt;- mouth_df$r * cos(mouth_df$t) + rnorm(100,0,0.03)\nmouth_df$y &lt;- mouth_df$y + mouth_df$r * sin(mouth_df$t) + rnorm(100,0,0.03)\n\nsmile_df &lt;- rbind(circle_df[,2:3],eyes_df,mouth_df[,2:3])\n# Run DBSCAN\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\n\nX = r.smile_df\n\ndbscan = DBSCAN(eps=0.15, min_samples=10)\ndbscan.fit(X)\nlabels = dbscan.labels_\nlabels\n# Plot result\nsmile_df &lt;- smile_df |&gt; mutate(label = as.factor(py$labels))\nggplot(smile_df,aes(x = x, y = y, color = label)) + geom_point()"
  },
  {
    "objectID": "posts/dimension-reduction/index.html",
    "href": "posts/dimension-reduction/index.html",
    "title": "Dimension Reduction (차원 축소)",
    "section": "",
    "text": "차원의 저주란..\n차원의 저주(curse of dimensionality)는 고차원 데이터에서 발생하는 여러 문제를 총체적으로 설명하는 용어입니다. 데이터의 차원이 증가할수록, 데이터 포인트 간의 거리가 점점 멀어져서 데이터가 희소해지는 현상이 발생합니다. 이러한 현상은 다음과 같은 문제를 야기합니다:\n\n거리 측정의 어려움: 고차원 공간에서는 데이터 포인트 간의 거리가 크게 증가하므로, 가까운 이웃을 찾거나 클러스터링하는 것이 어렵습니다.\n계산 복잡도 증가: 고차원 데이터에서 모델을 학습하거나 예측을 수행하는 데 필요한 계산량이 크게 증가합니다.\n과적합 위험 증가: 데이터의 차원이 증가할수록, 모델이 학습 데이터에 과도하게 적합되어 일반화 성능이 떨어지는 과적합(overfitting)이 발생하기 쉽습니다.\n특성 선택의 어려움: 고차원 데이터에서는 어떤 특성이 중요한지 판단하거나 적절한 특성 조합을 찾는 것이 어려워집니다.\n\n\n\n차원 축소란..\n차원 축소(dimension reduction)는 고차원 데이터를 저차원으로 변환하는 과정으로, 데이터 과학 및 머신러닝에서 중요한 역할을 합니다. 차원 축소의 필요성은 다음과 같습니다:\n\n데이터 시각화: 고차원 데이터를 2D 또는 3D로 줄여서 데이터의 패턴이나 구조를 쉽게 이해할 수 있게 합니다.\n계산 효율성: 저차원 데이터는 계산량이 줄어들어 머신러닝 모델의 학습 및 예측 속도가 빨라집니다.\n잡음 제거: 차원 축소는 데이터의 중요한 정보를 유지하면서 불필요한 변동성이나 잡음을 제거하는 데 도움이 됩니다.\n과적합 방지: 차원의 저주(curse of dimensionality)로 인해 고차원 데이터에는 과적합(overfitting)이 발생하기 쉽습니다. 차원 축소를 통해 과적합을 방지할 수 있습니다.\n\n\n\n주요 차원 축소 기법\n\nPCA (주성분 분석, Principal Component Analysis): 데이터의 분산을 최대한 보존하는 새로운 축을 찾아 고차원 데이터를 저차원으로 변환합니다. 선형적인 데이터 구조에 적합합니다. Principal Component Analysis\nNMF (음수 미포함 행렬 인수분해, Non-negative Matrix Factorization): 원래의 데이터 행렬을 음수가 아닌 두 개의 행렬의 곱으로 분해합니다. 이 방법은 데이터가 음수가 아닌 특성을 가질 때 유용하며, 데이터의 희소성을 고려할 수 있습니다.\nt-SNE (t-분포 확률적 임베딩, t-Distributed Stochastic Neighbor Embedding): 고차원 데이터의 점들 간 거리를 저차원에서의 확률 분포로 보존하려고 시도하는 비선형 차원 축소 기법입니다. 데이터의 군집 구조나 매니폴드 구조를 잘 보존합니다. t-SNE\nUMAP (Uniform Manifold Approximation and Projection)은 고차원 데이터를 저차원 공간으로 축소하는 비선형 차원 축소 기법 중 하나입니다. UMAP은 t-SNE와 유사하게 데이터의 군집 구조를 보존하면서 고차원 데이터를 시각화하거나 차원 축소하는 데 사용됩니다. 그러나 UMAP은 기하학적 특성을 더 잘 보존하고, 계산 효율성이 더 높아 대규모 데이터셋에도 적용 가능하다는 장점이 있습니다. UMAP"
  },
  {
    "objectID": "posts/dl-multi-variables-linear-regression/index.html",
    "href": "posts/dl-multi-variables-linear-regression/index.html",
    "title": "딥러닝을 위한 Multi variables Linear Regression",
    "section": "",
    "text": "이전의 Simple Linear Regression예제에 이어, 다변수를 통해 결과값을 예측하는 모델에 대해 알아보도록 하겠습니다.\n퀴즈1, 퀴즈2, 중간고사의 성적으로 기말고사의 성적을 예측하는 모델을 예시로 하겠습니다.\n\n\n# A tibble: 5 × 4\n  quiz1 quiz2 midterm final\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    73    80      75   152\n2    93    88      95   185\n3    89    91      90   180\n4    96    98     100   196\n5    73    66      70   142\n\n\n이전과 달리 벡터 기반으로 식을 나타내면 아래와 같습니다.\n\\[ H(x) = XW + b \\] 위의 사례로 식을 풀어쓰면, 하기와 같습니다.\nx1, x2, x3 는 quiz1, quiz2, midterm의 점수 (행 벡터) w1, w2, w3 는 quiz1, quiz2, mideterm에 대한 계수 (열 벡터)\n\\[ H(x) = x_1w_1 + x_2w_2 + x_3w_3 + b \\]\n이 선과 실제 데이터의 오차는 이전과 같습니다.\n\\[ cost(W,b) = \\frac{1}{m}\\sum_{i=1}^{m}(H(x^{(i)}) - y^{(i)})^2 \\]\n마찬가지로, cost(오차)를 최소화 할 수 있는 W,b의 쌍을 찾는 것입니다. 오차를 최소화 하는 값을 찾기 위해, Gradient Descent를 통해 확인해보도록 하겠습니다.\n\n초기 값 W,b에 대한 각각의 편미분 값을 구합니다.\n편미분 값에 learning rate \\(\\alpha\\)를 곱해줍니다.\nW, b에 곱해준 값을 뺀 후 W, b 값을 업데이트 합니다.\ncost가 만족할만한 수준이 될 때 까지 1 ~ 3을 반복합니다.\n\n예시 데이터를 아래의 스크립트를 통해 \\(XW + b\\)를 구해보도록 하겠습니다.\n\n# 필요한 라이브러리 불러오기\nlibrary(tidyverse)\nlibrary(glue)\n\n# Gradient Descent 함수 정의\ngradient_descent &lt;- function(X, y, alpha, iterations) {\n  n &lt;- length(y)\n  X &lt;- cbind(1, as.matrix(X)) # add intercept term to X matrix\n  m &lt;- ncol(X)\n  y &lt;- as.matrix(y)\n  W &lt;- matrix(rnorm(m), ncol = 1)\n  \n  for (i in 1:iterations) {\n    y_predicted &lt;- X %*% W\n    cost &lt;- y_predicted - y\n    W &lt;- W - alpha*(1/n)*t(X) %*% (y_predicted - y)\n  }\n  return(W)\n}\n\n\n# Gradient Descent 실행\nresult &lt;- gradient_descent(example[,-4], example[4], alpha = 0.00001, iterations = 2000)\n\n# 결과 출력\ncat(glue(\"절편 : {result[1,]}\nquiz1 : {result[2,]}\nquiz2 : {result[3,]}\nmidterm : {result[4,]}\"))\n\n절편 : 0.616439443438877\nquiz1 : 1.79538749134914\nquiz2 : 0.628151238241493\nmidterm : -0.405326284524674\n\ncat(\"예측: \\n\")\n\n예측: \n\nprint(cbind(1,as.matrix(example[,-4])) %*% result)\n\n        final\n[1,] 151.5324\n[2,] 184.3588\n[3,] 181.0883\n[4,] 193.9998\n[5,] 144.7649\n\ncat(\"실제: \\n\")\n\n실제: \n\nprint(as.matrix(example[,4]))\n\n     final\n[1,]   152\n[2,]   185\n[3,]   180\n[4,]   196\n[5,]   142"
  },
  {
    "objectID": "posts/dl-simple-linear-regression/index.html",
    "href": "posts/dl-simple-linear-regression/index.html",
    "title": "딥러닝을 위한 Simple Linear Regression",
    "section": "",
    "text": "하기와 같이 공부한 시간과 성적의 데이터가 있을 때, 시간을 통해 성적를 예측하고자 합니다. 두 변수의 관계가 선형적이라 가정할 때, 하기의 파란색 선과 같이 점수를 가장 잘 예측하는 선을 찾는 것이 이번 포스트의 목적입니다.\n\n\n# A tibble: 4 × 2\n  hours score\n  &lt;dbl&gt; &lt;dbl&gt;\n1    10    90\n2     9    80\n3     3    50\n4     2    30\n\n\n\n\n\n\n\n위의 선을 다음과 같은 식으로 표기하도록 해봅시다.\n\\[ H(x) = Wx + b \\] 이 선과 실제 데이터의 오차는 하기와 같이 표기할 수 있습니다.\n\\[ cost(W,b) = \\frac{1}{m}\\sum_{i=1}^{m}(H(x^{(i)}) - y^{(i)})^2 \\]\n여기서 우리의 목적은 cost(오차)를 최소화 할 수 있는 W,b의 쌍을 찾는 것입니다. 오차를 최소화 하는 값을 찾기 위해, Gradient Descent를 통해 확인해보도록 하겠습니다.\n\n초기 값 W,b에 대한 각각의 편미분 값을 구합니다.\n편미분 값에 learning rate \\(\\alpha\\)를 곱해줍니다.\nW, b에 곱해준 값을 뺀 후 W, b 값을 업데이트 합니다.\ncost가 만족할만한 수준이 될 때 까지 1 ~ 3을 반복합니다.\n\n\\[ W := W - \\alpha\\frac{\\partial}{\\partial W}cost(W,b) \\] \\[ b := b - \\alpha\\frac{\\partial}{\\partial b}cost(W,b) \\] 예시 데이터를 아래의 스크립트를 통해 \\(Wx + b\\)를 구해보도록 하겠습니다.\n\n# 필요한 라이브러리 불러오기\nlibrary(tidyverse)\nlibrary(glue)\n\n# Gradient Descent 함수 정의\ngradient_descent_simple_lr &lt;- function(X, y, alpha = 0.005, iterations = 3000) {\n  m &lt;- length(X)\n  b &lt;- runif(1) # 절편 초기화\n  w &lt;- runif(1) # 가중치 초기화\n  \n  for (i in 1:iterations) {\n    y_hat &lt;- w * X + b\n    error &lt;- y - y_hat\n    b_gradient &lt;- sum(error) * (-2 / m)\n    w_gradient &lt;- sum(error * X) * (-2 / m)\n    b &lt;- b - alpha * b_gradient\n    w &lt;- w - alpha * w_gradient\n  }\n  \n  return(list(\"b\" = b, \"W\" = w))\n}\n\n# Gradient Descent 실행\nresult &lt;- gradient_descent_simple_lr(example$hours, example$score)\n\n# 결과 출력\n\ncat(glue(\"절편 : {result$b} \\n가중치 : {result$W}\"))\n\n절편 : 22.889648962238 \n가중치 : 6.6012872777775\n\nmod &lt;- lm(score ~ hours, example)\ncat(mod$coefficients)\n\n22.9 6.6"
  },
  {
    "objectID": "posts/dual-number/index.html",
    "href": "posts/dual-number/index.html",
    "title": "이원수와 전진자동미분",
    "section": "",
    "text": "이원수(Dual Number)란..\n이원수(dual number)는 복소수의 일반화로 볼 수 있는 수 체계입니다. 듀얼 넘버는 다음과 같은 형태로 표현됩니다:\n\\[\n\\alpha + {\\beta}{\\epsilon}\n\\]\n여기서 a와 b는 실수이며, ε는 듀얼 단위로, ε^2 = 0 (ε ≠ 0)을 만족합니다. 복소수에서 허수 단위 i와 비슷한 역할을 합니다.\n이원수를 활용한 미분은 오일러의 수학적 아이디어를 기반으로 합니다. 이원수의 성질을 이용하면, 함수의 미분을 정확하게 계산하는데 사용할 수 있습니다.\n이원수를 사용하여 함수 \\(f(x)\\)의 도함수를 구하는 방법은 다음과 같습니다:\n\n함수 \\(f(x)\\)를 이원수 형태로 확장합니다. 즉, \\(x = \\alpha + {\\beta}{\\epsilon}\\)로 설정합니다.\n함수 \\(f(x)\\)에 듀얼 넘버를 대입하여, \\(f(\\alpha + {\\beta}{\\epsilon})\\)를 계산합니다.\n계산된 결과에서 ε에 대한 계수를 찾습니다. 이 계수는 함수 \\(f(x)\\)의 도함수 값이 됩니다.\n\n예를 들어, \\(f(x)=x^2\\)를 미분해 보겠습니다.\n\\(f(x) = (\\alpha + {\\beta}{\\epsilon})^2 = \\alpha^2 + 2\\alpha\\beta\\epsilon + \\beta^2\\epsilon^2\\)\n위 식에서 \\(\\epsilon^2 = 0\\)이므로, 결과는 다음과 같습니다:\n\\(f(\\alpha + \\beta\\epsilon) = (\\alpha + {\\beta}{\\epsilon})^2 = \\alpha^2 + 2\\alpha\\beta\\epsilon\\)\n여기서 ε에 대한 계수는 2a입니다. 따라서 \\(f'(x) = 2x\\)가 됩니다. 이 결과는 기대한 대로 함수 \\(f(x)=x^2\\)의 도함수 값입니다.\n위를 일반화 할 경우, \\(f(\\alpha) + f'(\\alpha)\\cdot\\beta\\epsilon\\)으로 정리할 수 있습니다."
  },
  {
    "objectID": "posts/dummy-variables/index.html",
    "href": "posts/dummy-variables/index.html",
    "title": "N-1 더미 변수를 활용하는 이유",
    "section": "",
    "text": "Sample Data 생성\nlibrary(tidyverse)\n\nset.seed(42)\nsmp_dt &lt;- tibble(Season = rep(c(\"Spring\", \"Summer\", \"Fall\", \"Winter\"),2),\nSales = as.integer(runif(8,min = 300, max = 1000)))\n\n# A tibble: 8 × 2\n  Season Sales\n  &lt;chr&gt;  &lt;int&gt;\n1 Spring   940\n2 Summer   955\n3 Fall     500\n4 Winter   881\n5 Spring   749\n6 Summer   663\n7 Fall     815\n8 Winter   394\n\n\n직관적으로..\n단순히 생각하였을 때, n개의 더미 변수를 사용할 경우, Spring ~ Winter까지의 열 벡터를 합하면, Intercept 열이 되어, 선형독립을 만족치 못한다. 그렇기에 역행렬이 구해지지 않는 것이 당연하다고 볼 수 있다. 하기 예제로 실제로 그러한지 확인해보자.\n      Spring Summer Fall Winter Intercept Sales\n[1,]      1      0    0      0         1   940\n[2,]      0      1    0      0         1   955\n[3,]      0      0    1      0         1   500\n[4,]      0      0    0      1         1   881\n[5,]      1      0    0      0         1   749\n[6,]      0      1    0      0         1   663\n[7,]      0      0    1      0         1   815\n[8,]      0      0    0      1         1   394\n\n\nn 더미 변수의 경우..\nsmp_dt_dummy_1 &lt;-\ntibble( \n      Spring = c(1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L),\n      Summer = c(0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L),\n        Fall = c(0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L),\n      Winter = c(0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L),\n      Intercept = rep(1, 8),\n      Sales = smp_dt$Sales\n)\nsmp_dt_dummy_1_mat &lt;- smp_dt_dummy_1 |&gt; as.matrix()\nsmp_dt_dummy_1_mat\n\n      Spring Summer Fall Winter Intercept Sales\n[1,]      1      0    0      0         1   940\n[2,]      0      1    0      0         1   955\n[3,]      0      0    1      0         1   500\n[4,]      0      0    0      1         1   881\n[5,]      1      0    0      0         1   749\n[6,]      0      1    0      0         1   663\n[7,]      0      0    1      0         1   815\n[8,]      0      0    0      1         1   394\nLeast Square Formula 에 따라 \\((A^TA)^{-1} \\cdot A^Tb\\) 를 계산하고자 할 때, \\(Det(A^TA)\\)의 값이 0이 되어, 역행렬을 구할 수 없다. 에러 메시지를 통해 해당 행렬이 선형적으로 종속되어있음을 확인할 수 있다.\nsolve(t(smp_dt_dummy_1_mat[,1:5]) %*% smp_dt_dummy_1_mat[,1:5])\n %*% t(smp_dt_dummy_1_mat[,1:5]) %*% (smp_dt_dummy_1_mat[,6])\n#---------------------------------------------------------------------\nError in solve.default(t(smp_dt_dummy_1_mat[, 1:5]) %*% smp_dt_dummy_1_mat[,  : \n  Lapack routine dgesv: system is exactly singular: U[5,5] = 0\n#------------------------------------------------------------------------\ndet(t(smp_dt_dummy_1_mat[,1:5]) %*% smp_dt_dummy_1_mat[,1:5])\n[1] 0\n\n\nn-1 더미 변수의 경우..\nsmp_dt_dummy_2 &lt;-\n  tibble(\n      Spring = c(1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L),\n      Summer = c(0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L),\n        Fall = c(0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L),\n      Intercept = rep(1, 8),\n      Sales = smp_dt$Sales\n  )\n\nsmp_dt_dummy_2\n\n      Spring Summer Fall Intercept Sales\n[1,]      1      0    0         1   940\n[2,]      0      1    0         1   955\n[3,]      0      0    1         1   500\n[4,]      0      0    0         1   881\n[5,]      1      0    0         1   749\n[6,]      0      1    0         1   663\n[7,]      0      0    1         1   815\n[8,]      0      0    0         1   394\n반면 n-1 dummy variables의 경우, Least Square Formula의 계산이 제대로 된다.\nsolve(t(smp_dt_dummy_2_mat[,1:4]) %*% smp_dt_dummy_2_mat[,1:4]) %*% \nt(smp_dt_dummy_2_mat[,1:4]) %*% (smp_dt_dummy_2_mat[,5])\n\n           [,1]\nSpring    207.0\nSummer    171.5\nFall       20.0\nIntercept 637.5\n\nlm(Sales ~., smp_dt_dummy_2)\n\nCall:\nlm(formula = Sales ~ ., data = smp_dt_dummy_2)\n\nCoefficients:\n(Intercept)       Spring       Summer         Fall\n      637.5        207.0        171.5         20.0"
  },
  {
    "objectID": "posts/ensemble-learning/index.html",
    "href": "posts/ensemble-learning/index.html",
    "title": "Ensemble Learning",
    "section": "",
    "text": "Ensemble Learning은..\n앙상블 학습(Ensemble learning)은 여러 개의 기본 학습 모델(base learners)을 결합하여 더 강력한 예측 모델을 만드는 머신러닝 방법입니다. 앙상블 학습의 핵심 아이디어는 개별 모델의 예측 결과를 종합하여, 더 나은 예측 성능을 달성하고자 하는 것입니다. 앙상블 학습은 일반적으로 높은 성능과 안정성을 제공하며, 과적합(overfitting)을 방지하는 효과도 있습니다.\n앙상블 학습에는 여러 가지 전략이 있으며, 대표적인 방법으로는 다음과 같은 것들이 있습니다:\n\n배깅(Bagging, Bootstrap Aggregating): 배깅은 여러 개의 기본 학습 모델을 병렬로 훈련시키고, 그 결과를 평균(회귀) 또는 투표(분류) 방식으로 종합합니다. 학습 데이터셋을 여러 개의 부트스트랩(bootstrap) 샘플로 생성하여 각 모델을 별도로 학습시킵니다. 이를 통해 모델의 분산을 줄이고 과적합을 방지할 수 있습니다. 대표적인 예로 랜덤 포레스트(Random Forest)가 있습니다. Random Forest\n부스팅(Boosting): 부스팅은 기본 학습 모델을 순차적으로 훈련시키면서, 이전 모델의 오차를 보완하는 방식으로 작동합니다. 각 모델의 가중치를 조절하여, 오차가 큰 데이터 포인트에 더 집중하게 합니다. 이를 통해 모델의 편향을 줄이고 성능을 향상시킬 수 있습니다. 대표적인 예로 에이다부스트(AdaBoost), 그레디언트 부스팅(Gradient Boosting), XGBoost, LightGBM 등이 있습니다. Gradient Boosting\n\n앙상블 학습은 서로 다른 알고리즘 또는 하이퍼파라미터를 사용한 다양한 모델을 결합하여 전체적인 성능을 향상시키는 데에 초점을 맞춥니다. 이러한 접근 방식은 각 모델의 장점을 활용하고, 서로 다른 모델이 가진 단점을 상쇄하여 더 안정적이고 일반화된 예측을 수행할 수 있게 합니다.\n앙상블 학습의 몇 가지 주요 이점은 다음과 같습니다:\n\n성능 향상: 앙상블 학습은 여러 모델의 예측력을 결합하여 더 높은 성능의 모델을 만듭니다. 이는 각 모델이 가진 지식과 전문성을 종합하고, 다양한 문제 해결 방식을 고려할 수 있기 때문입니다.\n과적합 방지: 앙상블 학습은 여러 모델을 결합함으로써 과적합을 완화할 수 있습니다. 각 모델은 서로 다른 관점에서 데이터를 학습하므로, 단일 모델이 가진 과적합 문제를 상쇄하는 효과가 있습니다.\n일반화 능력 향상: 앙상블 학습은 다양한 모델을 통합함으로써 더 일반적인 패턴과 관계를 파악할 수 있습니다. 이로 인해 앙상블 모델은 새로운 데이터에 대한 예측 능력이 향상됩니다.\n안정성: 앙상블 학습은 여러 모델을 사용하기 때문에, 단일 모델의 결함이나 노이즈에 영향을 받지 않는 안정적인 예측을 수행할 수 있습니다."
  },
  {
    "objectID": "posts/entropy/index.html",
    "href": "posts/entropy/index.html",
    "title": "Entropy 쉽게 이해하기",
    "section": "",
    "text": "Entropy란..\n엔트로피(entropy)는 정보 이론에서 확률 변수의 불확실성을 측정하는 지표입니다. 엔트로피는 어떤 정보를 표현하기 위해 필요한 평균 비트 수를 나타내며, 이를 통해 정보의 복잡성이나 압축 가능성을 파악할 수 있습니다. 높은 엔트로피는 높은 불확실성을 의미하며, 낮은 엔트로피는 낮은 불확실성을 의미합니다.\n쉽게 설명하자면, 한 그룹에서의 놀라움의 기대값이라고 볼 수 있습니다. 여기서 놀라움이란, \\(log(\\frac{1}{p(x)})\\)로 표현됩니다. 로또에 당첨이 될 확률이 매우 낮기에, 당첨되면 놀라움이 큰 것과 같은 맥락으로 이해할 수 있습니다.\n90%의 확률로 앞면이 나오는 동전을 100번 던지는 것을 예로 들어보겠습니다:\n\n\n\n\n앞\n뒤\n\n\n\n\n확률\n0.9\n0.1\n\n\n놀라움\n0.15\n3.32\n\n\n\n100번을 던질 경우의 놀라움은 \\((0.9 \\times 100 \\times 1.5) + (0.1 \\times 100 \\times 3.32)\\) 입니다. 이 놀라움의 기대값 즉 Entropy는 \\(\\frac{(0.9 \\times 100 \\times 1.5) + (0.1 \\times 100 \\times 3.32)}{100}\\)로, 0.47입니다.\n다만, 여기서 던진 횟수는 약분이 되기에 위의 Entropy를 일반화하면 아래와 같이 표기됩니다:\n\\[\n\\sum log(\\frac{1}{p(x)}) \\cdot p(x)\n\\]"
  },
  {
    "objectID": "posts/error-metrics/index.html",
    "href": "posts/error-metrics/index.html",
    "title": "1종 오류 2종 오류 헷갈리지 않기",
    "section": "",
    "text": "1종 오류 / 2종 오류\n쉽게 설명하자면, 1종 오류는 경솔한 것 / 2종 오류는 답답한 것으로 표현할 수 있습니다.\n코로나 검사를 예를 들어봅시다. 코로나에 걸리지 않은 사람을 코로나로 진단하는 것, 즉 경솔함은 1종 오류로 판단할 수 있습니다.\n코로나에 걸린 사람을 코로나에 걸리지 않았다고 진단하는 것, 즉 답답함은 2종 오류로 판단됩니다.\n\n즉 상기 그림에서, FP는 1종 오류로 FN은 2종 오류입니다."
  },
  {
    "objectID": "posts/gpt-iterative-prompt-development/index.html",
    "href": "posts/gpt-iterative-prompt-development/index.html",
    "title": "Iterative Prompt Development",
    "section": "",
    "text": "DeepLearning.AI에서 제공하는 ChatGPT Prompt 강좌를 요약한 글입니다. 실제 OpenAI 개발자가 진행하는 강좌이기에, 단순히 이용자의 경험과 감으로 제공되는 prompt 보다 나을 것으로 예상하여 정리합니다.\n모든 예시는 GPT 4.0 API로 작성되었습니다.\n목차 :\n\n프롬프팅의 원칙\n\n명확하고 구체적인 지침 작성\n모델에게 생각할 시간 주기\n\n프롬프트 개발하기\nGPT 기능 활용\n\nSummarizing\nInferring\nTransfroming\nExpanding\n\nGPT를 활용한 Chatbot 작성\n\n이번 문서에서는 프롬프트 개발에 대해 알아보겠습니다.\n프롬프트 개발은 iterative하게 프롬프트를 분석하고 수정하는 과정으로 진행됩니다.\n\n아래의 제품의 스펙을 마케팅용 문구로 변환하는 프롬프트를 통해 프롬프트 개발을 진행해보겠습니다.\nUser\n\nYour task is to help a marketing team create a \ndescription for a retail website of a product based \non a technical fact sheet.\n\nWrite a product description based on the information \nprovided in the technical specifications wrapped by \n&lt;&gt;.\n\nTechnical specifications:\n\n&lt;OVERVIEW\n- Part of a beautiful family of mid-century inspired office furniture, \nincluding filing cabinets, desks, bookcases, meeting tables, and more.\n- Several options of shell color and base finishes.\n- Available with plastic back and front upholstery (SWC-100) \nor full upholstery (SWC-110) in 10 fabric and 6 leather options.\n- Base finish options are: stainless steel, matte black, \ngloss white, or chrome.\n- Chair is available with or without armrests.\n- Suitable for home or business settings.\n- Qualified for contract use.\n\nCONSTRUCTION\n- 5-wheel plastic coated aluminum base.\n- Pneumatic chair adjust for easy raise/lower action.\n\nDIMENSIONS\n- WIDTH 53 CM | 20.87”\n- DEPTH 51 CM | 20.08”\n- HEIGHT 80 CM | 31.50”\n- SEAT HEIGHT 44 CM | 17.32”\n- SEAT DEPTH 41 CM | 16.14”\n\nOPTIONS\n- Soft or hard-floor caster options.\n- Two choices of seat foam densities: \n medium (1.8 lb/ft3) or high (2.8 lb/ft3)\n- Armless or 8 position PU armrests \n\nMATERIALS\nSHELL BASE GLIDER\n- Cast Aluminum with modified nylon PA6/PA66 coating.\n- Shell thickness: 10 mm.\nSEAT\n- HD36 foam\n\nCOUNTRY OF ORIGIN - Italy&gt;\nChatGPT\n\nMid-Century Inspired Office Chair\n\nImmerse yourself in the elegance of mid-century design with our chic office chair. As part of an extensive range of office furniture that includes filing cabinets, desks, bookcases, and more, this chair will add a stylish touch to your workspace at home or in the office.\n\nChoose from a myriad of shell color options and base finishes, including stainless steel, matte black, gloss white, and chrome. The chair comes in two versions – with a plastic back and front upholstery (SWC-100), or a full upholstery option (SWC-110). Each version is available in 10 fabric and 6 leather choices, allowing you to customize the chair to perfectly match your interior decor.\n\nComfort is key with this chair, thanks to its 5-wheel plastic coated aluminum base and pneumatic chair adjustment, allowing easy raise and lower action for the perfect fit. Whether you prefer it with or without armrests, this chair has you covered. Additionally, you have the option of an 8-position PU armrest for even more convenience and comfort.\n\nThe chair measures 53cm (20.87\") in width, 51cm (20.08\") in depth, and 80cm (31.50\") in height. The seat height is 44cm (17.32\"), and the seat depth is 41cm (16.14\"). Select from soft or hard-floor caster options and two seat foam densities, medium (1.8 lb/ft3), or high (2.8 lb/ft3), to cater to your comfort preferences.\n\nThe shell base of the chair is crafted from cast aluminum with a modified nylon PA6/PA66 coating, ensuring robustness and durability. The shell thickness is a solid 10mm. The seat is filled with premium HD36 foam, providing excellent comfort for prolonged sitting.\n\nCrafted with finesse in Italy, this office chair effortlessly combines style, functionality, and comfort, making it a valuable addition to any work setting. Suitable for both domestic and business settings, and qualified for contract use, this chair is as versatile as it is beautiful. Upgrade your workspace today with this mid-century inspired office chair.\n전반적으로 올바르게 동작한듯 하나, 답변이 지나치게 길다는 느낌을 받았습니다.\nUse at most 50 words.를 활용하여 답변을 줄여보겠습니다.\nUser\n\nYour task is to help a marketing team create a \ndescription for a retail website of a product based \non a technical fact sheet.\n\nWrite a product description based on the information \nprovided in the technical specifications wrapped by \n&lt;&gt;.\n\nUse at most 50 words.\n\nTechnical specifications:\n&lt;~생략~&gt;\nChatGPT\n\nExperience the elegance of our Italian-made, mid-century inspired office chair. Available in multiple colors and finishes, this versatile chair offers pneumatic adjustments, optional armrests, and customizable foam density for optimal comfort. Durably constructed, it's a stylish addition to any home or business setting.\n답변이 짧아지긴 하였으나, 제품에 꼭 필요한 정보들을 놓치고 있는 것으로 보입니다.\n프롬프트를 다시 수정해보겠습니다.\nUser\n\nYour task is to help a marketing team create a \ndescription for a retail website of a product based \non a technical fact sheet.\n\nWrite a product description based on the information \nprovided in the technical specifications wrapped by \n&lt;&gt;.\n\nThe description is intended for furniture retailers, \nso should be technical in nature and focus on the \nmaterials the product is constructed from.\n\nUse at most 50 words.\n\nTechnical specifications:\n&lt;~생략~&gt;\nChatGPT\n\nThis Italian-crafted, mid-century inspired office chair is constructed with a 5-wheel aluminum base coated in plastic, a 10mm shell made of cast aluminum with a modified nylon PA6/PA66 coating, and a seat cushioned with HD36 foam. Upholstery options include 10 fabrics and 6 leathers.\n더 나아가서 웹 페이지에 해당 제품을 등록시키기 위한 HTML이 필요하다고 가정해봅시다.\nUser\n\nYour task is to help a marketing team create a \ndescription for a retail website of a product based \non a technical fact sheet.\n\nWrite a product description based on the information \nprovided in the technical specifications wrapped by \n&lt;&gt;.\n\nThe description is intended for furniture retailers, \nso should be technical in nature and focus on the \nmaterials the product is constructed from.\n\nAt the end of the description, include every 7-character \nProduct ID in the technical specification.\n\nAfter the description, include a table that gives the \nproduct's dimensions. The table should have two columns.\nIn the first column include the name of the dimension. \nIn the second column include the measurements in inches only.\n\nGive the table the title 'Product Dimensions'.\n\nFormat everything as HTML that can be used in a website. \nPlace the description in a &lt;div&gt; element.\n\nTechnical specifications:\n&lt;~생략~&gt;\n&lt;div&gt;\n    &lt;h2&gt;Mid-Century Inspired Office Furniture Collection&lt;/h2&gt;\n    &lt;p&gt;This product is part of an elegant family of mid-century inspired office furniture. The variety in the collection includes everything from filing cabinets and desks to bookcases and meeting tables.&lt;/p&gt;\n    &lt;p&gt;Our chair comes with several customization options. You can choose from various shell colors and base finishes to match your unique aesthetic. Available models include a version with a plastic back and front upholstery (SWC-100) or full upholstery (SWC-110) in 10 fabric and 6 leather options. Base finish options include stainless steel, matte black, gloss white, or chrome.&lt;/p&gt;\n    &lt;p&gt;This chair model is available with or without armrests and is perfectly suited for both home or business settings. It is even qualified for contract use, making it a versatile addition to any environment.&lt;/p&gt;\n    &lt;h3&gt;Construction and Materials&lt;/h3&gt;\n    &lt;p&gt;The chair's structure features a 5-wheel plastic coated aluminum base. Its pneumatic adjust mechanism allows for easy raise/lower action, ensuring comfortable use for a wide range of individuals. The chair is also outfitted with cast aluminum gliders with a modified nylon PA6/PA66 coating and a shell thickness of 10 mm for added durability.&lt;/p&gt;\n    &lt;p&gt;The seat is constructed using high-quality HD36 foam, offering superior comfort. We offer soft or hard-floor caster options, and two choices of seat foam densities: medium (1.8 lb/ft3) or high (2.8 lb/ft3). You also have the choice between an armless version or a model with 8 position PU armrests.&lt;/p&gt;\n    &lt;p&gt;All of our products are proudly made in Italy, ensuring the highest standards of craftsmanship and material quality.&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Product IDs:&lt;/strong&gt; SWC-100, SWC-110&lt;/p&gt;\n    &lt;h3&gt;Product Dimensions&lt;/h3&gt;\n    &lt;table&gt;\n        &lt;tr&gt;&lt;th&gt;Dimension&lt;/th&gt;&lt;th&gt;Measurement (inches)&lt;/th&gt;&lt;/tr&gt;\n        &lt;tr&gt;&lt;td&gt;Width&lt;/td&gt;&lt;td&gt;20.87\"&lt;/td&gt;&lt;/tr&gt;\n        &lt;tr&gt;&lt;td&gt;Depth&lt;/td&gt;&lt;td&gt;20.08\"&lt;/td&gt;&lt;/tr&gt;\n        &lt;tr&gt;&lt;td&gt;Height&lt;/td&gt;&lt;td&gt;31.50\"&lt;/td&gt;&lt;/tr&gt;\n        &lt;tr&gt;&lt;td&gt;Seat Height&lt;/td&gt;&lt;td&gt;17.32\"&lt;/td&gt;&lt;/tr&gt;\n        &lt;tr&gt;&lt;td&gt;Seat Depth&lt;/td&gt;&lt;td&gt;16.14\"&lt;/td&gt;&lt;/tr&gt;\n    &lt;/table&gt;\n&lt;/div&gt;"
  },
  {
    "objectID": "posts/gpt-principles-of-prompting-1/index.html",
    "href": "posts/gpt-principles-of-prompting-1/index.html",
    "title": "Principles of Prompting 1",
    "section": "",
    "text": "DeepLearning.AI에서 제공하는 ChatGPT Prompt 강좌를 요약한 글입니다. 실제 OpenAI 개발자가 진행하는 강좌이기에, 단순히 이용자의 경험과 감으로 제공되는 prompt 보다 나을 것으로 예상하여 정리합니다.\n모든 예시는 GPT 4.0 API로 작성되었습니다.\n목차 :\n이번 문서에서는 프롬프트 작성의 1 원칙과 원칙에 따라 작성하기 위한 몇 가지 팁을 소개하도록 하겠습니다."
  },
  {
    "objectID": "posts/gpt-principles-of-prompting-1/index.html#프롬프트-원칙",
    "href": "posts/gpt-principles-of-prompting-1/index.html#프롬프트-원칙",
    "title": "Principles of Prompting 1",
    "section": "프롬프트 원칙",
    "text": "프롬프트 원칙\n\n원칙 1: 명확하고 구체적인 지침을 작성하세요.\n원칙 2: 모델에게 생각할 시간을 주세요.\n\n\n명확하고 구체적인 지침을 작성하는 요령\n\n1. 구분 기호를 사용하여 입력의 뚜렷한 부분을 명확하게 표시하세요\n아래 예시와 같이, 요약이 필요할 때, 요약이 필요한 문장을 &lt;&gt;로 묶어서 필요한 부분만 요약할 수 있습니다.\nUser\nSummarize the text Parenthetical senteces by &lt;&gt; into a single sentence.\n\n&lt;You should express what you want a model to do by \\ \nproviding instructions that are as clear and \\ \nspecific as you can possibly make them. \\ \nThis will guide the model towards the desired output, \\ \nand reduce the chances of receiving irrelevant \\ \nor incorrect responses. Don't confuse writing a \\ \nclear prompt with writing a short prompt. \\ \nIn many cases, longer prompts provide more clarity \\ \nand context for the model, which can lead to \\ \nmore detailed and relevant outputs.&gt;\nChatGPT\nTo achieve desired outputs and minimize irrelevant or incorrect responses, you should provide a model with clear and specific instructions, remembering that longer prompts often offer more clarity and context, leading to more detailed and relevant results.\n\n\n2. 특정한 구조의 출력을 요청하세요\n아래의 예시와 같이 JSON, HTML과 같은 유형의 출력을 요청하여 결과를 얻을 수 있습니다.\nUser\nGenerate a list of three made-up book titles along \\ \nwith their authors and genres. \nProvide them in JSON format with the following keys: \nbook_id, title, author, genre.\n[\n  {\n    \"book_id\": \"1\",\n    \"title\": \"Eclipse of Shadows\",\n    \"author\": \"Liam S. Hartfield\",\n    \"genre\": \"Fantasy\"\n  },\n  {\n    \"book_id\": \"2\",\n    \"title\": \"Quantum Echoes\",\n    \"author\": \"Dr. Eleanor Vasquez\",\n    \"genre\": \"Science Fiction\"\n  },\n  {\n    \"book_id\": \"3\",\n    \"title\": \"Beneath The Silent City\",\n    \"author\": \"Martin O'Leary\",\n    \"genre\": \"Historical Fiction\"\n  }\n]\n\n\n3. 모델에게 조건이 충족되었는지 확인하도록 요청하세요\n아래 예시는 제가 제공한 문단에 대해 모델에게 지시 순서를 요청하는 프롬프트입니다. 제공한 문단에 지시가 있을 경우와 없을 경우에 대한 프롬프트를 제공하여, 좀 더 명확한 결과를 얻을 수 있습니다.\nUser\nYou will be provided with text wrapped by &lt;&gt;. \nIf it contains a sequence of instructions, \\ \nre-write those instructions in the following format:\n\nStep 1 - ...\nStep 2 - …\n…\nStep N - …\n\nIf the text does not contain a sequence of instructions, \\ \nthen simply write \\\"No steps provided.\\\"\n\n\n&lt;Making a cup of tea is easy! First, you need to get some \\ \nwater boiling. While that's happening, \\ \ngrab a cup and put a tea bag in it. Once the water is \\ \nhot enough, just pour it over the tea bag. \\ \nLet it sit for a bit so the tea can steep. After a \\ \nfew minutes, take out the tea bag. If you \\ \nlike, you can add some sugar or milk to taste. \\ \nAnd that's it! You've got yourself a delicious \\ \ncup of tea to enjoy.&gt;\nChatGPT\nStep 1 - Get some water boiling.\nStep 2 - Grab a cup and put a tea bag in it.\nStep 3 - Once the water is hot enough, pour it over the tea bag.\nStep 4 - Let the tea steep for a few minutes.\nStep 5 - After steeping, remove the tea bag.\nStep 6 - If desired, add sugar or milk to taste.\nStep 7 - Enjoy your delicious cup of tea.\nUser\nYou will be provided with text wrapped by &lt;&gt;. \nIf it contains a sequence of instructions, \\ \nre-write those instructions in the following format:\n\nStep 1 - ...\nStep 2 - …\n…\nStep N - …\n\nIf the text does not contain a sequence of instructions, \\ \nthen simply write \\\"No steps provided.\\\"\n\n&lt;The sun is shining brightly today, and the birds are \\\nsinging. It's a beautiful day to go for a \\ \nwalk in the park. The flowers are blooming, and the \\ \ntrees are swaying gently in the breeze. People \\ \nare out and about, enjoying the lovely weather. \\ \nSome are having picnics, while others are playing \\ \ngames or simply relaxing on the grass. It's a \\ \nperfect day to spend time outdoors and appreciate the \\ \nbeauty of nature.&gt;\nChatGPT\nNo steps provided.\n\n\n4. 모델에게 Few-shot Learning을 제공하세요\n모델에게 내가 원하는 사례를 보여주고, 질문을 하면 원하는 결과를 얻을 수 있습니다.\nUser\nYour task is to answer in a consistent style.\n\n&lt;child&gt;: Teach me about patience.\n\n&lt;grandparent&gt;: The river that carves the deepest \\ \nvalley flows from a modest spring; the \\ \ngrandest symphony originates from a single note; \\ \nthe most intricate tapestry begins with a solitary thread.\n\n&lt;child&gt;: Teach me about resilience.\nChatGPT\n&lt;grandparent&gt;: The mightiest oak in the forest was once a little nut that stood its ground;  \nthe highest mountain withstands the harshest weather, yet remains unyielding; \nthe most enduring diamond begins its journey as a humble piece of coal."
  },
  {
    "objectID": "posts/gpt-principles-of-prompting-2/index.html",
    "href": "posts/gpt-principles-of-prompting-2/index.html",
    "title": "Principles of Prompting 2",
    "section": "",
    "text": "DeepLearning.AI에서 제공하는 ChatGPT Prompt 강좌를 요약한 글입니다. 실제 OpenAI 개발자가 진행하는 강좌이기에, 단순히 이용자의 경험과 감으로 제공되는 prompt 보다 나을 것으로 예상하여 정리합니다.\n모든 예시는 GPT 4.0 API로 작성되었습니다.\n목차 :\n이번 문서에서는 프롬프트 작성의 2 원칙과 원칙에 따라 작성하기 위한 몇 가지 팁을 소개하도록 하겠습니다."
  },
  {
    "objectID": "posts/gpt-principles-of-prompting-2/index.html#프롬프트-원칙",
    "href": "posts/gpt-principles-of-prompting-2/index.html#프롬프트-원칙",
    "title": "Principles of Prompting 2",
    "section": "프롬프트 원칙",
    "text": "프롬프트 원칙\n\n원칙 1: 명확하고 구체적인 지침을 작성하세요.\n원칙 2: 모델에게 생각할 시간을 주세요.\n\n\n모델에게 생각할 시간을 주는 요령\n\n1. 모델에게 과제 수행을 위한 단계를 제공하세요\n단순히 주어진 문장을 프랑스어로 요약 후, 문장의 등장인물이 누구였는지 묻는 것 보다,\n요약 -&gt; 번역 -&gt; 인물 찾기라는 절차를 제공하여 더 정확한 결과를 얻을 수 있습니다.\nUser\nPerform the following actions: \n1 - Summarize the following text wrapped by &lt;&gt;\\\nwith 1 sentence.\n2 - Translate the summary into French.\n3 - List each name in the French summary.\n4 - Output a json object that contains the following \\\nkeys: french_summary, num_names.\n\nSeparate your answers with line breaks.\n\n&lt;In a charming village, siblings Jack and Jill set out on \\ \na quest to fetch water from a hilltop \\ \nwell. As they climbed, singing joyfully, misfortune \\ \nstruck—Jack tripped on a stone and tumbled \\ \ndown the hill, with Jill following suit. \\ \nThough slightly battered, the pair returned home to \\ \ncomforting embraces. Despite the mishap, \\ \ntheir adventurous spirits remained undimmed, and they \\ \ncontinued exploring with delight.&gt;\nChatGPT\n1 - Sentence summary:\nIn a charming village, siblings Jack and Jill went on a quest to fetch water from a hilltop well,\nbut they tripped and fell, yet despite this mishap, their adventurous spirits remained undeterred.\n\n2 - French translation:\nDans un charmant village, les frères et sœurs Jack et Jill sont partis en quête pour chercher de l'eau à un puits en haut d'une colline, \nmais ils ont trébuché et sont tombés, cependant, malgré cet incident, leur esprit aventureux est resté indéfectible.\n\n3 - Names in the French summary:\n\nJack\nJill\n4 - JSON output:\n\n{\n  \"french_summary\": \"Dans un charmant village, les frères et sœurs Jack et Jill sont partis en quête pour chercher de l'eau à un puits en haut d'une colline, mais ils ont trébuché et sont tombés, cependant, malgré cet incident, leur esprit aventureux est resté indéfectible.\",\n  \"num_names\": 2\n}\n\n\n2. 결론을 내리기 전에 모델 스스로 해결책을 찾도록 지시하세요\nGPT는 언어생성 모델이기에 수학 문제에는 매우 취약한 결과를 보입니다.\nUser\nDetermine if the student's solution is correct or not.\n\nQuestion:\nI'm building a solar power installation and I need \\\n help working out the financials. \n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost \\ \nme a flat $100k per year, and an additional $10 / square \\\nfoot\nWhat is the total cost for the first year of operations \nas a function of the number of square feet.\n\nStudent's Solution:\nLet x be the size of the installation in square feet.\nCosts:\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\nChatGPT\nThe student's solution is correct.\nThe total cost for the first year of operations is indeed 450x + 100,000,\nwhere x is the size of the installation in square feet.\nThe student correctly included all costs: land, solar panels, and maintenance.\n위의 사례와 같이, Maintenance Cost가 \\(100000 + 10x\\)가 올바른 답임에도 불구하고,\n제가 제공한 프롬프트가 올바른 정답이라고 답하고 있습니다.\n모델에게 우선 답을 찾고, 제가 제공한 답과 비교하게 함으로서 이를 예방할 수 있습니다.\nYour task is to determine if the student's solution \\\nis correct or not.\nTo solve the problem do the following:\n- First, work out your own solution to the problem. \n- Then compare your solution to the student's solution \\ \nand evaluate if the student's solution is correct or not. \nDon't decide if the student's solution is correct until \nyou have done the problem yourself.\n\nUse the following format:\nQuestion:\n\n&lt;question here&gt;\n\nStudent's solution:\n\n&lt;student's solution here&gt;\n\nActual solution:\n\n&lt;steps to work out the solution and your solution here&gt;\n\nIs the student's solution the same as actual solution \\\njust calculated:\n\n&lt;yes or no&gt;\n\nStudent grade:\n\n&lt;correct or incorrect&gt;\n\n\nQuestion:\n\n&lt;I'm building a solar power installation and I need help \\\nworking out the financials. \n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost \\\nme a flat $100k per year, and an additional $10 / square \\\nfoot\nWhat is the total cost for the first year of operations \\\nas a function of the number of square feet.&gt;\n\nStudent's solution:\n\n&lt;Let x be the size of the installation in square feet.\nCosts:\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000&gt;\n\nActual solution:\nChatGPT\n\nLet's denote the size of the installation in square feet as x.\n\nCosts:\n1. Land cost: $100 * x\n2. Solar panel cost: $250 * x\n3. Maintenance cost: $100,000 + $10 * x\n\nSo, the total cost for the first year of operations is:\nLand cost + Solar panel cost + Maintenance cost\n\nTherefore, we have:\nTotal cost = $100 * x + $250 * x + $100,000 + $10 * x\n           = $360 * x + $100,000\n           \nIs the student's solution the same as actual solution just calculated:\nNo\n\nStudent grade:\nIncorrect"
  },
  {
    "objectID": "posts/gpt-prompt/index.html",
    "href": "posts/gpt-prompt/index.html",
    "title": "GPT 템플릿",
    "section": "",
    "text": "GPT에게 할 수 있는 Command\n\nKeep going : 컨티뉴보다 이쪽이 더 확실\nMake a list of : 목록을 작성하라. 아래쪽의 ‘쓰다’ 만큼 넓은 범위를 참조함. 글머리 기호 붙여서 알기 쉽게 정리해줌.\nExplain : 모든 주제에 대한 간단한 설명을 얻기 위해 사용\nImprove : 컨텐츠를 제공하고 GPT에 정확도를 향상시키거나 컨텐츠를 추가하도록 요청\nPlan : 목표를 더 작은 단계로 분류하도록 요청\nSummarize : 요약하도록 요청\nExpand : 이전 답변에 대한 자세한 내용을 설명하도록 요청. 또는 내가 위나 아래에 제공한 문장 또는 장면을 더 자세히 풀어서 설명해줌.\n\n\n\n언어 모델이 선택한 최적의 프롬프트\n\n“Let’s work this out in a step by step way to be sure we have the right answer.”\n출처: [2211.01910] Large Language Models Are Human-Level Prompt Engineers\n\n\nGPT 코딩\nAct as CODEX (“COding DEsign eXpert”), an expert coder with experience in multiple coding languages. Always follow the coding best practices by writing clean, modular code with proper security measures and leveraging design patterns. You can break down your code into parts whenever possible to avoid breaching the chatgpt output character limit. Write code part by part when I send “continue”. If you reach the character limit, I will send “continue” and then you should continue without repeating any previous code. Do not assume anything from your side; please ask me a numbered list of essential questions before starting. If you have trouble fixing a bug, ask me for the latest code snippets for reference from the official documentation. I am using [MacOS], [VSCode] and prefer [brew] package manager. Start a conversation as “CODEX: Hi, what are we coding today?”\n출처: GPT-4 CODEX: Coding Design Expert; A Secret Prompt To Rule Them All | by 𝚃𝚑𝚎 𝙻𝚊𝚝𝚎𝚜𝚝 𝙽𝚘𝚠 ~ 𝙰𝙸 | Mar, 2023 | Artificial Intelligence in Plain English"
  },
  {
    "objectID": "posts/gpt-summarize/index.html",
    "href": "posts/gpt-summarize/index.html",
    "title": "Summarise using GPT",
    "section": "",
    "text": "DeepLearning.AI에서 제공하는 ChatGPT Prompt 강좌를 요약한 글입니다. 실제 OpenAI 개발자가 진행하는 강좌이기에, 단순히 이용자의 경험과 감으로 제공되는 prompt 보다 나을 것으로 예상하여 정리합니다.\n모든 예시는 GPT 4.0 API로 작성되었습니다.\n목차 :\n\n프롬프팅의 원칙\n\n명확하고 구체적인 지침 작성\n모델에게 생각할 시간 주기\n\n프롬프트 개발하기\nGPT 기능 활용\n\nSummarizing\nInferring\nTransfroming\nExpanding\n\nGPT를 활용한 Chatbot 작성\n\n이번 문서에서는 GPT를 활용하여 요약을 하는 방법들에 대해 알아보겠습니다.\n다음과 같은 제품 리뷰를 요약한다고 생각해봅시다.\n&lt;Got this panda plush toy for my daughter's birthday, \\\nwho loves it and takes it everywhere. It's soft and \\ \nsuper cute, and its face has a friendly look. It's \\ \na bit small for what I paid though. I think there \\ \nmight be other options that are bigger for the \\ \nsame price. It arrived a day earlier than expected, \\ \nso I got to play with it myself before I gave it \\ \nto her.&gt;\n요약될 문장을 특정 문자 수 이내로 제한하거나, 요약할 내용을 어디에 중점을 둘 것인지 정할 수 있습니다.\nUser\n\nYour task is to generate a short summary of a product \\\nreview from an ecommerce site. \n\nSummarize the review below, wrapped by &lt;&gt; \nin at most 30 words. \n\nReview: &lt;~생략~&gt;\nChatGPT\n\nThe reviewer is pleased with the panda plush toy's softness, cuteness, and friendly look. However, they believe it's smaller than its price suggests. It arrived earlier than expected.\n리뷰를 30 단어 이내로 잘 요약하였습니다.\nUser\n\nYour task is to generate a short summary of a product \\\nreview from an ecommerce site to give feedback to the \\\npricing deparmtment, responsible for determining the \\\nprice of the product.  \n\nSummarize the review below, wrapped by &lt;&gt; \nin at most 30 words, and focusing on any aspects \\\nthat are relevant to the price and perceived value. \n\nReview: &lt;~생략~&gt;\nChatGPT\n\nThe panda plush toy is adored, soft, and cute but perceived as slightly small for its price point. Despite early delivery, the customer suggests better value alternatives may exist.\n이전과 달리 가격에 중점을 두고 요약을 한 결과입니다.\n여러가지 리뷰를 표로 요약해보겠습니다.\nUser\n\nYour task is to generate a short summary of a product \\\nreview from an ecommerce site. \n\nSummarize the review below, wrapped by \"\"\" in at most 30 words. \n\nShare result as table, including [type of product, summary of review, sentiment of review]\n\n\"\"\"\nNeeded a nice lamp for my bedroom, and this one \\\nhad additional storage and not too high of a price \\\npoint. Got it fast - arrived in 2 days. The string \\\nto the lamp broke during the transit and the company \\\nhappily sent over a new one. Came within a few days \\\nas well. It was easy to put together. Then I had a \\\nmissing part, so I contacted their support and they \\\nvery quickly got me the missing piece! Seems to me \\\nto be a great company that cares about their customers \\\nand products. \n\"\"\"\n\n \"\"\"\nMy dental hygienist recommended an electric toothbrush, \\\nwhich is why I got this. The battery life seems to be \\\npretty impressive so far. After initial charging and \\\nleaving the charger plugged in for the first week to \\\ncondition the battery, I've unplugged the charger and \\\nbeen using it for twice daily brushing for the last \\\n3 weeks all on the same charge. But the toothbrush head \\\nis too small. I’ve seen baby toothbrushes bigger than \\\nthis one. I wish the head was bigger with different \\\nlength bristles to get between teeth better because \\\nthis one doesn’t.  Overall if you can get this one \\\naround the $50 mark, it's a good deal. The manufactuer's \\\nreplacements heads are pretty expensive, but you can \\\nget generic ones that're more reasonably priced. This \\\ntoothbrush makes me feel like I've been to the dentist \\\nevery day. My teeth feel sparkly clean! \n\"\"\"\n\n \"\"\"\nSo, they still had the 17 piece system on seasonal \\\nsale for around $49 in the month of November, about \\\nhalf off, but for some reason (call it price gouging) \\\naround the second week of December the prices all went \\\nup to about anywhere from between $70-$89 for the same \\\nsystem. And the 11 piece system went up around $10 or \\\nso in price also from the earlier sale price of $29. \\\nSo it looks okay, but if you look at the base, the part \\\nwhere the blade locks into place doesn’t look as good \\\nas in previous editions from a few years ago, but I \\\nplan to be very gentle with it (example, I crush \\\nvery hard items like beans, ice, rice, etc. in the \\ \nblender first then pulverize them in the serving size \\\nI want in the blender then switch to the whipping \\\nblade for a finer flour, and use the cross cutting blade \\\nfirst when making smoothies, then use the flat blade \\\nif I need them finer/less pulpy). Special tip when making \\\nsmoothies, finely cut and freeze the fruits and \\\nvegetables (if using spinach-lightly stew soften the \\ \nspinach then freeze until ready for use-and if making \\\nsorbet, use a small to medium sized food processor) \\ \nthat you plan to use that way you can avoid adding so \\\nmuch ice if at all-when making your smoothie. \\\nAfter about a year, the motor was making a funny noise. \\\nI called customer service but the warranty expired \\\nalready, so I had to buy another one. FYI: The overall \\\nquality has gone done in these types of products, so \\\nthey are kind of counting on brand recognition and \\\nconsumer loyalty to maintain sales. Got it in about \\\ntwo days.\n\"\"\"\n\n\n\n\n\n\n\n\nType of Product\nSummary of Review\nSentiment of Review\n\n\n\n\nBedroom Lamp\nThe lamp was affordable, arrived promptly, and had additional storage. The customer had issues with a broken string and a missing part, but customer service quickly resolved both issues.\nPositive\n\n\nElectric Toothbrush\nThe electric toothbrush has a long-lasting battery life and makes teeth feel very clean. However, the head of the toothbrush is too small. The customer finds the manufacturer’s replacement heads expensive but suggests cheaper generic ones.\nMixed\n\n\nBlender System\nThe blender system was bought at a discount, but its prices later increased significantly. The quality isn’t as good as earlier models, and the motor had issues after a year. Customer also shares tips on how to use it.\nMixed"
  },
  {
    "objectID": "posts/gradient-boosting/index.html",
    "href": "posts/gradient-boosting/index.html",
    "title": "Gradient Boosting",
    "section": "",
    "text": "Gradient Boosting이란..\n그래디언트 부스팅(Gradient Boosting)은 앙상블 학습 기법 중 하나로, 일련의 약한 학습기(보통 결정 트리)를 순차적으로 학습시켜 강력한 모델을 만드는 방법입니다. 그래디언트 부스팅은 손실 함수의 그래디언트(기울기)를 최소화하는 방향으로 모델을 업데이트하는 과정을 반복하며, 이를 통해 각 학습기가 이전 학습기의 오차를 줄이는 방향으로 학습됩니다.\n그래디언트 부스팅의 주요 단계는 다음과 같습니다:\n\n초기 예측: 모든 데이터 포인트에 대해 동일한 초기 예측 값을 설정합니다. 이 값은 일반적으로 평균(회귀) 또는 가장 빈번한 클래스(분류)로 설정됩니다.\n약한 학습기 학습: 첫 번째 약한 학습기(일반적으로 결정 트리)를 학습시키고, 예측 오차를 계산합니다.\n손실 함수 및 그래디언트 계산: 손실 함수(예: 평균 제곱 오차, 로그 손실 등)를 사용하여 예측 오차를 측정하고, 그래디언트를 계산합니다.\n새로운 약한 학습기 학습: 이전 학습기의 오차를 줄이는 방향으로 새로운 약한 학습기를 학습시킵니다. 이 과정은 그래디언트에 음의 가중치를 부여하여 이루어집니다.\n학습기 결합: 모든 약한 학습기의 예측을 가중치를 적용하여 결합합니다. 이렇게 하여 최종 예측이 개선됩니다.\n수렴 여부 확인: 손실 함수 값이 더 이상 개선되지 않거나 지정된 반복 횟수에 도달할 때까지 단계 2-5를 반복합니다.\n\n그래디언트 부스팅은 다양한 문제에서 높은 성능을 보이며, 회귀와 분류 문제 모두에 적용할 수 있습니다. XGBoost, LightGBM 및 CatBoost와 같은 그래디언트 부스팅 구현은 대용량 데이터셋에서도 빠르게 학습되도록 최적화되어 있으며, 고성능 그래디언트 부스팅 모델을 생성하는 데 널리 사용됩니다.\n\n\nHistGradientBoosting 예제\nr\n# load required library\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(reticulate)\n\n\npython\n# load required library\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n\n\npython\n# Split Data\ndata = load_breast_cancer()\nX, y = data.data, data.target\ncolname_x = data.feature_names\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\npython\n# Init Model\nmodel = HistGradientBoostingClassifier(random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n\nr\n# Calc Accuracy\nX_test &lt;- as_tibble(py$X_test,.name_repair)\ny_test &lt;- tibble(py$y_test)\ny_pred = tibble(py$y_pred)\n\nresult &lt;- X_test |&gt; bind_cols(y_test,y_pred)\ncolnames(result) &lt;- c(py$colname_x,\"truth\",\"estimate\")\nresult$truth &lt;- as.factor(result$truth)\nresult$estimate &lt;- as.factor(result$estimate)\n\nyardstick::accuracy(result,truth,estimate)\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.974"
  },
  {
    "objectID": "posts/hierarchical/index.html",
    "href": "posts/hierarchical/index.html",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Hierarchical Clustering은..\n데이터 포인트 간의 거리를 기반으로 클러스터를 형성하는데, 처음에는 각 데이터 포인트가 하나의 클러스터로 간주되며 가장 가까운 클러스터끼리 병합하는 과정을 반복하면서 최종적으로 하나의 클러스터로 합쳐집니다. 계층적 클러스터링은 덴드로그램(dendrogram)을 사용하여 클러스터 구조를 시각화할 수 있습니다.\n\n\nHierachical Clustring의 병합 방법 (Linkage Method)\n\n완전 연결법 (Complete Linkage): 클러스터 간 거리를 두 클러스터에 속하는 모든 데이터 포인트 쌍의 거리 중 최대값으로 정의하는 방법입니다. 이 방법은 비교적 밀집되고 잘 구분된 클러스터를 형성하는 경향이 있습니다. (method = “complete”)\n단일 연결법 (Single Linkage): 클러스터 간 거리를 두 클러스터에 속하는 모든 데이터 포인트 쌍의 거리 중 최소값으로 정의하는 방법입니다. 이 방법은 길고 늘어진 체인 형태의 클러스터를 형성할 수 있으며, 노이즈에 민감한 특성이 있습니다. (method = “single”)\n평균 연결법 (Average Linkage): 클러스터 간 거리를 두 클러스터에 속하는 모든 데이터 포인트 쌍의 거리의 평균값으로 정의하는 방법입니다. 이 방법은 노이즈에 상대적으로 덜 민감하며, 단일 연결법에 비해 더 균형 잡힌 클러스터를 형성하는 경향이 있습니다. (method = “average”)\n중심 연결법 (Centroid Linkage): 클러스터 간 거리를 두 클러스터의 중심(centroid, 즉 평균 벡터) 간의 거리로 정의하는 방법입니다. 이 방법은 보다 밀집된 클러스터를 형성하는 경향이 있지만, 거리 척도의 선택에 민감한 특성이 있습니다. (method = “centroid”)\n\n\n\nHierarchical Clustering 예제\n\n\n\nResult of Hierachical clustering\n\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(cluster)\n\n# Extract numeric columns\niris_numeric &lt;- iris[, 1:4]\n\n# Standardize the data\niris_std &lt;- scale(iris_numeric)\n\n# Compute Euclidean distance matrix\ndist_matrix &lt;- dist(iris_std, method = \"euclidean\")\n\n# Perform hierarchical clustering\nhc &lt;- hclust(dist_matrix, method = \"complete\")\n\n# Plot dendrogram\nplot(hc, labels = iris$Species, cex = 0.6, hang = -1)\n\n# Cut the tree into clusters\nk &lt;- 3 # Number of clusters\ncluster_assignments &lt;- cutree(hc, k)\n\n# Add cluster assignments to the Iris dataset\n\niris &lt;- iris |&gt; mutate(Cluster = case_when(\n  cluster_assignments == 1 ~ \"setosa\",\n  cluster_assignments == 2 ~ \"versicolor\",\n  .default = \"virginica\"\n))\n\niris$Cluster &lt;- as.factor(iris$Cluster)\n\niris |&gt; yardstick::conf_mat(Species, Cluster)\niris |&gt; yardstick::accuracy(Species, Cluster)\nTruth\nPrediction   setosa versicolor virginica\n  setosa         49          0         0\n  versicolor      1         21         2\n  virginica       0         29        48\n\naccuracy multiclass     0.787"
  },
  {
    "objectID": "posts/intro-to-tensorflow/index.html",
    "href": "posts/intro-to-tensorflow/index.html",
    "title": "2023 MIT 6.S191 01 Intro to TensorFlow",
    "section": "",
    "text": "2023년 MIT에서 강의된 6.S191 강좌의 Lab의이해를 돕기위해 작성되었습니다.\nLab에 대해서는 이곳에서 확인가능하며, 해당 강좌는 여기서 확인가능합니다.\n\nLab1-Part1 Intro to Tensorflow\nLab1-Part2 Music Generation\nLab2-Part1 MNIST\nLab2-Part2 Diagnosing Bias\nLab3-Part1 Introduction Caspa\nLab3-Part2 Bias and Uncertainty\n\n\n0.1 Load Required Library\n%tensorflow_version 2.x\nimport tensorflow as tf\n\n# Download and import the MIT Introduction to Deep Learning package\n!pip install mitdeeplearning\nimport mitdeeplearning as mdl\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n1.1 왜 TensorFlow는 TensorFlow인가요?\nTensorFlow는 다차원 배열로 생각할 수 있는 데이터 구조인 텐서의 흐름(노드/수학적 연산)을 처리하기 때문에 ’TensorFlow’라고 불립니다.\n텐서는 문자열이나 정수와 같은 기본 데이터 타입의 n차원 배열로 표현되며, 벡터와 행렬을 더 높은 차원으로 일반화할 수 있는 방법을 제공합니다.\n텐서의 형태는 차원 수 (ndim)와 각 차원의 크기(rank)를 정의합니다. 먼저 Scalar를 예로 0-d 텐서를 살펴보겠습니다:\nsport = tf.constant(\"Tennis\", tf.string)\nnumber = tf.constant(1.41421356237, tf.float64)\n\nprint(f\"`sport` is a {tf.rank(sport).numpy()}-d Tensor\")\nprint(f\"`number` is a {tf.rank(number).numpy()}-d Tensor\")\n`sport` is a 0-d Tensor\n`number` is a 0-d Tensor\n벡터와 리스트는 1-d 텐서입니다.\nsports = tf.constant([\"Tennis\", \"Basketball\"], tf.string)\nnumbers = tf.constant([3.141592, 1.414213, 2.71821], tf.float64)\n\nprint(f\"`sports` is a {tf.rank(sports).numpy()}-d Tensor\")\nprint(f\"`numbers` is a {tf.rank(numbers).numpy()}-d Tensor\")\n`sports` is a 1-d Tensor\n`numbers` is a 1-d Tensor\n다음으로 2차원 텐서 (행렬) 및 고차원 텐서를 생성해 봅시다.\n예를 들어, 향후 이미지 처리 및 컴퓨터 비전과 관련된 실습에서는 4-d 텐서를 사용하게 될 것입니다.\n여기서 차원은 배치에 포함된 예제 이미지의 수, 이미지 높이, 이미지 너비, 컬러 채널 수에 해당합니다.\nmatrix = tf.constant([[1,4],[3,2]],tf.float64)\nimages = tf.zeros([10,256,256,3],tf.float64) # 10 images with RGB color, 256 x 256 pixels\nprint(tf.rank(images).numpy().tolist())\nprint(tf.shape(images).numpy().tolist())\n4\n[10, 256, 256, 3]\n보시다시피 shape는 각 텐서 차원에 있는 요소의 수를 제공합니다.\n이 함수는 매우 유용하며 자주 사용하게 될 것입니다.\n\n\nTensor의 계산\nTensorFlow에서 계산을 생각하고 시각화하는 편리한 방법은 그래프로 표현하는 것입니다.\n그래프는 데이터를 저장하는 텐서와 이러한 텐서에 일정한 순서로 작용하는 수학적 연산으로 정의할 수 있습니다.\n간단한 예시를 통해 TensorFlow를 사용하여 이 계산을 정의해 보겠습니다:\n\n# Node A와 B를 정의하고 값을 초기화 합니다.\na = tf.constant(15)\nb = tf.constant(61)\n\n# A와 B를 더합니다\nc1 = tf.add(a,b)\nc2 = a + b # TensorFLow는 `+`를 오버라이드 하여 `c1`과 같이 동작하게 합니다.\nprint(c1)\nprint(c2)\ntf.Tensor(76, shape=(), dtype=int32)\ntf.Tensor(76, shape=(), dtype=int32)\n텐서플로 연산으로 구성된 계산 그래프를 생성하고 연산을 실행하여 결과를 반환한 결과 값이 76인 텐서를 생성한 것을 보았습니다.\n이제 조금 더 복잡한 예를 살펴보겠습니다:\n\n여기서는 두 개의 입력 a, b를 받아 출력 e를 계산합니다.\n그래프의 각 노드는 일부 입력을 받아 일부 연산을 수행한 후 그 출력을 다른 노드로 전달하는 연산을 나타냅니다.\n이 계산 함수를 구성하는 간단한 함수를 텐서플로에서 정의해 보겠습니다:\n### Defining Tensor computations ###\n\n# Construct a simple computation function\ndef func(a,b):\n  c = tf.add(a,b)\n  d = tf.subtract(b,1)\n  e = tf.multiply(c,d)\n  return e\n\n# Consider example values for a,b\na, b = 1.5, 2.5\n# Execute the computation\ne_out = func(a,b)\nprint(e_out)\ntf.Tensor(6.0, shape=(), dtype=float32)\n\n\n1.3 TensorFlow에서의 Neural Network\n드디어 뉴럴 네트워크를 구현할 차례입니다!\nTensorFlow에서는 Keras라는 딥러닝 모델을 위한 직관적인 High level API를 제공합니다. 먼저 하나의 Dense Layer를 가진 single perceptron($ y=σ(Wx+b)$)을 구현해보도록 해보겠습니다. 여기서 W는 행렬의 weights, b는 bias, x는 input, σ는 activation 함수, y는 output을 의미합니다.\n\n### Defining a network Layer ###\n\n# n_output_nodes: number of output nodes\n# input_shape: shape of the input\n# x: input to the layer\n\nclass OurDenseLayer(tf.keras.layers.Layer):\n  def __init__(self, n_output_nodes):\n    super(OurDenseLayer, self).__init__()\n    self.n_output_nodes = n_output_nodes\n\n  def build(self, input_shape):\n    d = int(input_shape[-1])\n    # Define and initialize parameters: a weight matrix W and bias b\n    # Note that parameter initialization is random!\n    self.W = self.add_weight(\"weight\", shape=[d, self.n_output_nodes]) # note the dimensionality\n    self.b = self.add_weight(\"bias\", shape=[1, self.n_output_nodes]) # note the dimensionality\n\n  def call(self, x):\n    z = tf.add(tf.matmul(self.X,self.W),self.b)\n    y = tf.sigmoid(z)\n    return y\n\n# Since layer parameters are initialized randomly, we will set a random seed for reproducibility\ntf.random.set_seed(1)\nlayer = OurDenseLayer(3)\nlayer.build((1,2))\nx_input = tf.constant([[1,2.]], shape=(1,2))\ny = layer.call(x_input)\n\n# test the output!\nprint(y.numpy())\n[[0.45985997 0.646087   0.245385  ]]\n편리하게도 텐서플로에는 신경망에서 일반적으로 사용되는 여러 레이어가 정의되어 있습니다.\n이제 단일 레이어를 사용하여 간단한 신경망을 정의하는 대신 Keras의 Sequential 모델과 단일 Dense 레이어를 사용하여 네트워크를 정의하겠습니다.\nSequential API를 사용하면 빌딩 블록처럼 레이어를 쌓아서 신경망을 쉽게 만들 수 있습니다.\n\n### Defining a neural network using the Sequential API ###\n\n# Import relevant packages\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\nn_output_nodes = 3\nmodel = Sequential()\ndense_layer = Dense(n_output_nodes, activation='sigmoid')\nmodel.add(dense_layer)\n\nx_input = tf.constant([[1,2.]], shape=(1,2))\nprint(model(x_input).numpy())\ntf.Tensor([[0.6124562  0.91317874 0.8520293 ]], shape=(1, 3), dtype=float32)\nSequential모델을 정의하는 대신에 Model class의 sub-class를 만들어 활용할 수 있습니다.\n이를 통하여 커스텀 레이어, 커스텀 트레이닝, 커스텀 activation을 유연하게 정의할 수 있습니다.\n### Defining a model using subclassing ###\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense\n\nclass SubclassModel(tf.keras.Model):\n\n  # In __init__, we define the Model's layers\n  def __init__(self, n_output_nodes):\n    super(SubclassModel, self).__init__()\n    '''TODO: Our model consists of a single Dense layer. Define this layer.''' \n    self.dense_layer = Dense(n_output_nodes, activation='sigmoid')\n\n  # In the call function, we define the Model's forward pass.\n  def call(self, inputs):\n    return self.dense_layer(inputs)\nn_output_nodes = 3\nmodel = SubclassModel(n_output_nodes)\n\nx_input = tf.constant([[1,2.]], shape=(1,2))\n\nprint(model.call(x_input))\ntf.Tensor([[0.09956823 0.34443256 0.8754808 ]], shape=(1, 3), dtype=float32)\n\n\n1.4 TensorFlow의 자동 미분\n자동 미분은 TensorFlow에서 가장 중요한 부분 중 하나이며 역전파를 이용한 훈련의 근간입니다.\n미분을 계산하기 위해 tf.GradientTape를 사용하겠습니다.\n네트워크를 통해 Forward Pass가 이루어지면 모든 연산이 “테이프”에 기록되고, 미분을 계산하기 위해 테이프가 역방향으로 재생됩니다.\n기본적으로 테이프는 역방향으로 재생된 후 폐기되므로 특정 tf.GradientTape는 하나의 미분만 계산할 수 있으며 이후 호출은 런타임 오류를 발생시킵니다.\n하지만 persistent gradient tape를 생성하면 동일한 계산을 통해 여러 미분을 계산할 수 있습니다.\n먼저 GradientTape를 사용해 미분을 계산하고 계산을 위해 액세스하는 방법을 살펴보겠습니다. 간단한 함수 \\(y=x^2\\)를 정의하고 미분을 계산합니다:\n### Gradient computation with GradientTape ###\n\n# y = x^2\n# Example: x = 3.0\nx = tf.Variable(3.0)\n\n# Initiate the gradient tape\nwith tf.GradientTape() as tape:\n  # Define the function\n  y = x * x\n# Access the gradient -- derivative of y with respect to x\ndy_dx = tape.gradient(y, x)\n\nprint(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\n뉴럴 네트워크를 훈련할 때에는 loss 함수를 최소화 하기위해 SGD를 활용하여 미분합니다.\n다음 예시에서는 Loss 함수를 SGD를 활용하여 최소화해보도록 하겠습니다.\n\\(L = (x- x_f)^2\\)\n### Function minimization with automatic differentiation and SGD ###\n\n# Initialize a random value for our initial x\nx = tf.Variable([tf.random.normal([1])])\nprint(\"Initializing x={}\".format(x.numpy()))\n\nlearning_rate = 1e-2 # learning rate for SGD\nhistory = []\n# Define the target value\nx_f = 4\n\n# We will run SGD for a number of iterations. At each iteration, we compute the loss, \n#   compute the derivative of the loss with respect to x, and perform the SGD update.\nfor i in range(500):\n  with tf.GradientTape() as tape:\n    loss = (x-x_f)**2\n\n  # loss minimization using gradient tape\n  grad = tape.gradient(loss, x) # compute the derivative of the loss with respect to x\n  new_x = x - learning_rate*grad # sgd update\n  x.assign(new_x) # update the value of x\n  history.append(x.numpy()[0])\n\n# Plot the evolution of x as we optimize towards x_f!\nplt.plot(history)\nplt.plot([0, 500],[x_f,x_f])\nplt.legend(('Predicted', 'True'))\nplt.xlabel('Iteration')\nplt.ylabel('x value')"
  },
  {
    "objectID": "posts/k-means/index.html",
    "href": "posts/k-means/index.html",
    "title": "K-means clustering",
    "section": "",
    "text": "image of kmeans\n\n\n\nK-means 알고리즘은..\nCentroid Based Clustering 기법 중 하나로, 데이터 포인트들을 K개의 군집으로 나누는 비지도 학습(unsupervised learning) 방법입니다. K-means 알고리즘은 다음과 같은 과정을 거쳐 데이터를 군집화합니다.\n\n초기화: K개의 초기 중심점(centroid)을 무작위로 선택하거나, 데이터 포인트에서 무작위로 추출합니다.\n할당: 각 데이터 포인트를 가장 가까운 중심점에 할당하여, K개의 군집을 생성합니다. 일반적으로 유클리디안 거리(Euclidean distance)를 사용하여 거리를 측정합니다.\n업데이트: 각 군집의 중심점을 새롭게 계산합니다. 새로운 중심점은 해당 군집에 속한 데이터 포인트들의 평균 위치입니다.\n수렴 여부 확인: 중심점의 위치가 더 이상 변하지 않거나, 미리 정한 반복 횟수에 도달할 때까지 2단계와 3단계를 반복합니다.\n\nK-means 알고리즘은 간단하고 이해하기 쉬워 널리 사용되지만, 몇 가지 단점도 가지고 있습니다. 주요 단점으로는 다음과 같습니다.\n\nK값을 미리 설정해야 하는데, 이는 최적의 군집 개수를 알기 어려운 경우 문제가 될 수 있습니다.\n초기 중심점 선택에 따라 결과가 달라질 수 있어, 여러 번 실행하여 최적의 결과를 찾아야 할 수도 있습니다.\n군집의 모양이 원형이 아닌 경우나 군집 크기가 다른 경우, 성능이 저하될 수 있습니다.\n\n이러한 단점에도 불구하고 K-means 알고리즘은 군집화에 사용되는 대표적인 방법 중 하나로 널리 적용되고 있습니다.\nlibrary(cluster)\n\niris_data &lt;- iris[, -5]\n\n# Apply K-means (K=3)\nset.seed(42)\nkmeans_result &lt;- kmeans(iris_data, centers = 3, nstart = 20)\n\ntable(iris$Species, kmeans_result$cluster)\nclusplot(iris_data, kmeans_result$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)\n              1  2  3\n  setosa     50  0  0\n  versicolor  0 48  2\n  virginica   0 14 36"
  },
  {
    "objectID": "posts/kelly-criterion/index.html",
    "href": "posts/kelly-criterion/index.html",
    "title": "켈리 기준",
    "section": "",
    "text": "켈리 기준이란..\n켈리 기준(Kelly Criterion)은 투자와 도박에서 최적의 배당을 결정하기 위한 공식입니다. 이 공식은 1956년에 John Larry Kelly Jr.에 의해 개발되었습니다. 켈리 기준의 목표는 장기적인 자본 가치 증가를 최대화하는 배당금을 찾는 것입니다.\n켈리 공식은 다음과 같습니다:\n\\[ f* = \\frac{p}{a} - \\frac{q}{b} \\]\n여기서:\n\\(f*\\)는 최적의 배당 비율을 나타냅니다.\n\\(a\\)는 순손해률입니다.\n\\(b\\)는 순이익률입니다.\n\\(p\\)는 이길 확률입니다.\n\\(q\\)는 질 확률이며, \\((1 - p)\\)로 계산할 수 있습니다.\n켈리 기준은 투자자가 얼마나 많은 돈을 투자하거나 얼마나 많은 돈을 배팅해야 하는지 결정하는 데 도움이 됩니다. 이 공식을 사용하면 장기적으로 수익을 극대화하고 리스크를 최소화할 수 있습니다.\n\n\n켈리 기준 유도\n\\[ r = (1+fb)^p \\times (1-fa)^q \\]\n위의 식에 로그를 취하면,\n\\[ logr = p log(1+fb) + q log(1 - fa) \\]\n위의 식을 f에 대해 미분하면,\n\\[ 0 = \\frac{pb}{(1+fb)} - \\frac{qa}{(1-fa)}\\]\n\\[ pb \\cdot (1-fa) = qa \\cdot (1+fb) \\]\n이를 정리하면, 하기와 같이 켈리 기준식이 나옵니다.\n\\[ f* = \\frac{p}{a} - \\frac{q}{b} \\]\n\n\n예시\n승률이 60%인 게임에서 이기면 내기 금액만큼 벌고, 지면 내기 금액 모두를 잃는 게임을 가정해봅시다.\n켈리 기준에 따르면, 전체 금액의 20%을 투자 하였을 때 수익을 극대화 할 수 있습니다.\n그렇다면, 1000원의 초기자금으로 1000번 게임했을 때, 전체 금액의 10% ~ 100%까지 투자하였을 경우의 기대 수익을 시뮬레이션해보도록 하겠습니다.\n\nset.seed(42)\n\nnum_games &lt;- 1000\nbet_ratio &lt;- seq(0.1,1,by = 0.1)\nresult &lt;- numeric(10)\n\nfor(r in 1:10){\nratio &lt;- bet_ratio[r]\nmoney &lt;- 1000\nfor(i in 1:num_games){\n    random_number &lt;- runif(1)\n    if(random_number &lt;0.6) {\n        money &lt;- money + (money*ratio)\n    } else {\n        money &lt;- money - (money*ratio)\n    }\n}\n\nresult[r] &lt;- money\n}\n\nprint(result)\n\n [1]  3.444046e+11  3.703831e+11  4.379182e+05  3.782560e-06  1.354241e-20\n [6]  5.060881e-32  7.664663e-65 8.607885e-111 6.451605e-228  0.000000e+00\n\n\n시뮬레이션에서도 볼 수 있듯이, 20%의 비율로 배팅하였을 때 가장 큰 수익을 얻을 수 있음을 알수 있습니다."
  },
  {
    "objectID": "posts/kl-divergence/index.html",
    "href": "posts/kl-divergence/index.html",
    "title": "Kullback-Leibler Divergence 쉽게 이해하기",
    "section": "",
    "text": "본 문서를 읽기 앞서, Entropy에 대한 이해가 필요합니다. 먼저 읽고 오시길 권장드립니다.\n\nKL Divergence란..\nKL Divergence(Kullback-Leibler Divergence)는 실제 모델과 예측 분포 간의 차이를 측정하는 방법입니다.\nKL Divergence는 또한 정보 이론에서 상대 엔트로피로 알려져 있습니다.\n\n\nKL Divergence의 간단한 예시\n수식을 살펴보기 앞서, 간단한 예시로 두 환률 분포 간의 차이를 측정해보겠습니다.\n이전 Entropy문서를 읽으셨다면, Entropy의 공식이 하기와 같음을 기억하실 것입니다. 또한 Entropy의 의미가 놀라움의 평균인 것도 함께 기억하실 것입니다.\n\\[\n\\sum log(\\frac{1}{p(x)}) \\cdot p(x)\n\\]\n한번 6면체 주사위를 생각해봅시다.\n6면체 주사위의 실제 Entropy는 \\(6 \\times log(\\frac{1}{1/6}) \\cdot 1/6 \\simeq 1.792\\) 입니다.\n하지만 제가 실험을 잘못 설계해서 이 주사위의 확률 값을 잘못 계산했다고 가정해봅시다.\n1 ~ 6 까지의 확률은 (0.1,0.2,0.1,0.2,0.2,0.2) 입니다. 이를 q(x)라고 하겠습니다.\n그렇다면, 제가 예측한 놀라움과 실제 확률을 통해 Entropy의 계산 역시 가능할 것입니다.(Cross Entropy) \\(\\sum log(\\frac{1}{q(x)}) \\cdot 1/6 \\simeq 1.84\\)\n이 두 값의 차, 0.048이 두 분포의 차, KL Divergence의 값입니다. 이제 KL Divergence의 수식을 보도록 합시다.\n\n\nKL Divergence 수식\n위키피디아의 쿨백-라이블러 발산의 글에 따르면 이산 확률에서의 쿨백-라이블러 발산은 하기와 같습니다:\n\\[D_{KL}(P||Q) = \\sum_i p(i) log \\frac{P(i)}{Q(i)} \\]\n우리가 위에서 계산했던 식을 일반화 한다면, \\[ \\sum log(\\frac{1}{q(x)}) \\cdot p(x) - \\sum log(\\frac{1}{p(x)}) \\cdot p(x) \\]\np(x)로 묶는다면,\n\\[ \\sum p(x)  (log(\\frac{1}{q(x)}) -  log(\\frac{1}{p(x)})) \\]\nlog를 정리하면, 우리가 원했던 식이 나옵니다.\n\\[ \\sum p(x) log(\\frac{p(x)}{q(x)}) \\]"
  },
  {
    "objectID": "posts/knapsack/index.html",
    "href": "posts/knapsack/index.html",
    "title": "배낭 문제",
    "section": "",
    "text": "개요\nortools를 사용하여 배낭 문제를 해결하는 예제를 살펴보겠습니다.\n일반적인 애플리케이션은 상자를 배송 트럭에 효율적으로 로드하는 것입니다.\n\n\n\n문제 설정\n850의 무게를 담을 수 있는 컨테이너와 50가지 항목의 아이템들이 있습니다.\n아이템은 값과 무게로 이루어져있습니다.\nvalues = [\n    360, 83, 59, 130, 431, 67, 230, 52, 93, 125, 670, 892, 600, 38, 48, 147,\n    78, 256, 63, 17, 120, 164, 432, 35, 92, 110, 22, 42, 50, 323, 514, 28,\n    87, 73, 78, 15, 26, 78, 210, 36, 85, 189, 274, 43, 33, 10, 19, 389, 276,\n    312\n]\nweights = [[\n    7, 0, 30, 22, 80, 94, 11, 81, 70, 64, 59, 18, 0, 36, 3, 8, 15, 42, 9, 0,\n    42, 47, 52, 32, 26, 48, 55, 6, 29, 84, 2, 4, 18, 56, 7, 29, 93, 44, 71,\n    3, 86, 66, 31, 65, 0, 79, 20, 65, 52, 13\n]]\ncapacities = [850]\n\n\n문제 해결\nfrom ortools.algorithms import pywrapknapsack_solver\n\nsolver = pywrapknapsack_solver.KnapsackSolver(\n    pywrapknapsack_solver.KnapsackSolver.\n    KNAPSACK_MULTIDIMENSION_BRANCH_AND_BOUND_SOLVER, 'KnapsackExample')\n\nsolver.Init(values, weights, capacities)\ncomputed_value = solver.Solve()\npacked_items = []\npacked_weights = []\ntotal_weight = 0\nprint('Total value =', computed_value)\nfor i in range(len(values)):\n    if solver.BestSolutionContains(i):\n        packed_items.append(i)\n        packed_weights.append(weights[0][i])\n        total_weight += weights[0][i]\n\nprint('Total weight:', total_weight)\nprint('Packed items:', packed_items)\nprint('Packed_weights:', packed_weights)\nTotal value = 7534\nTotal weight: 850\nPacked items: [0, 1, 3, 4, 6, 10, 11, 12, 14, 15, 16, 17, 18, 19, 21, 22, 24, 27, 28, 29, 30, 31,\n               32, 34, 38, 39, 41, 42, 44, 47, 48, 49]\nPacked_weights: [7, 0, 22, 80, 11, 59, 18, 0, 3, 8, 15, 42, 9, 0, 47, 52, 26, 6, 29, 84, 2, 4,\n                 18, 7, 71, 3, 66, 31, 0, 65, 52, 13]"
  },
  {
    "objectID": "posts/knapsack-n-bags/index.html",
    "href": "posts/knapsack-n-bags/index.html",
    "title": "배낭 문제 (여러 배낭)",
    "section": "",
    "text": "개요\nortools를 사용하여 여 배낭 문제를 해결하는 예제를 살펴보겠습니다.\n\n\n문제 설정\ndata = {}\ndata['weights'] = [\n    48, 30, 42, 36, 36, 48, 42, 42, 36, 24, 30, 30, 42, 36, 36\n]\ndata['values'] = [\n    10, 30, 25, 50, 35, 30, 15, 40, 30, 35, 45, 10, 20, 30, 25\n]\nassert len(data['weights']) == len(data['values'])\ndata['num_items'] = len(data['weights'])\ndata['all_items'] = range(data['num_items'])\n\ndata['bin_capacities'] = [100, 100, 100, 100, 100]\ndata['num_bins'] = len(data['bin_capacities'])\ndata['all_bins'] = range(data['num_bins'])\n\n\n문제 해결\nsolver = pywraplp.Solver.CreateSolver('SCIP')\n\n# x[i, b] = 1 if item i is packed in bin b.\nx = {}\nfor i in data['all_items']:\n    for b in data['all_bins']:\n        x[i, b] = solver.BoolVar(f'x_{i}_{b}')\n        \n# Each item is assigned to at most one bin.\nfor i in data['all_items']:\n    solver.Add(sum(x[i, b] for b in data['all_bins']) &lt;= 1)\n\n# The amount packed in each bin cannot exceed its capacity.\nfor b in data['all_bins']:\n    solver.Add(\n        sum(x[i, b] * data['weights'][i]\n            for i in data['all_items']) &lt;= data['bin_capacities'][b])\n            \n# Maximize total value of packed items.\nobjective = solver.Objective()\nfor i in data['all_items']:\n    for b in data['all_bins']:\n        objective.SetCoefficient(x[i, b], data['values'][i])\nobjective.SetMaximization()\n\nstatus = solver.Solve()\nif status == pywraplp.Solver.OPTIMAL:\n    print(f'Total packed value: {objective.Value()}')\n    total_weight = 0\n    for b in data['all_bins']:\n        print(f'Bin {b}')\n        bin_weight = 0\n        bin_value = 0\n        for i in data['all_items']:\n            if x[i, b].solution_value() &gt; 0:\n                print(\n                    f\"Item {i} weight: {data['weights'][i]} value: {data['values'][i]}\"\n                )\n                bin_weight += data['weights'][i]\n                bin_value += data['values'][i]\n        print(f'Packed bin weight: {bin_weight}')\n        print(f'Packed bin value: {bin_value}\\n')\n        total_weight += bin_weight\n    print(f'Total packed weight: {total_weight}')\nelse:\n    print('The problem does not have an optimal solution.')\nTotal packed value: 395.0\nBin 0\nItem 4 weight: 36 value: 35\nItem 9 weight: 24 value: 35\nItem 14 weight: 36 value: 25\nPacked bin weight: 96\nPacked bin value: 95\n\nBin 1\nItem 1 weight: 30 value: 30\nItem 8 weight: 36 value: 30\nItem 10 weight: 30 value: 45\nPacked bin weight: 96\nPacked bin value: 105\n\nBin 2\nItem 2 weight: 42 value: 25\nItem 5 weight: 48 value: 30\nPacked bin weight: 90\nPacked bin value: 55\n\nBin 3\nItem 7 weight: 42 value: 40\nItem 13 weight: 36 value: 30\nPacked bin weight: 78\nPacked bin value: 70\n\nBin 4\nItem 3 weight: 36 value: 50\nItem 12 weight: 42 value: 20\nPacked bin weight: 78\nPacked bin value: 70\n\nTotal packed weight: 438"
  },
  {
    "objectID": "posts/linear-optimization/index.html",
    "href": "posts/linear-optimization/index.html",
    "title": "생산계획",
    "section": "",
    "text": "개요\nortools를 사용하여 선형 프로그래밍 문제를 해결하는 예제로 생산 계획 최적화 문제를 살펴보겠습니다.\n이 예제에서는 두 가지 제품을 생산하는 공장이 있으며, 각 제품에는 일정한 이익이 있습니다.\n공장은 제한된 자원을 사용하여 이익을 최대화하려고 합니다.\n\n\n문제 설정\n\n제품 A와 제품 B를 생산할 수 있으며, 각각 $20, $40의 이익을 얻을 수 있습니다.\n\n제품 A는 자원 1을 3 단위, 자원 2를 2 단위 사용합니다.\n\n제품 B는 자원 1을 2 단위, 자원 2를 5 단위 사용합니다.\n\n공장은 자원 1을 30 단위, 자원 2를 40 단위 사용할 수 있습니다.\n\n목표: 이익을 최대화하면서 제한된 자원을 사용하여 제품 A와 B의 최적 생산량을 찾으세요.\n\n\n문제 해결\nfrom ortools.linear_solver import pywraplp\n\n# 선형 프로그래밍 솔버 생성\nsolver = pywraplp.Solver.CreateSolver('GLOP')\n\n# 변수 생성 (정수형의 경우 IntVar)\nproduct_a = solver.NumVar(0, solver.infinity(), 'Product_A')\nproduct_b = solver.NumVar(0, solver.infinity(), 'Product_B')\n\n\n# 제약 조건 추가\nconstraint1 = solver.Constraint(0, 30, 'Resource_1')\nconstraint1.SetCoefficient(product_a, 3)\nconstraint1.SetCoefficient(product_b, 2)\n\nconstraint2 = solver.Constraint(0, 40, 'Resource_2')\nconstraint2.SetCoefficient(product_a, 2)\nconstraint2.SetCoefficient(product_b, 5)\n\n# 목적 함수 정의 및 최대화\nobjective = solver.Objective()\nobjective.SetCoefficient(product_a, 20)\nobjective.SetCoefficient(product_b, 40)\nobjective.SetMaximization()\n\n# 문제 해결\nstatus = solver.Solve()\n\n# 결과 출력\nif status == pywraplp.Solver.OPTIMAL:\n    print('Objective value =', objective.Value())\n    print('Product A =', product_a.solution_value())\n    print('Product B =', product_b.solution_value())\nelse:\n    print('The problem does not have an optimal solution.')\nObjective value = 345.4545454545455\nProduct A = 6.363636363636365\nProduct B = 5.454545454545454"
  },
  {
    "objectID": "posts/mit-6s191/index.html",
    "href": "posts/mit-6s191/index.html",
    "title": "2023 MIT 6.S191 Deep Learning",
    "section": "",
    "text": "2023년 MIT에서 강의된 6.S191 강좌의 Lab의이해를 돕기위해 작성되었습니다.\nLab에 대해서는 이곳에서 확인가능하며, 해당 강좌는 여기서 확인가능합니다.\n\nLab1-Part1 Intro to Tensorflow\nLab1-Part2 Music Generation\nLab2-Part1 MNIST\nLab2-Part2 Diagnosing Bias\nLab3-Part1 Introduction Caspa\nLab3-Part2 Bias and Uncertainty"
  },
  {
    "objectID": "posts/mnist/index.html",
    "href": "posts/mnist/index.html",
    "title": "2023 MIT 6.S191 03 MNIST",
    "section": "",
    "text": "2023년 MIT에서 강의된 6.S191 강좌의 Lab의이해를 돕기위해 작성되었습니다.\nLab에 대해서는 이곳에서 확인가능하며, 해당 강좌는 여기서 확인가능합니다.\n\nLab1-Part1 Intro to Tensorflow\nLab1-Part2 Music Generation\nLab2-Part1 MNIST\nLab2-Part2 Diagnosing Bias\nLab3-Part1 Introduction Caspa\nLab3-Part2 Bias and Uncertainty\n\n이번 과제에서는 CNN 모델을 통해 손으로 쓰여진 0 ~ 9 의 글씨를 예측하는 모형을 만듭니다.\n60,000 개의 train 이미지와 10,000 개의 test 이미지로 이루어진 MNIST 데이터 셋을 활용할 것입니다.\n# Import Tensorflow 2.0\n%tensorflow_version 2.x\nimport tensorflow as tf \n\n!pip install mitdeeplearning\nimport mitdeeplearning as mdl\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom tqdm import tqdm\n\n# Check that we are using a GPU, if not switch runtimes\n#   using Runtime &gt; Change Runtime Type &gt; GPU\nassert len(tf.config.list_physical_devices('GPU')) &gt; 0\n\n3.1 MNIST 데이터\n데이터를 다운받고, 어떻게 이루어져있는지 확인하도록 하겠습니다.\nmnist = tf.keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n# convert dim from (n_imgs x 28 x 28) to (n_imgs x 28 x 28 x 1) to make channel\ntrain_images = (np.expand_dims(train_images, axis=-1)/255.).astype(np.float32)\ntrain_labels = (train_labels).astype(np.int64)\ntest_images = (np.expand_dims(test_images, axis=-1)/255.).astype(np.float32)\ntest_labels = (test_labels).astype(np.int64)\nplt.figure(figsize=(10,10))\nrandom_inds = np.random.choice(60000,36)\nfor i in range(36):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image_ind = random_inds[i]\n    plt.imshow(np.squeeze(train_images[image_ind]), cmap=plt.cm.binary)\n    plt.xlabel(train_labels[image_ind])\n\n\n\n3.2 MLP 구조의 모델\nCNN 모델을 만들기 앞서 MLP 구조의 모델을 먼저 만들어 보겠습니다.\n하기의 이미지와 같이 이미지는 총 784 (28 x 28)의 픽셀을 가지고 있습니다.\n이 784개의 input을 128개의 hidden layer로 통과시킨 뒤 10개 (0 ~ 9)의 output layer로 출력해보겠습니다.\n\ndef build_fc_model():\n  fc_model = tf.keras.Sequential([\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(128, activation = 'relu'),\n      tf.keras.layers.Dense(10, activation = 'softmax')\n      \n  ])\n  return fc_model\n\nmodel = build_fc_model()\n\n# configure loss and opimizer\nmodel.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-1), \n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n\nBATCH_SIZE = 64\nEPOCHS = 5\n\n# train model\nmodel.fit(train_images, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS)           \nEpoch 1/5\n938/938 [==============================] - 8s 3ms/step - loss: 0.3753 - accuracy: 0.8948\nEpoch 2/5\n938/938 [==============================] - 3s 3ms/step - loss: 0.1977 - accuracy: 0.9432\nEpoch 3/5\n938/938 [==============================] - 3s 3ms/step - loss: 0.1493 - accuracy: 0.9573\nEpoch 4/5\n938/938 [==============================] - 3s 3ms/step - loss: 0.1200 - accuracy: 0.9654\nEpoch 5/5\n938/938 [==============================] - 3s 3ms/step - loss: 0.1012 - accuracy: 0.9715\n&lt;keras.callbacks.History at 0x7f5b20e0ae60&gt;\n대략 train 데이터에서 97% 정도의 정확도를 보이는 것을 알 수 있습니다.\n이제 test 데이터에서의 결과를 확인해봅시다.\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('Test accuracy:', test_acc)\n313/313 [==============================] - 1s 3ms/step - loss: 0.1048 - accuracy: 0.9696\nTest accuracy: 0.9696000218391418\ntest 데이터에서의 결과가 약간 낮음을 알 수 있습니다. 이렇게 train 데이터와 test 데이터에서 차이가 날 때, overfitting되었다고 합니다.\n이제 CNN 모델을 만들어봅시다.\n\n\n3.3 CNN 구조의 모델\n하기와 같은 구조의 모델을 만들어 보겠습니다.\n\ndef build_cnn_model():\n    cnn_model = tf.keras.Sequential([\n\n        tf.keras.layers.Conv2D(32, (3, 3), padding = 'same', activation = tf.nn.relu), \n        tf.keras.layers.MaxPool2D(pool_size = (2, 2)),\n\n        tf.keras.layers.Conv2D(64, (3, 3), padding = 'same', activation = tf.nn.relu),\n        tf.keras.layers.MaxPool2D(pool_size = (2, 2)),\n\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n\n        # TODO: Define the last Dense layer to output the classification \n        # probabilities. Pay attention to the activation needed a probability\n        # output\n        tf.keras.layers.Dense(10, activation = tf.nn.softmax)\n    ])\n    \n    return cnn_model\n  \ncnn_model = build_cnn_model()\n# Initialize the model by passing some data through\ncnn_model.predict(train_images[[0]])\n# Print the summary of the layers in the model.\nprint(cnn_model.summary())\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 28, 28, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n 2D)                                                             \n                                                                 \n flatten_1 (Flatten)         (None, 3136)              0         \n                                                                 \n dense_2 (Dense)             (None, 128)               401536    \n                                                                 \n dense_3 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 421,642\nTrainable params: 421,642\nNon-trainable params: 0\n_________________________________________________________________\ncnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\ncnn_model.fit(train_images, train_labels, batch_size = BATCH_SIZE, epochs = EPOCHS)\nEpoch 1/5\n938/938 [==============================] - 8s 5ms/step - loss: 0.1493 - accuracy: 0.9546\nEpoch 2/5\n938/938 [==============================] - 5s 5ms/step - loss: 0.0447 - accuracy: 0.9860\nEpoch 3/5\n938/938 [==============================] - 7s 8ms/step - loss: 0.0297 - accuracy: 0.9905\nEpoch 4/5\n938/938 [==============================] - 5s 5ms/step - loss: 0.0217 - accuracy: 0.9930\nEpoch 5/5\n938/938 [==============================] - 4s 4ms/step - loss: 0.0165 - accuracy: 0.9948\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\n\nprint('Test accuracy:', test_acc)\n313/313 [==============================] - 1s 2ms/step - loss: 0.1048 - accuracy: 0.9696\nTest accuracy: 0.9696000218391418\n이제 CNN 모델을 통해 예측을 해보겠습니다.\npredictions = cnn_model.predict(test_images)\npredictions[0]\narray([4.6900855e-10, 5.6617080e-09, 3.1779661e-09, 3.1454579e-08,\n       4.5877988e-13, 7.0875771e-11, 3.0195982e-18, 9.9999988e-01,\n       5.5570687e-11, 1.1235415e-07], dtype=float32)\n10개의 output layer로 모델을 정의하였기에, 10개의 요소를 포함한 array가 출력됩니다.\nsoftmax는 여러개의 출력값을 확률적으로 나타내어 주기에, np.argmax를 통하여 어떤 값의 확률이 가장 큰지 알 수 있습니다.\nprediction = np.argmax(predictions[0])\n\nprint(prediction)\n7\n정말로 7이 맞는지 확인해 봅시다.\nprint(\"Label of this digit is:\", test_labels[0])\nplt.imshow(test_images[0,:,:,0], cmap=plt.cm.binary)\n\n좀 더 다양한 결과들을 확인해보겠습니다.\n\n\n\n1.4 GradientTape를 사용한 CNN\n1.3에서의 예시와 같이 모델의 optimizer와 loss를 사용할 경우 매우 편리하고 직관적이지만, 모델에 대한 통제력이 떨어집니다.\n이에 대한 대안으로 tf.GradientTape.gradient를 활용해보겠습니다.\n### 이전의 방식\ncnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\ncnn_model.fit(train_images, train_labels, batch_size = BATCH_SIZE, epochs = EPOCHS)\n### 새로운 방식\ncnn_model = build_cnn_model()\n\nbatch_size = 12\nloss_history = mdl.util.LossHistory(smoothing_factor=0.95) # to record the evolution of the loss\nplotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss', scale='semilogy')\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-2) # define our optimizer\n\nif hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n\nfor idx in tqdm(range(0, train_images.shape[0], batch_size)):\n  # First grab a batch of training data and convert the input images to tensors\n  (images, labels) = (train_images[idx:idx+batch_size], train_labels[idx:idx+batch_size])\n  images = tf.convert_to_tensor(images, dtype=tf.float32)\n\n  # GradientTape to record differentiation operations\n  with tf.GradientTape() as tape:\n    logits = cnn_model(images)\n    loss_value = tf.keras.backend.sparse_categorical_crossentropy(labels, logits)\n\n  loss_history.append(loss_value.numpy().mean()) # append the loss to the loss_history record\n  plotter.plot(loss_history.get())\n\n  # Backpropagation\n  grads = tape.gradient(loss_value, cnn_model.trainable_variables)\n  optimizer.apply_gradients(zip(grads, cnn_model.trainable_variables))"
  },
  {
    "objectID": "posts/music-generation/index.html",
    "href": "posts/music-generation/index.html",
    "title": "2023 MIT 6.S191 02 Music Generation with RNNs",
    "section": "",
    "text": "2023년 MIT에서 강의된 6.S191 강좌의 Lab의이해를 돕기위해 작성되었습니다.\nLab에 대해서는 이곳에서 확인가능하며, 해당 강좌는 여기서 확인가능합니다.\n\nLab1-Part1 Intro to Tensorflow\nLab1-Part2 Music Generation\nLab2-Part1 MNIST\nLab2-Part2 Diagnosing Bias\nLab3-Part1 Introduction Caspa\nLab3-Part2 Bias and Uncertainty\n\n\n2.1 Dependencies\n우선, 본 과제를 위한 라이브러리를 설치하도록 하겠습니다\n# Import Tensorflow 2.0\n%tensorflow_version 2.x\nimport tensorflow as tf \n\n# Download and import the MIT Introduction to Deep Learning package\n!pip install mitdeeplearning\nimport mitdeeplearning as mdl\n\n# Import all remaining packages\nimport numpy as np\nimport os\nimport time\nimport functools\nfrom IPython import display as ipythondisplay\nfrom tqdm import tqdm\n!apt-get install abcmidi timidity &gt; /dev/null 2&gt;&1\n\n# Check that we are using a GPU, if not switch runtimes\n#   using Runtime &gt; Change Runtime Type &gt; GPU\nassert len(tf.config.list_physical_devices('GPU')) &gt; 0\n\n\n2.2 데이터\nABC 기보법으로 구성된 아일랜드 포크 음악의 데이터를 이용하여 본 과제를 진행하겠습니다.\n# Download the dataset\nsongs = mdl.lab1.load_training_data()\n\n# Print one of the songs to inspect it in greater detail!\nexample_song = songs[0]\nprint(\"\\nExample song: \")\nprint(example_song)\nFound 817 songs in text\n\nExample song: \nX:1\nT:Alexander's\nZ: id:dc-hornpipe-1\nM:C|\nL:1/8\nK:D Major\n(3ABc|dAFA DFAd|fdcd FAdf|gfge fefd|(3efe (3dcB A2 (3ABc|!\ndAFA DFAd|fdcd FAdf|gfge fefd|(3efe dc d2:|!\nAG|FAdA FAdA|GBdB GBdB|Acec Acec|dfaf gecA|!\nFAdA FAdA|GBdB GBdB|Aceg fefd|(3efe dc d2:|!\nABC 기보법을 음악파일로 변환 후 실행시켜 봅시다.\nmdl.lab1.play_song(example_song)\n\n\n\n한가지 유념해야할 부분은 이 데이터가 단순히 음악의 노트만을 포함한 것이 아니라 노래 제목, 키, 템포까지 포함하고 있다는 것입니다. 이러한 다양한 문자열들이 학습에 있어서 어떠한 영향을 미칠까요? 이 텍스트 데이터를 숫자로 표현하기 위한 기법도 이번 과제에서 함께 알아봅시다.\n# Join our list of song strings into a single string containing all songs\nsongs_joined = \"\\n\\n\".join(songs) \n\n# Find all unique characters in the joined string\nvocab = sorted(set(songs_joined))\nprint(\"There are\", len(vocab), \"unique characters in the dataset\")\nThere are 83 unique characters in the dataset\n\n\n2.3 데이터 프리 프로세싱\n우리는 RNN을 훈련시켜 ABC 기보법의 패턴을 학습하고자 합니다.\n그리고 이 모델을 활용하여 새로운 음악을 만들 것입니다.\n이를 고려한다면, 우리가 모델에서 얻고자 하는 것은, 문자 열이 주어졌을 때 다음 문자가 무엇인가에 대한 것입니다.\nRNN 모델을 훈련시키기 앞서 텍스트 데이터를 숫자로 표현해보겠습니다.\n이를 위해 두 개의 lookup table을 만듭니다.\n하나는 83개의 character를 number로, 다른 하나는 number를 다시 character로 변환해주는 테이블입니다.\n### Define numerical representation of text ###\n\n# Create a mapping from character to unique index.\n# For example, to get the index of the character \"d\", \n#   we can evaluate `char2idx[\"d\"]`.  \nchar2idx = {u:i for i, u in enumerate(vocab)}\n\n# Create a mapping from indices to characters. This is\n#   the inverse of char2idx and allows us to convert back\n#   from unique index to the character in our vocabulary.\nidx2char = np.array(vocab)\n이제 각 문자들은 하기와 같이 숫자와 매칭됩니다.\n{\n  '\\n':   0,\n  ' ' :   1,\n  '!' :   2,\n  '\"' :   3,\n  '#' :   4,\n  \"'\" :   5,\n  '(' :   6,\n  ')' :   7,\n  ',' :   8,\n  '-' :   9,\n  '.' :  10,\n  '/' :  11,\n  '0' :  12,\n  '1' :  13,\n  '2' :  14,\n  '3' :  15,\n  '4' :  16,\n  '5' :  17,\n  '6' :  18,\n  '7' :  19,\n  ...\n}\n이제 각 노래를 숫자로 변환시켜봅시다.\n### Vectorize the songs string ###\ndef vectorize_string(string):\n  result = []\n  for c in string:\n    result.append(char2idx[c])\n  result = np.array(result)\n  return result  \n\nvectorized_songs = vectorize_string(songs_joined)\n'X:1\\nT:Alex' ---- characters mapped to int ----&gt; [49 22 13  0 45 22 26 67 60 79]\n데이터 셋을 숫자로 변경하였으니, training example과 target을 정해봅시다.\n우리는 RNN에 seq_length 만큼의 input을 넣고 seq_length + 1 까지의 숫자를 예측해야 합니다.\n예를 들어 seq_length 가 4일 때 “hell”이라는 단어가 input이 되면, “ello”를 예측하는 것입니다.\n### Batch definition to create training examples ###\n\ndef get_batch(vectorized_songs, seq_length, batch_size):\n  # the length of the vectorized songs string\n  n = vectorized_songs.shape[0] - 1\n  # randomly choose the starting indices for the examples in the training batch\n  idx = np.random.choice(n-seq_length, batch_size)\n\n  input_batch = [vectorized_songs[i : i+seq_length] for i in idx]\n  output_batch = [vectorized_songs[i+1 : i+seq_length+1] for i in idx]\n\n\n  # x_batch, y_batch provide the true inputs and targets for network training\n  x_batch = np.reshape(input_batch, [batch_size, seq_length])\n  y_batch = np.reshape(output_batch, [batch_size, seq_length])\n  return x_batch, y_batch\n\n\n2.4 RNN 모델\n이제 RNN 모델을 훈련할 모든 준비가 완료되었습니다!\nRNN을 이용해 ABC 기보법을 훈련하여 노래를 생성해보도록 합시다.\n우리가 사용할 Layer들은 다음과 같습니다:\n\ntf.keras.layers.Embedding: 각 문자의 숫자를 차원이 있는 벡터에 매핑하는 학습 가능한 조회 테이블로 구성된 입력 레이어입니다.\ntf.keras.layers.LSTM: 크기가 있는 LSTM 네트워크입니다.\ntf.keras.layers.Dense: 출력이 있는 출력 레이어입니다.\n\n\ndef LSTM(rnn_units): \n  return tf.keras.layers.LSTM(\n    rnn_units, \n    return_sequences=True, \n    recurrent_initializer='glorot_uniform',\n    recurrent_activation='sigmoid',\n    stateful=True,\n  )\n\n### Defining the RNN Model ###\n\n'''TODO: Add LSTM and Dense layers to define the RNN model using the Sequential API.'''\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    # Layer 1: Embedding layer to transform indices into dense vectors \n    #   of a fixed embedding size\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n\n    # Layer 2: LSTM with `rnn_units` number of units. \n    # TODO: Call the LSTM function defined above to add this layer.\n    LSTM(rnn_units),\n\n    # Layer 3: Dense (fully-connected) layer that transforms the LSTM output\n    #   into the vocabulary size. \n    # TODO: Add the Dense layer.\n    tf.keras.layers.Dense(vocab_size)\n  ])\n\n  return model\n\n# Build a simple model with default hyperparameters. You will get the \n#   chance to change these later.\nmodel = build_model(len(vocab), embedding_dim=256, rnn_units=1024, batch_size=32)\nmodel.summary()\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (32, None, 256)           21248     \n                                                                 \n lstm (LSTM)                 (32, None, 1024)          5246976   \n                                                                 \n dense (Dense)               (32, None, 83)            85075     \n                                                                 \n=================================================================\nTotal params: 5,353,299\nTrainable params: 5,353,299\nNon-trainable params: 0\n_________________________________________________________________\nx, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\npred = model(x)\nprint(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\nprint(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")\nInput shape:       (32, 100)  # (batch_size, sequence_length)\nPrediction shape:  (32, 100, 83) # (batch_size, sequence_length, vocab_size)\n이제 모델 훈련을 위한 Loss 함수를 정의해보겠습니다. 이 모델은 multi label을 예측하는 classification 모델입니다.\n이런 경우 tensorflow에서 sparase_categorical_crossentropy를 사용하는 것이 적절합니다.\n### Defining the loss function ###\n\ndef compute_loss(labels, logits):\n  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n  return loss\n\nexample_batch_loss = compute_loss(y, pred)\n\nprint(\"Prediction shape: \", pred.shape, \" # (batch_size, sequence_length, vocab_size)\") \nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n모델 학습을 위한 Hyperparameter를 세팅해 봅시다.\n### Hyperparameter setting and optimization ###\n\n# Optimization parameters:\nnum_training_iterations = 2000  # Increase this to train longer\nbatch_size = 4  # Experiment between 1 and 64\nseq_length = 100  # Experiment between 50 and 500\nlearning_rate = 5e-3  # Experiment between 1e-5 and 1e-1\n\n# Model parameters: \nvocab_size = len(vocab)\nembedding_dim = 256 \nrnn_units = 1024  # Experiment between 1 and 2048\n\n# Checkpoint location: \ncheckpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")\n이제 train을 시작해봅시다. 이번 과제에서는 Adam 옵티마이저와 tf.GradientTape를 활용합니다.\n### Define optimizer and training operation ###\n\n\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate)\n\n@tf.function\ndef train_step(x, y): \n  # Use tf.GradientTape()\n  with tf.GradientTape() as tape:\n  \n    y_hat = model(x)\n  \n    loss = compute_loss(y,y_hat)\n\n  # Now, compute the gradients \n  '''TODO: complete the function call for gradient computation. \n      Remember that we want the gradient of the loss with respect all \n      of the model parameters. \n      HINT: use `model.trainable_variables` to get a list of all model\n      parameters.'''\n  grads = tape.gradient(loss, model.trainable_variables)\n  \n  # Apply the gradients to the optimizer so it can update the model accordingly\n  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n  return loss\n\n##################\n# Begin training!#\n##################\n\nhistory = []\nplotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\nif hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n\nfor iter in tqdm(range(num_training_iterations)):\n\n  # Grab a batch and propagate it through the network\n  x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\n  loss = train_step(x_batch, y_batch)\n\n  # Update the progress bar\n  history.append(loss.numpy().mean())\n  plotter.plot(history)\n\n  # Update the model with the changed weights!\n  if iter % 100 == 0:     \n    model.save_weights(checkpoint_prefix)\n    \n# Save the trained model and the weights\nmodel.save_weights(checkpoint_prefix)\n\n이제 새로운 음악을 만들어보도록 합시다. 우선 batch_size를 1로 줄여 새로운 모델을 불러옵니다.\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1) # TODO\n\n# Restore the model weights for the last checkpoint after training\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel.build(tf.TensorShape([1, None]))\n\nmodel.summary()\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_4 (Embedding)     (1, None, 256)            21248     \n                                                                 \n lstm_4 (LSTM)               (1, None, 1024)           5246976   \n                                                                 \n dense_4 (Dense)             (1, None, 83)             85075     \n                                                                 \n=================================================================\nTotal params: 5,353,299\nTrainable params: 5,353,299\nNon-trainable params: 0\n_________________________________________________________________\n이제 prediction을 해보도록 하겠습니다.\n\n### Prediction of a generated song ###\n\ndef generate_text(model, start_string, generation_length=1000):\n  # Evaluation step (generating ABC text using the learned RNN model)\n\n  # convert the start string to numbers (vectorize)\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Here batch size == 1\n  model.reset_states()\n  tqdm._instances.clear()\n\n  for i in tqdm(range(generation_length)):\n      # evaluate the inputs and generate the next character predictions\n      predictions = model(input_eval)\n      \n      # Remove the batch dimension\n      predictions = tf.squeeze(predictions, 0)\n      \n      # use a multinomial distribution to sample'''\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n      \n      # Pass the prediction along with the previous hidden state\n      #   as the next inputs to the model\n      input_eval = tf.expand_dims([predicted_id], 0)\n      \n      # add the predicted character to the generated text!\n      text_generated.append(idx2char[predicted_id]) \n    \n  return (start_string + ''.join(text_generated))\n\ngenerated_text = generate_text(model, start_string=\"X\", generation_length=1000)\ngenerated_songs = mdl.lab1.extract_song_snippet(generated_text)"
  },
  {
    "objectID": "posts/mutual-information/index.html",
    "href": "posts/mutual-information/index.html",
    "title": "상호 정보량 (Mutual Informaion)",
    "section": "",
    "text": "상호 정보량은..\n상호 정보량(mutual information, MI)은 정보 이론(Information Theory)에서 두 확률 변수 간의 의존성을 측정하는 방법입니다. 상호 정보량은 두 확률 변수가 서로 얼마나 많은 정보를 공유하는지를 나타내며, 그 값이 클수록 두 변수 사이의 의존성이 높다고 볼 수 있습니다. 상호 정보량은 엔트로피와 조건부 엔트로피를 기반으로 한 수식으로 계산됩니다.\n두 확률 변수 X와 Y가 주어졌을 때, 상호 정보량은 다음과 같이 정의됩니다:\n\\[\nMI(X; Y) = \\displaystyle \\sum_{x \\in X }\\sum_{y \\in Y}p(x,y)log[\\frac{p(x,y)}{p(x),p(y)}]\n\\]\n\n\nR-squared와의 차이\n상호 정보량(mutual information, MI)과 결정계수(R-squared)는 두 변수 간의 관계를 측정하는 데 사용되는 통계적인 지표입니다. 그러나 이들은 서로 다른 가정과 계산 방법을 기반으로 하며, 각각 다른 측면을 강조합니다.\n\nMI는 두 변수 간의 일반적인 의존성을 측정하는 반면, R-squared는 두 변수 간의 선형 관계를 측정합니다. MI는 선형, 비선형, 모노토닉 등 모든 종류의 관계를 고려하지만, R-squared는 선형 관계에만 국한됩니다.\nMI는 양수 또는 0의 값을 가질 수 있으며, 값이 클수록 두 변수가 많은 정보를 공유한다는 것을 의미합니다. R-squared의 값은 0과 1 사이에 있으며, 값이 1에 가까울수록 선형 관계가 강하다는 것을 나타냅니다."
  },
  {
    "objectID": "posts/pca/index.html",
    "href": "posts/pca/index.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "좌측 3D 모델을 2차원으로 옮긴 모습.\n\nPCA는..\n주성분 분석(PCA, Principal Component Analysis)은 고차원의 데이터를 저차원의 데이터로 축소하거나 변환하는 데 사용되는 통계적 기법입니다. 이 방법은 데이터의 분산(variance)을 최대한 보존하면서 데이터를 새로운 좌표계로 변환합니다. PCA는 다음과 같은 과정을 거칩니다.\n\n데이터의 공분산 행렬(covariance matrix)을 계산합니다.\n공분산 행렬의 고유값과 고유벡터를 찾습니다.\n고유값이 큰 순서대로 해당 고유벡터를 정렬합니다. 이렇게 정렬된 고유벡터가 주성분(principal components)이며, 데이터의 분산을 나타냅니다.\n원하는 차원의 수만큼 주성분을 선택합니다. 예를 들어, 3차원 데이터를 2차원으로 축소하려면 가장 큰 두 개의 고유값에 해당하는 고유벡터를 선택합니다.\n선택한 주성분에 데이터를 투영하여 저차원의 데이터를 얻습니다.\n\nPCA의 주요 목적은 다음과 같습니다:\n\n차원 축소: 고차원의 데이터를 저차원의 데이터로 변환하여 데이터의 복잡성을 줄이고 계산 비용을 절감합니다.\n시각화: 고차원의 데이터를 2D 또는 3D로 표현하여 데이터의 패턴, 클러스터 또는 이상치를 쉽게 시각적으로 확인할 수 있게 합니다.\n노이즈 제거: 데이터의 주요 정보를 보존하면서 노이즈를 제거하여 데이터를 깨끗하게 만듭니다.\n특성 선택 및 추출: 데이터에서 가장 중요한 특성을 선택하거나 새로운 특성을 추출하여 데이터를 더 효과적으로 분석할 수 있습니다.\n\nPCA는 데이터 전처리, 시각화, 기계 학습 및 패턴 인식 등 다양한 분야에서 널리 사용되는 기법입니다. 하지만 주성분 분석에는 몇 가지 한계가 있습니다. 예를 들어, PCA는 선형적인 관계를 가정하기 때문에 비선형적인 데이터 구조를 제대로 표현하지 못할 수 있습니다. 이러한 경우에는 커널 PCA, t-SNE, UMAP 등 다른 차원 축소 기법을 고려할 수 있습니다.\n\n\nPCA 예제\n# Load required library\nlibrary(Rvcg)\nlibrary(rgl)\nlibrary(tidyverse)\n\n# Load 3D Dataset\nstl_mesh &lt;- vcgImport(\"D:/Utah_teapot_(solid).stl\")\n\n# Convert stl file to df\nvertices &lt;- cbind(stl_mesh$vb[1,],stl_mesh$vb[2,],stl_mesh$vb[3,])\n\n# Compress data in 2D Using PCA\npca_result &lt;- prcomp(vertices, center = TRUE, scale. = TRUE)\nreduced_data &lt;- pca_result$x[, 1:2]\nreduced_df &lt;- data.frame(reduced_data)\ncolnames(reduced_df) &lt;- c(\"PC1\", \"PC2\")\n\n# Visualize 2D data\n\nggplot(reduced_df, aes(x = PC2, y = PC1)) +\n  geom_point() +\n  theme_minimal() +\n  ggtitle(\"Teapot 3D Data Reduced to 2D using PCA\")\n\n\nPCA from Scratch\n# 예제 데이터 생성\nset.seed(42)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100, sd = 0.5)\nz &lt;- -x + y + rnorm(100, sd = 0.5)\ndata &lt;- data.frame(x, y, z)\n\n# 1. 데이터의 공분산 행렬 계산\ndata_cov &lt;- cov(data)\n\n# 2. 공분산 행렬의 고유값과 고유벡터 찾기\neigen_result &lt;- eigen(data_cov)\n\n# 고유값\neigen_values &lt;- eigen_result$values\n# 고유벡터\neigen_vectors &lt;- eigen_result$vectors\n\n# 3. 원하는 차원의 수만큼 주성분 선택 (여기서는 2차원으로 축소)\nnum_dimensions &lt;- 2\nselected_eigen_vectors &lt;- sorted_eigen_vectors[, 1:num_dimensions]\n\n# 4. 선택한 주성분에 데이터를 투영하여 저차원의 데이터 얻기\ndata_centered &lt;- scale(data, center = TRUE, scale = FALSE)\nreduced_data &lt;- data_centered %*% selected_eigen_vectors\n\n# 결과 출력\nreduced_data &lt;- data.frame(reduced_data)\ncolnames(reduced_data) &lt;- c(\"PC1\", \"PC2\")\nprint(reduced_data)"
  },
  {
    "objectID": "posts/random-forest/index.html",
    "href": "posts/random-forest/index.html",
    "title": "Random Forest",
    "section": "",
    "text": "Random Forest는..\n랜덤 포레스트는 배깅(Bagging)의 한 형태로서, 여러 개의 결정 트리를 조합하여 예측 결과를 도출하는 앙상블 기법입니다. 이 방법은 결정 트리의 가장 큰 문제점 중 하나인 과적합(overfitting)을 효과적으로 해결하고, 높은 예측 성능을 보여줍니다.\n랜덤 포레스트의 기본 아이디어는 다음과 같습니다:\n\n원본 데이터셋에서 부트스트랩 샘플링을 통해 여러 개의 샘플을 생성합니다. 이때, 중복 허용으로 원본 데이터셋과 같은 크기의 샘플을 만듭니다.\n각 샘플로부터 결정 트리를 학습시킵니다. 이 과정에서 무작위로 선택된 특성(feature)의 부분집합을 사용해 노드를 분할하는 최적의 분할을 찾습니다. 이러한 무작위성은 결정 트리 간의 상관관계를 낮추고, 다양성을 높여 과적합을 방지합니다.\n테스트 데이터셋에 대한 예측을 수행할 때, 각 결정 트리의 예측을 모아 최종 예측 결과를 도출합니다. 분류 문제의 경우 다수결 투표(Majority Voting) 방식을 사용해 최종 예측 클래스를 결정하며, 회귀 문제의 경우 각 트리의 예측값의 평균을 사용합니다.\n\n\n\nFashion MNIST 데이터를 활용한 Random Forest 예제\n [출처] : Fashion MNIST | Kaggle\nlibrary(tidymodels)\n\n## Load Dataset\nfmnist &lt;- arrow::read_feather(\"fashion-mnist.feather\")\nfmnist$label &lt;- as.factor(fmnist$label)\n\nset.seed(42)\nsplit_index &lt;- initial_split(fmnist,3/4,label)\ntrain_tb &lt;- training(split_index)\ntest_tb &lt;- testing(split_index)\n\n## Make workflow\n\nfmnist_recipe &lt;- recipe(label ~ ., data = train_tb) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; \n  prep()\n\nfmnist_rf &lt;- rand_forest(trees = 100, mtry = sqrt(784)) |&gt; \n  set_engine(\"ranger\", importance = \"impurity\") |&gt; \n  set_mode(\"classification\")\n\nfmnist_wf &lt;- workflow() |&gt;\n  add_recipe(fmnist_recipe) |&gt; \n  add_model(fmnist_rf)\n\nfmnist_wf_trained &lt;- fmnist_wf |&gt; fit(train_tb)\n\n# Predict with test dat\npredictions &lt;- predict(fmnist_wf_trained, test_tb) |&gt; \n  bind_cols(test_tb |&gt;  select(label))\n\naccuracy(predictions, truth = label, estimate = .pred_class)\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.882"
  },
  {
    "objectID": "posts/regularization/index.html",
    "href": "posts/regularization/index.html",
    "title": "Regularization (정규화 기법)",
    "section": "",
    "text": "모델의 정규화란…\nRegularization은 모델에 Bias를 추가하는 기법 중 하나로, 모델의 과적합을 피하기 위해 사용하는 기법 중 하나입니다. 일반적인 정규화 기법으로는 릿지 회귀(Ridge Regression), 라쏘 회귀(Lasso Regression), 그리고 엘라스틱넷 회귀(Elastic Net Regression)가 있습니다.\n\n릿지 회귀 (Ridge Regression) - L2 정규화:\n릿지 회귀는 선형 회귀에 L2 정규화를 추가한 것입니다. L2 정규화는 회귀 계수의 제곱합에 비례하는 패널티를 손실 함수에 추가합니다. 이 패널티는 모델의 계수를 작게 만들어 과적합을 방지하며, 일반화 성능을 향상시킵니다.\n라쏘 회귀 (Lasso Regression) - L1 정규화:\n라쏘 회귀는 선형 회귀에 L1 정규화를 추가한 것입니다. L1 정규화는 회귀 계수의 절댓값의 합에 비례하는 패널티를 손실 함수에 추가합니다. 라쏘 회귀는 계수를 정확히 0으로 만들어 희소한 모델을 생성하며, 이를 통해 변수 선택이 이루어집니다.\n엘라스틱넷 회귀 (Elastic Net Regression) - L1과 L2 정규화의 조합:\n엘라스틱넷 회귀는 L1 정규화와 L2 정규화를 모두 사용하는 방법입니다. 이 방법은 라쏘 회귀의 변수 선택 기능과 릿지 회귀의 일반화 성능을 모두 활용할 수 있습니다. 두 가지 하이퍼파라미터를 사용하여 정규화 항의 강도와 L1, L2 정규화의 비율을 조절합니다.\n\nRegularization을 일반화하면 다음과 같습니다 :\n\\[ \\hat{\\beta} \\equiv \\underset{\\beta}{\\operatorname{argmin}} (\\| y-X \\beta \\|^2 + \\lambda_2 \\|\\beta\\|^2 + \\lambda_1 \\|\\beta\\|_1) \\]\n\\(\\lambda_2\\)는 Ridge Regression을 위한 L2 Norm의 hyperparameter이며,\n\\(\\lambda_1\\)는 Lasso Regression을 위한 L1 Norm의 hyperparameter입니다.\n [출처] Lasso and Ridge Regression in Python Tutorial | DataCamp\n\n\n정규화 기법의 예시\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-7\n\nset.seed(42)\n\nn &lt;- 1000\np &lt;- 5000\nreal_p &lt;- 15\n\nx &lt;- matrix(rnorm(n * p), nrow = n, ncol = p) # initialize 5000 variables\ny &lt;- apply(x[, 1:real_p], 1, sum) + rnorm(n) # Row sums of 1 ~ 15 variables\n\ntrain_rows &lt;- sample(1:n, .66 * n)\nx_train &lt;- x[train_rows, ]\nx_test &lt;- x[-train_rows, ]\ny_train &lt;- y[train_rows]\ny_test &lt;- y[-train_rows]\n\n# when Logistic Reg., \"deviance\", \"binomial\"\nridge_fit &lt;- cv.glmnet(x_train, y_train, type.measure = \"mse\", alpha = 0, family = \"gaussian\")\nridge_fit_predict &lt;- predict(ridge_fit, s = ridge_fit$lambda.1se, newx = x_test)\n(y_test - ridge_fit_predict)^2 |&gt; mean()\n\n[1] 14.88459\n\nlasso_fit &lt;- cv.glmnet(x_train, y_train, type.measure = \"mse\", alpha = 1, family = \"gaussian\")\nlasso_fit_predict &lt;- predict(lasso_fit, s = lasso_fit$lambda.1se, newx = x_test)\n(y_test - lasso_fit_predict)^2 |&gt; mean()\n\n[1] 1.184701\n\nelastic_fit &lt;- cv.glmnet(x_train, y_train, type.measure = \"mse\", alpha = 0.5, family = \"gaussian\")\nelastic_fit_predict &lt;- predict(elastic_fit, s = elastic_fit$lambda.1se, newx = x_test)\n(y_test - elastic_fit_predict)^2 |&gt; mean()\n\n[1] 1.23797\n\nhead(coef(lasso_fit)) # Review weights of each variables\n\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept) 0.04414432\nV1          0.85216415\nV2          0.85494342\nV3          0.96978821\nV4          0.92690229\nV5          0.80608407"
  },
  {
    "objectID": "posts/t-sne/index.html",
    "href": "posts/t-sne/index.html",
    "title": "t-sne",
    "section": "",
    "text": "t-SNE란…\nt-SNE(t-Distributed Stochastic Neighbor Embedding)는 고차원 데이터를 저차원 공간(주로 2차원 또는 3차원)으로 시각화하기 위해 사용되는 비선형 차원 축소 기법입니다. t-SNE는 원본 고차원 데이터에서의 데이터 포인트 간 유사도와 축소된 저차원 공간에서의 데이터 포인트 간 유사도를 비슷하게 유지하려고 합니다. 이 기법은 특히 데이터의 군집 구조를 보존하는 데 효과적이라고 알려져 있습니다.\nt-SNE는 다음과 같은 과정으로 진행됩니다:\n\n유사도 계산: 고차원 데이터에서 각 데이터 포인트 간의 유사도를 계산합니다. 이 때, 가우시안 커널을 사용하여 조건부 확률을 구합니다. 이 조건부 확률은 한 데이터 포인트가 다른 데이터 포인트와 얼마나 가까운지를 나타내는 값입니다.\n저차원 맵핑: 초기에 무작위로 설정된 저차원 공간에서의 데이터 포인트 간 유사도를 계산합니다. t-SNE는 이 단계에서 t-분포를 사용하여 유사도를 계산합니다. 이는 원본 고차원 데이터에서의 군집 구조를 보존하면서 저차원 공간으로의 맵핑을 더 쉽게 만들어줍니다.\n최적화: 고차원 데이터에서의 유사도와 저차원 공간에서의 유사도가 최대한 비슷해지도록, 저차원 공간의 데이터 포인트 위치를 조정합니다. 이 최적화 과정은 그래디언트 디센트(Gradient Descent)와 같은 방법을 사용하여 진행됩니다.\n\nt-SNE는 차원 축소 결과를 통해 고차원 데이터에서의 군집 구조와 패턴을 시각적으로 이해하기 쉽게 해줍니다. 하지만 계산 복잡도가 높아 큰 데이터셋에 적용하기 어려울 수 있고, 최적화 과정의 무작위성으로 인해 결과의 재현성이 낮을 수 있다는 단점이 있습니다.\nlibrary(reticulate)\nlibrary(tidyverse)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.manifold import TSNE\n\n# Load the MNIST dataset\nmnist = datasets.fetch_openml('mnist_784')\nX, y = mnist.data, mnist.target\n\n# Select a subset of the dataset to reduce computation time\nn_samples = 5000\nX_sample = X[:n_samples]\ny_sample = y[:n_samples]\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_sample)\n\ny_sample\ny_sample = np.asarray(y_sample)\nX_tsne &lt;- py$X_tsne\ny_sample &lt;- py$y_sample\n\ntsne_df &lt;- tibble(x1 = X_tsne[,1], x2 = X_tsne[,2], y = y_sample)\n\nhead(tsne_df)\n# A tibble: 6 × 3\n    x1     x2  y        \n1  17.1   9.26  5        \n2  59.9   24.4  0        \n3  14.0  -53.1  4        \n4 -62.3   12.5  1        \n5  -3.57 -26.4  9        \n6 -32.6   61.9  2\n# Draw Plot\ntsne_df |&gt;\n  ggplot(aes(x = x1, y = x2, color = y)) +\n  geom_jitter() +\n  ggtitle(\"Result of t-sne on MNIST\")+\n  ggthemes::theme_fivethirtyeight()"
  },
  {
    "objectID": "posts/umap/index.html",
    "href": "posts/umap/index.html",
    "title": "UMAP",
    "section": "",
    "text": "UMAP이란…\nUMAP (Uniform Manifold Approximation and Projection)은 고차원 데이터를 저차원 공간으로 축소하는 비선형 차원 축소 기법 중 하나입니다. UMAP은 t-SNE와 유사하게 데이터의 군집 구조를 보존하면서 고차원 데이터를 시각화하거나 차원 축소하는 데 사용됩니다. 그러나 UMAP은 기하학적 특성을 더 잘 보존하고, 계산 효율성이 더 높아 대규모 데이터셋에도 적용 가능하다는 장점이 있습니다.\nUMAP 알고리즘의 주요 과정은 다음과 같습니다:\n\n고차원 데이터의 지역 구조 파악: 각 데이터 포인트의 근처 이웃을 찾아 데이터의 지역적 구조를 파악합니다. 이 과정에서는 일반적으로 k-최근접 이웃(KNN) 알고리즘이 사용됩니다.\n지역 구조를 기반으로 한 고차원 데이터의 그래프 생성: 각 데이터 포인트와 그 이웃 간의 유사도를 기반으로 고차원 데이터의 그래프를 생성합니다. 이 때, 거리 측정에는 주로 유클리디안 거리나 코사인 유사도를 사용하며, 가중치는 멀리 떨어진 포인트에 대해 더 낮은 가중치를 부여하는 방식으로 할당됩니다.\n저차원 임베딩을 위한 그래프 생성: 저차원 공간에서도 원본 고차원 데이터의 지역 구조를 유지하려고 노력하며, 그래프 기반 최적화를 통해 고차원 그래프와 저차원 그래프 간의 거리를 최소화합니다.\n최적화: 그래디언트 디센트(Gradient Descent)와 같은 방법을 사용하여 저차원 임베딩 공간의 데이터 포인트 위치를 조정하면서, 고차원 그래프와 저차원 그래프 간의 거리를 최소화하는 위치를 찾습니다.\n\nUMAP은 t-SNE에 비해 기하학적 특성을 더 잘 보존하고, 계산 효율성이 높아 대규모 데이터셋에도 적용할 수 있는 장점이 있습니다. 또한, UMAP은 차원 축소 결과를 통해 고차원 데이터에서의 군집 구조와 패턴을 시각적으로 이해하기 쉽게 해줍니다. 이러한 이유로 UMAP은 많은 데이터 과학자들이 선호하는 차원 축소 기법 중 하나입니다.\nlibrary(reticulate)\nlibrary(tidyverse)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport umap\n\n# Load the MNIST dataset\nmnist = datasets.fetch_openml('mnist_784')\nX, y = mnist.data, mnist.target\n\n# Select a subset of the dataset to reduce computation time\nn_samples = 5000\nX_sample = X[:n_samples]\ny_sample = y[:n_samples]\n\n# Apply UMAP\numap_reducer = umap.UMAP(n_components=2, random_state=42)\nX_umap = umap_reducer.fit_transform(X_sample)\ny_sample = np.asarray(y_sample)\nX_umap &lt;- py$X_umap\ny_sample &lt;- py$y_sample\n\numap_df &lt;- tibble(x1 = X_umap[,1], x2 = X_umap[,2], y = y_sample)\n\nhead(umap_df)\n# A tibble: 6 × 3\n    x1    x2   y        \n1  4.05  7.81  5        \n2  0.744 3.75  0        \n3 10.9   0.124 4        \n4 11.9   7.83  1        \n5 11.6   2.26  9        \n6  8.29  5.54  2\n# Draw Plot\numap_df |&gt;\n  ggplot(aes(x = x1, y = x2, color = y)) +\n  geom_jitter() +\n  ggtitle(\"Result of UMAP on MNIST\")+\n  ggthemes::theme_fivethirtyeight()"
  },
  {
    "objectID": "posts/word-embedding/index.html",
    "href": "posts/word-embedding/index.html",
    "title": "Word Embedding과 Word2Vec",
    "section": "",
    "text": "Word Embedding은..\n단어 임베딩은 단어들을 고차원 벡터 공간에 매핑하는 과정입니다. 이렇게 변환된 벡터는 단어 간의 의미적 관계를 반영하도록 합니다. 비슷한 의미를 가진 단어들은 벡터 공간에서 가까운 위치에 놓이게 됩니다. 이를 통해 자연어 처리 모델은 단어 간의 유사성을 파악하고 문장이나 문서의 의미를 이해할 수 있습니다. 예를 들어, “왕 - 남자 + 여자 = 여왕”과 같은 관계를 벡터 연산을 통해 찾을 수 있습니다.\n\n상기 예시의 시각화\n기초적인 Word Embedding의 학습은 한 문장의 다음 단어(토큰)을 예측하는 Neural Network(이하 NN)을 학습하는 것입니다. 예를 들면 “사과/는” 이라는 토큰을 input으로 넣으면 “빨갛다”라는 토큰을 예측하는 방식입니다. 이렇게 학습된 NN에서 각 단어에 할당된 각 hidden layer들에 대한 가중들이 단어들을 벡터 공간에 매핑한 결과입니다. 위의 왕/남자/여왕/여자의 예시를 일반화할 경우, 각 단어들이 2개의 hidden layer에 연결되어 있고, 각 layer는 성별/직위를 뜻한다고 볼 수 있습니다.\n\n\nWord2Vec은..\nWord2Vec은 구글 연구자들이 개발한 단어 임베딩 기법입니다. Word2Vec은 큰 텍스트 데이터셋에서 단어들의 의미적 관계를 학습하는 데 사용되는 신경망 기반 모델입니다. Word2Vec에는 주로 두 가지 학습 방법이 사용됩니다:\n\nCBOW (Continuous Bag of Words): 이 방법은 주변 단어들을 사용하여 중심 단어를 예측하는 방식입니다. 즉, 주변 문맥을 기반으로 단어의 의미를 학습합니다. “사과는 빨갛다” 예시에서 “사과”와 “빨갛다”를 입력으로 사용하여 “는”을 출력으로 학습하는 방식입니다.\nSkip-gram: 이 방법은 중심 단어를 사용하여 주변 단어들을 예측하는 방식입니다. 이 방법은 주로 큰 데이터셋에 적합하며, 희소한 단어들에 대해서도 더 나은 임베딩을 생성합니다. 마찬가지의 예시를 활용하자면, 여기서는 “는”을 입력으로 사용하고, “사과”와 “빨갛다”를 출력으로 학습하는 방식입니다.\n\n\n\n예제: Seoul - Korea + Japan = ?\nlibrary(tidyverse)\nglove_6B_50d &lt;- read_table(\"C:/Users/tranq/Desktop/glove.6B.50d.txt\", \n                           col_names = FALSE)\n\nword_embeddings &lt;- glove_6B_50d[, 1]\nvector_embeddings &lt;- as.matrix(glove_6B_50d[, -1])\n\nget_word_vector &lt;- function(word) {\n  word_index &lt;- which(word_embeddings == word)\n  if (length(word_index) == 0) {\n    return(NULL)\n  }\n  return(vector_embeddings[word_index, ])\n}\n\nfind_closest_word &lt;- function(result_vector, n = 5) {\n\n  similarity_scores &lt;- vector_embeddings %*% result_vector / (sqrt(rowSums(vector_embeddings^2)) * sqrt(sum(result_vector^2)))\n  closest_indices &lt;- order(similarity_scores, decreasing = TRUE)[1:n]\n  return(word_embeddings[closest_indices,1])\n}\n\nseoul &lt;- get_word_vector(\"seoul\")\nkorea &lt;- get_word_vector(\"korea\")\njapan &lt;- get_word_vector(\"japan\")\n\nresult_vector &lt;- soeul - korea + japan\n\nclosest_word &lt;- find_closest_word(result_vector, n = 5)\nprint(closest_word)\n# A tibble: 5 × 1\n  X1      \n  &lt;chr&gt;   \n1 tokyo   \n2 osaka   \n3 japan   \n4 shanghai\n5 seoul"
  },
  {
    "objectID": "posts/zifs-law/index.html",
    "href": "posts/zifs-law/index.html",
    "title": "Zipf’s Law (단어 사용의 분포)",
    "section": "",
    "text": "Zipf’s Law는..\nZipf의 법칙(Zipf’s Law)은 언어학과 정보 이론에서 관찰되는 경험적인 법칙으로, 주어진 말뭉치(corpus)에서 단어의 사용 빈도와 순위 사이에 특정한 관계가 있다는 것을 기술합니다. 이 법칙은 미국의 언어학자 조지 캥스리 지프(George Kingsley Zipf)에 의해 1930년대에 발견되었습니다.\nZipf의 법칙에 따르면, 말뭉치에서 각 단어의 사용 빈도는 그 단어의 순위에 반비례합니다. 다시 말해, 가장 빈번하게 사용되는 단어의 빈도는 두 번째로 빈번하게 사용되는 단어의 빈도보다 대략 두 배 많으며, 세 번째로 빈번하게 사용되는 단어의 빈도보다 대략 세 배 많은 식입니다.\n브라운 대학교 현대 미국 영어 표준 말뭉치의 경우, 가장 사용 빈도가 높은 단어는 영어 정관사  “the”이며 전체 문서에서 7%의 빈도(약 백만 개 남짓의 전체 사용 단어 중 69,971회)를 차지한다. 두 번째로 사용 빈도가 높은 단어는 “of”로 약 3.5% 남짓(36,411회)한 빈도를 차지하며, 세 번째로 사용 빈도가 높은 단어는 “and”(28,852회)로, 지프의 법칙에 정확히 들어 맞는다. 약 135개 항목의 어휘만으로 브라운 대학 말뭉치의 절반을 나타낼 수 있다.\nZipf의 법칙은 여러 언어와 다양한 텍스트에서 일관되게 관찰되는 현상으로, 인간 언어와 정보 처리의 기본 원리를 이해하는 데 도움이 됩니다. 이 법칙의 정확한 원인은 아직 명확하게 밝혀지지 않았지만, 자연 언어가 최적화된 정보 전달 방식을 따르는 결과라는 설명이 제안되고 있습니다.\n\n\nZipf’s Law 예제 (제인 오스틴 소설)\n\n이성과 감성 / 오만과 편견 / 맨스필드 파크 / 엠마 / 노생거 사원 / 설득 모두 Zipf’s law를 만족함을 확인할 수 있다.\n# Load Library\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(janeaustenr)\n\nword_count &lt;- austen_books() |&gt;\n  unnest_tokens(word, text) |&gt;\n  count(book, word, sort = TRUE)\n\ntotal_word &lt;- word_count |&gt; group_by(book) |&gt; summarise(total = sum(n))\n\nword_count &lt;- word_count |&gt; left_join(total_word, by = \"book\")\n\nhead(word_count)\n\nggplot(word_count, aes(n/total, fill = book)) +\n  geom_histogram(show.legend = FALSE) +\n  xlim(NA, 0.0009) +\n  facet_wrap(~book, ncol = 2, scales = \"free_y\")\n# A tibble: 6 × 4\n  book           word      n  total\n  &lt;fct&gt;          &lt;chr&gt; &lt;int&gt;  &lt;int&gt;\n1 Mansfield Park the    6206 160460\n2 Mansfield Park to     5475 160460\n3 Mansfield Park and    5438 160460\n4 Emma           to     5239 160996\n5 Emma           the    5201 160996\n6 Emma           and    4896 160996"
  }
]