---
title: "Kullback-Leibler Divergence 쉽게 이해하기"
author: "Jinwook Chang"
date: "2023-05-12"
date-modified: "2023-05-12"
categories: [DataScience, Math, Statistics]
---

본 문서를 읽기 앞서, [Entropy](../entropy)에 대한 이해가 필요합니다.
먼저 읽고 오시길 권장드립니다.

### KL Divergence란..

KL Divergence(Kullback-Leibler Divergence)는 실제 모델과 예측 분포 간의 차이를 측정하는 방법입니다.  
KL Divergence는 또한 정보 이론에서 상대 엔트로피로 알려져 있습니다.

### KL Divergence의 간단한 예시

수식을 살펴보기 앞서, 간단한 예시로 두 환률 분포 간의 차이를 측정해보겠습니다.  
이전 [Entropy](../entropy)문서를 읽으셨다면, Entropy의 공식이 하기와 같음을 기억하실 것입니다. 또한 Entropy의 의미가 놀라움의 평균인 것도 함께 기억하실 것입니다.

$$
\sum log(\frac{1}{p(x)}) \cdot p(x)
$$

한번 6면체 주사위를 생각해봅시다.  
6면체 주사위의 실제 Entropy는 $6 \times log(\frac{1}{1/6}) \cdot 1/6 \simeq 1.792$ 입니다.

하지만 제가 실험을 잘못 설계해서 이 주사위의 확률 값을 잘못 계산했다고 가정해봅시다.  
1 ~ 6 까지의 확률은 (0.1,0.2,0.1,0.2,0.2,0.2) 입니다. 이를 q(x)라고 하겠습니다.  
그렇다면, 제가 예측한 놀라움과 실제 확률을 통해 Entropy의 계산 역시 가능할 것입니다. $\sum log(\frac{1}{q(x)}) \cdot 1/6 \simeq 1.84$

이 두 값의 차, 0.048이 두 분포의 차, KL Divergence의 값입니다.
이제 KL Divergence의 수식을 보도록 합시다.

### KL Divergence 수식

위키피디아의 [쿨백-라이블러 발산](https://ko.wikipedia.org/wiki/%EC%BF%A8%EB%B0%B1-%EB%9D%BC%EC%9D%B4%EB%B8%94%EB%9F%AC_%EB%B0%9C%EC%82%B0)의 글에 따르면 이산 확률에서의 쿨백-라이블러 발산은 하기와 같습니다:

$$D_{KL}(P||Q) = \sum_i p(i) log \frac{P(i)}{Q(i)} $$

우리가 위에서 계산했던 식을 일반화 한다면,
$$ \sum log(\frac{1}{q(x)}) \cdot p(x) - \sum log(\frac{1}{p(x)}) \cdot p(x) $$

p(x)로 묶는다면,

$$ \sum p(x)  (log(\frac{1}{q(x)}) -  log(\frac{1}{p(x)})) $$

log를 정리하면, 우리가 원했던 식이 나옵니다.

$$ \sum p(x) log(\frac{p(x)}{q(x)}) $$