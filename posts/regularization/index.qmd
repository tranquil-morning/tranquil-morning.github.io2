---
title: "Regularization (정규화 기법)"
author: "Jinwook Chang"
date: "2023-04-15"
date-modified: "2023-05-15"
categories: [DataScience,Statistics,Script]
---

### 모델의 정규화란…

Regularization은 모델에 Bias를 추가하는 기법 중 하나로, 모델의 과적합을 피하기 위해 사용하는 기법 중 하나입니다.   일반적인 정규화 기법으로는 릿지 회귀(Ridge Regression), 라쏘 회귀(Lasso Regression), 그리고 엘라스틱넷 회귀(Elastic Net Regression)가 있습니다.

1. 릿지 회귀 (Ridge Regression) - L2 정규화:  
릿지 회귀는 선형 회귀에 L2 정규화를 추가한 것입니다. L2 정규화는 회귀 계수의 제곱합에 비례하는 패널티를 손실 함수에 추가합니다. 이 패널티는 모델의 계수를 작게 만들어 과적합을 방지하며, 일반화 성능을 향상시킵니다.

2. 라쏘 회귀 (Lasso Regression) - L1 정규화:  
라쏘 회귀는 선형 회귀에 L1 정규화를 추가한 것입니다. L1 정규화는 회귀 계수의 절댓값의 합에 비례하는 패널티를 손실 함수에 추가합니다. 라쏘 회귀는 계수를 정확히 0으로 만들어 희소한 모델을 생성하며, 이를 통해 변수 선택이 이루어집니다.
3. 엘라스틱넷 회귀 (Elastic Net Regression) - L1과 L2 정규화의 조합:  
엘라스틱넷 회귀는 L1 정규화와 L2 정규화를 모두 사용하는 방법입니다. 이 방법은 라쏘 회귀의 변수 선택 기능과 릿지 회귀의 일반화 성능을 모두 활용할 수 있습니다. 두 가지 하이퍼파라미터를 사용하여 정규화 항의 강도와 L1, L2 정규화의 비율을 조절합니다.

Regularization을 일반화하면 다음과 같습니다 : 

$$ \hat{\beta} \equiv \underset{\beta}{\operatorname{argmin}} (\| y-X \beta \|^2 + \lambda_2 \|\beta\|^2 + \lambda_1 \|\beta\|_1) $$

$\lambda_2$는 Ridge Regression을 위한 L2 Norm의 hyperparameter이며,  
$\lambda_1$는 Lasso Regression을 위한 L1 Norm의 hyperparameter입니다.

![](front.png)
[출처] [Lasso and Ridge Regression in Python Tutorial | DataCamp](https://www.datacamp.com/tutorial/tutorial-lasso-ridge-regression)

L2 규제화는 계수 공간에서 원 혹은 타원형의 등고선을 형성하며,  
이는 오차를 최소화하는 최적의 해가 L2 규제화 원의 경계에 존재하게 됩니다.  

L2 규제화는 계수를 제한하지만, 이것이 완전히 0이 될 확률은 매우 낮습니다.  
이는 L2 규제화 원의 경계가 축에 정확히 정렬되지 않기 때문입니다.

반면에 L1 규제화는 계수 공간에서 마름모 모양의 등고선을 형성합니다.  
이 마름모의 모서리는 축에 정렬되어 있습니다.  
그로 인해 최적의 해가 축(즉, 계수가 0인 위치)에 위치할 확률이 높아집니다.  
따라서 L1 규제화는 계수를 완전히 0으로 만드는 효과가 있습니다.

### 정규화 기법의 예시

```{r}
library(glmnet)
set.seed(42)

n <- 1000
p <- 5000
real_p <- 15

x <- matrix(rnorm(n * p), nrow = n, ncol = p) # initialize 5000 variables
y <- apply(x[, 1:real_p], 1, sum) + rnorm(n) # Row sums of 1 ~ 15 variables

train_rows <- sample(1:n, .66 * n)
x_train <- x[train_rows, ]
x_test <- x[-train_rows, ]
y_train <- y[train_rows]
y_test <- y[-train_rows]

# when Logistic Reg., "deviance", "binomial"
ridge_fit <- cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 0, family = "gaussian")
ridge_fit_predict <- predict(ridge_fit, s = ridge_fit$lambda.1se, newx = x_test)
(y_test - ridge_fit_predict)^2 |> mean()

lasso_fit <- cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 1, family = "gaussian")
lasso_fit_predict <- predict(lasso_fit, s = lasso_fit$lambda.1se, newx = x_test)
(y_test - lasso_fit_predict)^2 |> mean()

elastic_fit <- cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 0.5, family = "gaussian")
elastic_fit_predict <- predict(elastic_fit, s = elastic_fit$lambda.1se, newx = x_test)
(y_test - elastic_fit_predict)^2 |> mean()

head(coef(lasso_fit)) # Review weights of each variables
```