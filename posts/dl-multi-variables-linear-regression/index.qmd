---
title: "딥러닝을 위한 Multi variables Linear Regression"
author: "Jinwook Chang"
date: "2023-05-05"
date-modified: "2023-05-05"
categories: [DataScience, Regression, DeepLearning, Script]
---

이전의 [Simple Linear Regression](../dl-simple-linear-regression)예제에 이어, 다변수를 통해 결과값을 예측하는 모델에 대해 알아보도록 하겠습니다.  
퀴즈1, 퀴즈2, 중간고사의 성적으로 기말고사의 성적을 예측하는 모델을 예시로 하겠습니다.

```{r, message = FALSE, warning = FALSE, echo = FALSE}
library(tidyverse)
example <- tibble(
  quiz1 = c(73, 93, 89, 96, 73),
  quiz2 = c(80, 88, 91, 98, 66),
  midterm = c(75, 95, 90, 100, 70),
  final = c(152,185,180,196,142)
)
example
```

이전과 달리 벡터 기반으로 식을 나타내면 아래와 같습니다.

$$ H(x) = XW + b $$ 
위의 사례로 식을 풀어쓰면, 하기와 같습니다.  
x1, x2, x3 는 quiz1, quiz2, midterm의 점수  (행 벡터)
w1, w2, w3 는 quiz1, quiz2, mideterm에 대한 계수  (열 벡터)

$$ H(x) = x_1w_1 + x_2w_2 + x_3w_3 + b $$


이 선과 실제 데이터의 오차는 이전과 같습니다.


$$ cost(W,b) = \frac{1}{m}\sum_{i=1}^{m}(H(x^{(i)}) - y^{(i)})^2 $$


마찬가지로, cost(오차)를 최소화 할 수 있는 W,b의 쌍을 찾는 것입니다. 오차를 최소화 하는 값을 찾기 위해, Gradient Descent를 통해 확인해보도록 하겠습니다.

1.  초기 값 W,b에 대한 각각의 편미분 값을 구합니다.
2.  편미분 값에 learning rate $\alpha$를 곱해줍니다.
3.  W, b에 곱해준 값을 뺀 후 W, b 값을 업데이트 합니다.
4.  cost가 만족할만한 수준이 될 때 까지 1 \~ 3을 반복합니다.

예시 데이터를 아래의 스크립트를 통해 $XW + b$를 구해보도록 하겠습니다.

```{r, message = FALSE, warning = FALSE}
# 필요한 라이브러리 불러오기
library(tidyverse)
library(glue)

# Gradient Descent 함수 정의
gradient_descent <- function(X, y, alpha, iterations) {
  n <- length(y)
  X <- cbind(1, as.matrix(X)) # add intercept term to X matrix
  m <- ncol(X)
  y <- as.matrix(y)
  W <- matrix(rnorm(m), ncol = 1)
  
  for (i in 1:iterations) {
    y_predicted <- X %*% W
    cost <- y_predicted - y
    W <- W - alpha*(1/n)*t(X) %*% (y_predicted - y)
  }
  return(W)
}


# Gradient Descent 실행
result <- gradient_descent(example[,-4], example[4], alpha = 0.00001, iterations = 2000)

# 결과 출력
cat(glue("절편 : {result[1,]}
quiz1 : {result[2,]}
quiz2 : {result[3,]}
midterm : {result[4,]}"))

cat("예측: \n")
print(cbind(1,as.matrix(example[,-4])) %*% result)
cat("실제: \n")
print(as.matrix(example[,4]))

```
