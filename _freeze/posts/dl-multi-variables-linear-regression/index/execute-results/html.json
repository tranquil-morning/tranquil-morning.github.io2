{
  "hash": "8e487f5be2fca70b2d72f3b4ba9db806",
  "result": {
    "markdown": "---\ntitle: \"딥러닝을 위한 Multi variables Linear Regression\"\nauthor: \"Jinwook Chang\"\ndate: \"2023-05-05\"\ndate-modified: \"2023-05-05\"\ncategories: [DataScience, Regression, DeepLearning, Script]\n---\n\n\n이전의 [Simple Linear Regression](../dl-simple-linear-regression)예제에 이어, 다변수를 통해 결과값을 예측하는 모델에 대해 알아보도록 하겠습니다.  \n퀴즈1, 퀴즈2, 중간고사의 성적으로 기말고사의 성적을 예측하는 모델을 예시로 하겠습니다.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 4\n  quiz1 quiz2 midterm final\n  <dbl> <dbl>   <dbl> <dbl>\n1    73    80      75   152\n2    93    88      95   185\n3    89    91      90   180\n4    96    98     100   196\n5    73    66      70   142\n```\n:::\n:::\n\n\n이전과 달리 벡터 기반으로 식을 나타내면 아래와 같습니다.\n\n$$ H(x) = XW + b $$ \n위의 사례로 식을 풀어쓰면, 하기와 같습니다.  \nx1, x2, x3 는 quiz1, quiz2, midterm의 점수  (행 벡터)\nw1, w2, w3 는 quiz1, quiz2, mideterm에 대한 계수  (열 벡터)\n\n$$ H(x) = x_1w_1 + x_2w_2 + x_3w_3 + b $$\n\n\n이 선과 실제 데이터의 오차는 이전과 같습니다.\n\n\n$$ cost(W,b) = \\frac{1}{m}\\sum_{i=1}^{m}(H(x^{(i)}) - y^{(i)})^2 $$\n\n\n마찬가지로, cost(오차)를 최소화 할 수 있는 W,b의 쌍을 찾는 것입니다. 오차를 최소화 하는 값을 찾기 위해, Gradient Descent를 통해 확인해보도록 하겠습니다.\n\n1.  초기 값 W,b에 대한 각각의 편미분 값을 구합니다.\n2.  편미분 값에 learning rate $\\alpha$를 곱해줍니다.\n3.  W, b에 곱해준 값을 뺀 후 W, b 값을 업데이트 합니다.\n4.  cost가 만족할만한 수준이 될 때 까지 1 \\~ 3을 반복합니다.\n\n예시 데이터를 아래의 스크립트를 통해 $XW + b$를 구해보도록 하겠습니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 필요한 라이브러리 불러오기\nlibrary(tidyverse)\nlibrary(glue)\n\n# Gradient Descent 함수 정의\ngradient_descent <- function(X, y, alpha, iterations) {\n  n <- length(y)\n  X <- cbind(1, as.matrix(X)) # add intercept term to X matrix\n  m <- ncol(X)\n  y <- as.matrix(y)\n  W <- matrix(rnorm(m), ncol = 1)\n  \n  for (i in 1:iterations) {\n    y_predicted <- X %*% W\n    cost <- y_predicted - y\n    W <- W - alpha*(1/n)*t(X) %*% (y_predicted - y)\n  }\n  return(W)\n}\n\n\n# Gradient Descent 실행\nresult <- gradient_descent(example[,-4], example[4], alpha = 0.00001, iterations = 2000)\n\n# 결과 출력\ncat(glue(\"절편 : {result[1,]}\nquiz1 : {result[2,]}\nquiz2 : {result[3,]}\nmidterm : {result[4,]}\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n절편 : 0.622163484428753\nquiz1 : 0.452707524024104\nquiz2 : 0.525627914161855\nmidterm : 1.01605264457251\n```\n:::\n\n```{.r .cell-code}\ncat(\"예측: \\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n예측: \n```\n:::\n\n```{.r .cell-code}\nprint(cbind(1,as.matrix(example[,-4])) %*% result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        final\n[1,] 151.9240\n[2,] 185.5042\n[3,] 180.1900\n[4,] 197.1989\n[5,] 139.4849\n```\n:::\n\n```{.r .cell-code}\ncat(\"실제: \\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n실제: \n```\n:::\n\n```{.r .cell-code}\nprint(as.matrix(example[,4]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     final\n[1,]   152\n[2,]   185\n[3,]   180\n[4,]   196\n[5,]   142\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}