[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Experienced Supply Chain Manager\nData Scientist\nSamsung Electronics (Jul 2019 ~ )\nMS in Data Science\nBS in Industrial Engineering\nBA in Sports Industry Management"
  },
  {
    "objectID": "about.html#jinwook-chang-張鎭旭",
    "href": "about.html#jinwook-chang-張鎭旭",
    "title": "About",
    "section": "",
    "text": "Experienced Supply Chain Manager\nData Scientist\nSamsung Electronics (Jul 2019 ~ )\nMS in Data Science\nBS in Industrial Engineering\nBA in Sports Industry Management"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Note of myself",
    "section": "",
    "text": "배낭 문제\n\n\n\nDataScience\n\n\nOptimization\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n생산계획\n\n\n\nDataScience\n\n\nOptimization\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1종 오류 2종 오류 헷갈리지 않기\n\n\n\nDataScience\n\n\nStatistics\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGPT 템플릿\n\n\n\nAI\n\n\nGPT\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEntropy 쉽게 이해하기\n\n\n\nDataScience\n\n\nMath\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n상호 정보량 (Mutual Informaion)\n\n\n\nDataScience\n\n\nMetric\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embedding과 Word2Vec\n\n\n\nDataScience\n\n\nNLP\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n이원수와 전진자동미분\n\n\n\nDataScience\n\n\nOptimization\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n인과관계의 조건과 탐색\n\n\n\nDataScience\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDBSCAN\n\n\n\nDataScience\n\n\nClustering\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Boosting\n\n\n\nDataScience\n\n\nEnsembleLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest\n\n\n\nDataScience\n\n\nEnsembleLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEnsemble Learning\n\n\n\nDataScience\n\n\nMainTheme\n\n\nEnsembleLearning\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nK-means clustering\n\n\n\nDataScience\n\n\nClustering\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nt-sne\n\n\n\nDataScience\n\n\nDimensionReduction\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUMAP\n\n\n\nDataScience\n\n\nDimensionReduction\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\nMainTheme\n\n\nDataScience\n\n\nClustering\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDimension Reduction (차원 축소)\n\n\n\nMainTheme\n\n\nDataScience\n\n\nDimensionReduction\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nN-1 더미 변수를 활용하는 이유\n\n\n\nDataScience\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRegularization (정규화 기법)\n\n\n\nDataScience\n\n\nOptimization\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCovariance Matrix\n\n\n\nDataScience\n\n\nMath\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHierarchical Clustering\n\n\n\nDataScience\n\n\nClustering\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis\n\n\n\nDataScience\n\n\nDimensionReduction\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAlpha max plus Beta min (Fast 2D Eculedian Distance)\n\n\n\nDataScience\n\n\nOptimization\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBenford’s Law\n\n\n\nDataScience\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nZipf’s Law (단어 사용의 분포)\n\n\n\nDataScience\n\n\nNLP\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 9, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/alpha-max-plus-beta-min/index.html",
    "href": "posts/alpha-max-plus-beta-min/index.html",
    "title": "Alpha max plus Beta min (Fast 2D Eculedian Distance)",
    "section": "",
    "text": "Alpha max plus Beta min 알고리즘은 두 제곱의 합의 제곱근을 고속으로 근사하는 방법입니다. 이 계산은 직각 삼각형의 빗변 길이를 구하거나, 2차원 벡터의 놈, 또는 실수와 허수부를 가진 복소수 z = a + bi의 크기를 구하는 데 사용되는 유용한 기능입니다.\n이 알고리즘은 계산 비용이 높은 제곱 및 제곱근 연산을 수행하는 대신 비교, 곱셈 및 덧셈과 같은 간단한 연산을 사용합니다. α와 β 매개변수를 적절하게 선택함으로써 곱셈 연산을 이진 자릿수의 단순한 시프트로 줄일 수 있습니다. 이는 고속 디지털 회로 구현에 특히 적합합니다.\n알파 맥스 플러스 베타 민 알고리즘은 다음과 같이 표현할 수 있습니다:\n\\[\n\\sqrt{A^2 + B^2} ≈ α \\cdot max(A, B) + β \\cdot min(A, B)\n\\]\n여기서 A와 B는 주어진 두 수이고, max(A, B)는 A와 B 중 큰 값을 나타내며, min(A, B)는 작은 값을 나타냅니다. α와 β는 근사치를 최적화하는 선택된 매개변수입니다.\n이 알고리즘은 두 제곱의 합의 제곱근을 직접 계산하는 것에 비해 빠르고 계산적으로 효율적인 대안을 제공하면서도 합리적인 정확도를 유지합니다.\n\n\n\nUntitled"
  },
  {
    "objectID": "posts/alpha-max-plus-beta-min/index.html#benchmark-of-alpha-max-plus-beta-min",
    "href": "posts/alpha-max-plus-beta-min/index.html#benchmark-of-alpha-max-plus-beta-min",
    "title": "Alpha max plus Beta min (Fast 2D Eculedian Distance)",
    "section": "Benchmark of Alpha max plus Beta min",
    "text": "Benchmark of Alpha max plus Beta min\n# 기본 제곱근 계산 함수\nsqrt_sum_of_squares &lt;- function(a, b) {\n  return(sqrt(a^2 + b^2))\n}\n\n# 알파 맥스 플러스 베타 민 근사 함수\nalpha_max_plus_beta_min &lt;- function(a, b, alpha, beta) {\n  return(alpha * max(a, b) + beta * min(a, b))\n}\n\n# 테스트 케이스 생성\nnum_tests &lt;- 10000\ntest_cases &lt;- data.frame(a = runif(num_tests), b = runif(num_tests))\n\n# 파라미터 설정\nalpha &lt;- 0.960\nbeta &lt;- 0.397\n\n# 기본 제곱근 계산 시간 측정\nstart_time &lt;- Sys.time()\nfor (i in 1:num_tests) {\n  sqrt_sum_of_squares(test_cases$a[i], test_cases$b[i])\n}\nend_time &lt;- Sys.time()\nsqrt_time &lt;- end_time - start_time\n\n# 알파 맥스 플러스 베타 민 계산 시간 측정\nstart_time &lt;- Sys.time()\nfor (i in 1:num_tests) {\n  alpha_max_plus_beta_min(test_cases$a[i], test_cases$b[i], alpha, beta)\n}\nend_time &lt;- Sys.time()\napprox_time &lt;- end_time - start_time\n\n# 결과 출력\ncat(\"기본 제곱근 계산 시간: \", sqrt_time, \"\\n\")\ncat(\"알파 맥스 플러스 베타 민 근사 시간: \", approx_time, \"\\n\")\n\n## 기본 제곱근 계산 시간:  0.09882092\n## 알파 맥스 플러스 베타 민 근사 시간:  0.03761482"
  },
  {
    "objectID": "posts/benfords-law/index.html",
    "href": "posts/benfords-law/index.html",
    "title": "Benford’s Law",
    "section": "",
    "text": "벤포드의 법칙(Benford’s Law), 또는 첫 번째 숫자 법칙(First-Digit Law)은 다양한 실생활 데이터 집합에서 특정 숫자가 처음 자리에 나타날 확률에 관한 경험적 법칙입니다. 이 법칙은 1881년에 관찰되었으나, 물리학자 프랭크 벤포드(Frank Benford)가 1938년에 이 법칙을 더 널리 알렸기 때문에 그의 이름이 붙었습니다.\n벤포드의 법칙에 따르면, 첫 번째 숫자가 n인 데이터의 비율은 다음 공식으로 계산할 수 있습니다:\nP(n) = log10(n + 1) - log10(n) = log10(1 + 1/n)\n이 공식에 따르면, 첫 번째 숫자가 1일 확률은 약 30.1%, 2일 확률은 약 17.6%, 9일 확률은 약 4.6%입니다.\n벤포드의 법칙은 인구, 경제, 과학, 지리 등 다양한 분야의 데이터에 적용됩니다. 벤포드의 법칙은 데이터의 숫자 분포를 분석하는 데 도움이 되며, 이상치 탐지, 부정 행위 감지, 데이터 오류 확인 등에 활용됩니다.\n예를 들어 회계 분야에서 벤포드의 법칙을 이용해 부정 회계를 찾아낼 수 있습니다. 벤포드의 법칙에 따르면, 금융 데이터의 첫 자리 숫자 분포는 특정한 패턴을 따르는데, 이 패턴과 크게 벗어난 데이터가 발견되면 부정 행위의 가능성을 의심할 수 있습니다.\n[1 ≤ k ≤ 9 에서의 k의 분포 P_k %의 그래프]"
  },
  {
    "objectID": "posts/benfords-law/index.html#benfords-law-예제",
    "href": "posts/benfords-law/index.html#benfords-law-예제",
    "title": "Benford’s Law",
    "section": "Benford’s Law 예제",
    "text": "Benford’s Law 예제\n# Load required packages\nlibrary(gapminder)\nlibrary(tidyverse)\n\n# Load data from the gapminder package\ndata &lt;- gapminder\n\n# Filter the dataset to get the most recent population data for each country\nlatest_data &lt;- data |&gt; filter(year == max(year))\n\n# Extract the first digits from the population data\nfirst_digits &lt;- as.character(latest_data$pop) |&gt; str_sub(1,1) |&gt; as.integer()\n\n# Calculate the frequencies of the first digits\nobserved_freq &lt;- as.vector(table(first_digits) / length(first_digits))\n\n# Compute the expected frequencies according to Benford's Law\ndigits &lt;- 1:9\nbenford_freq &lt;- log10(1 + 1/digits)\n\n# Compare the observed and expected frequencies\ncomparison &lt;- tibble(\n  Digit = digits,\n  Observed_Frequency = observed_freq,\n  Benford_Frequency = benford_freq\n)\n\n# Reshape the data into a tidy format\ncomparison_tidy &lt;- comparison |&gt;\n  pivot_longer(cols = c(Observed_Frequency, Benford_Frequency), names_to = \"Type\", values_to = \"Frequency\")\n\n# Create a bar plot with ggplot2\nggplot(comparison_tidy, aes(x = factor(Digit), y = Frequency, fill = Type)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Comparing Observed and Benford's Law Frequencies\",\n       x = \"First Digit\",\n       y = \"Frequency\",\n       fill = \"Frequency Type\") +\n  theme_minimal()\n\n# Perform a chi-squared goodness-of-fit test\nobserved_counts &lt;- table(first_digits)\nchisq_test &lt;- chisq.test(observed_counts, p = benford_freq)\n\n# Print the results of the chi-squared test\ncat(\"Chi-squared test statistic:\", chisq_test$statistic, \"\\n\")\ncat(\"Degrees of freedom:\", chisq_test$parameter, \"\\n\")\ncat(\"p-value:\", chisq_test$p.value, \"\\n\")\n\n# Check if the observed data follows Benford's Law (using a significance level of 0.05)\nif (chisq_test$p.value &gt; 0.05) {\n  cat(\"The data follows Benford's Law.\\n\")\n} else {\n  cat(\"The data does not follow Benford's Law.\\n\")\n}\n\n\n\nfigure_1.png\n\n\nChi-squared test for given probabilities\n\ndata:  observed_counts\nX-squared = 4.9697, df = 8, p-value = 0.7608\n\n# The data follows Benford's Law."
  },
  {
    "objectID": "posts/causal-relationship/index.html",
    "href": "posts/causal-relationship/index.html",
    "title": "인과관계의 조건과 탐색",
    "section": "",
    "text": "영국의 철학자 데이비드 흄이 그의 저서 “인간 지성의 조사”에서 인간 지식의 인과관계를 인식하는데 있어 필요한 세 가지 조건을 제시하였습니다. 조건은 아래와 같습니다 :\n\n인접성(contiguity) : 인과 관계에 있는 두 사건이 공간적으로나 시간적으로 가까워야 함을 의미합니다. 예를 들어, 어떤 원인이 결과를 초래하려면, 그 원인과 결과는 공간적으로 인접해 있어야 하며, 시간적으로도 가까운 시점에서 발생해야 합니다. 이 개념은 인과 관계를 분석할 때 중요한 역할을 합니다.\n시간적 선행(temporal precedence) : 인과 관계에 있는 두 사건이 시간 순서에 따라 발생해야 함을 의미합니다. 원인이 항상 결과에 선행해야 합니다. 즉, 원인이 먼저 발생하고 그 다음에 결과가 발생합니다.\n지속적 연관(constant conjunction) : 지속적 연관은 인과 관계에 있는 두 사건이 일정하게 발생한다는 것을 의미합니다. 이 개념은 두 사건 사이에 인과 관계가 있다고 추론할 때 중요한 역할을 합니다. 예를 들어, 어떤 원인이 항상 특정한 결과와 함께 발생하면, 그 두 사건 사이에 인과 관계가 있다고 추론할 수 있습니다.\n\n마찬가지로 영국의 철학자인 존 스튜어트 밀의 경우, 인과관계를 탐색하는 데에 있어 5가지 방법을 제시하였습니다. 이는 ’Mill’s Method’라고도 불립니다. 다섯 가지 방법은 하기와 같습니다:\n\n동시 발생의 방법 (Method of Agreement): 서로 다른 상황에서 결과가 발생할 때 공통적으로 존재하는 요소를 인과 요인으로 간주합니다.\n차이의 방법 (Method of Difference): 결과가 발생하는 상황과 발생하지 않는 상황에서의 유일한 차이점을 인과 요인으로 간주합니다.\n공변의 방법 (Method of Concomitant Variation): 인과 요인과 결과가 함께 변하는 정도를 관찰하여 인과 관계를 찾습니다.\n잔여의 방법 (Method of Residues): 이미 알려진 인과 요인을 제외한 후, 남아있는 결과와 요인 사이의 인과 관계를 찾습니다.\n역설적 합리화의 방법 (Method of Contra-positive Reasoning): 인과 요인이 없을 때 결과도 발생하지 않는 것을 관찰하여 인과 관계를 확인합니다."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "front_image.jpg\n\n\n\nClustering은..\n클러스터링(clustering)은 데이터 마이닝과 머신러닝에서 사용되는 비지도 학습(unsupervised learning) 방법 중 하나입니다. 클러스터링의 목적은 주어진 데이터셋에서 유사한 특성을 가진 데이터 포인트들을 그룹화하는 것입니다. 이렇게 그룹화된 데이터 포인트들의 집합을 클러스터(cluster)라고 합니다. 클러스터링은 고객 세분화, 이미지 분류, 문서 군집화, 이상치 탐지, 추천 시스템 등 다양한 분야에서 활용됩니다. 클러스터링을 통해 데이터의 구조와 패턴을 발견하고, 새로운 인사이트를 얻거나 의사 결정을 돕는 데 도움이 됩니다. 주요 기법은 다음과 같습니다:\n\n중심 기반 클러스터링(Centroid based clustering): 각 클러스터의 중심을 정의하고, 각 데이터 포인트를 가장 가까운 중심에 할당하는 방식으로 클러스터를 형성합니다. 중심과 데이터 포인트 간 거리의 제곱합을 최소화하는 방식으로 반복적으로 최적화합니다. K-means clustering\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): 데이터 포인트의 밀도에 기반한 클러스터링 방법으로, 밀도가 높은 영역을 클러스터로 인식하고, 밀도가 낮은 영역은 노이즈로 처리합니다. 이 방법은 클러스터의 개수를 미리 지정할 필요가 없으며, 노이즈에 대한 처리가 가능합니다. DBSCAN\n계층적 클러스터링(Hierarchical clustering): 데이터 포인트 간의 거리나 유사도를 기반으로 가장 가까운 데이터 포인트나 클러스터를 병합하는 방식으로 진행됩니다. 이 방법은 덴드로그램(dendrogram)이라는 나무 형태의 구조로 클러스터링 과정을 시각화할 수 있습니다. Hierarchical Clustering\n스펙트럼 클러스터링(Spectral clustering): 데이터의 유사도 행렬을 사용하여 그래프를 구성하고, 그래프의 스펙트럼(고유값과 고유벡터)을 기반으로 클러스터를 형성합니다. 이 방법은 비선형 구조를 가진 데이터에 적합하며, 클러스터의 모양이 복잡한 경우에도 잘 작동합니다.\n\n\n\n데이터 포인트 간의 거리는..\n\n유클리디언 거리 (Euclidean Distance): 두 점 사이의 직선 거리를 계산하는 가장 기본적인 거리 메트릭입니다. 이 거리는 기하학적 공간에서 두 점 사이의 거리를 직관적으로 이해하기 쉽습니다. 유클리디안 거리는 L2 노름(norm)으로도 알려져 있습니다.\n맨하탄 거리 (Manhattan Distance): 각 축에 따라 수직으로 이동하여 두 점 사이의 거리를 계산하는 방법으로, L1 노름(norm)으로도 알려져 있습니다. 이 거리 메트릭은 그리드 기반의 데이터에서 종종 사용됩니다.\n코사인 유사도 (Cosine Similarity): 두 벡터 간의 코사인 각도를 사용하여 유사성을 측정하는 방법입니다. 값의 범위는 -1에서 1까지이며, 1에 가까울수록 벡터 간의 방향이 유사함을 나타냅니다. 이 메트릭은 텍스트 문서와 같이 고차원 데이터에서 유용하게 사용됩니다.\nMahalanobis distance(마할라노비스 거리): 다차원 공간에서 두 데이터 포인트 간의 거리를 측정하는 방법 중 하나입니다. 이 거리 측정 방법은 각 차원의 스케일(scale)과 상호간의 공분산(covariance)을 고려하여 거리를 계산합니다. 즉, 각 차원의 중요성을 고려하여 거리를 계산하기 때문에 특이한 데이터나 이상치(outlier)에 덜 민감하다는 장점이 있습니다."
  },
  {
    "objectID": "posts/covariance/index.html",
    "href": "posts/covariance/index.html",
    "title": "Covariance Matrix",
    "section": "",
    "text": "공분산 행렬은…\n공분산 행렬(covariance matrix)은 변수들 간의 공분산을 요소로 갖는 정방행렬(square matrix)입니다. 공분산은 두 변수가 함께 변하는 정도를 측정하는 값으로, 하나의 변수가 증가할 때 다른 변수가 어떻게 변하는지를 나타냅니다.\n공분산 행렬에서 대각선 요소는 각 변수의 분산(variance)을 나타내며, 이는 해당 변수가 얼마나 퍼져 있는지를 측정합니다. 비대각선 요소는 서로 다른 두 변수 간의 공분산을 나타냅니다.\nn개의 변수를 가진 데이터 세트의 공분산 행렬은 n x n 크기의 정방행렬이며, 행렬의 (i, j) 위치에 있는 요소는 변수 i와 변수 j의 공분산을 나타냅니다.\n예를 들어, 데이터 세트에 변수 X와 변수 Y가 있다고 가정하면 공분산 행렬은 다음과 같이 나타낼 수 있습니다.\n\nCov(X, X)   Cov(X, Y)\nCov(Y, X)   Cov(Y, Y)\n\n여기서 Cov(X, X)는 변수 X의 분산, Cov(Y, Y)는 변수 Y의 분산, Cov(X, Y)와 Cov(Y, X)는 변수 X와 변수 Y의 공분산을 나타냅니다.\n공분산 행렬은 데이터의 선형 관계를 나타내는 정보를 포함하며, 주성분 분석(PCA)과 같은 통계 및 기계 학습 기법에서 중요한 역할을 합니다. 공분산 행렬을 사용하면 데이터의 구조를 파악하고 변수 간의 상관 관계를 분석할 수 있습니다."
  },
  {
    "objectID": "posts/dbscan/index.html",
    "href": "posts/dbscan/index.html",
    "title": "DBSCAN",
    "section": "",
    "text": "Result of DBSCAN\n\n\n\nDBSCAN은..\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)은 밀도 기반의 클러스터링 알고리즘 중 하나로, 1996년에 Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu에 의해 제안되었습니다. DBSCAN은 밀도 기반의 클러스터링을 수행하기 때문에 클러스터의 모양이 원형이 아닌 경우에도 유연하게 적용할 수 있으며, 노이즈를 포함한 데이터에 대해서도 강인한 성능을 보입니다.\nDBSCAN 알고리즘은 다음과 같은 주요 개념을 활용합니다:\n\n이웃 반경(epsilon, ε): 데이터 포인트에서 주어진 거리 내에 있는 이웃 포인트를 찾는 데 사용되는 거리입니다.\n최소 포인트(MinPts): 밀집 영역이라고 간주되기 위해 주어진 이웃 반경 내에 존재해야 하는 포인트의 최소 개수입니다.\n\n알고리즘의 동작 방식은 다음과 같습니다:\n\n데이터셋 내의 모든 포인트를 순회하며, 아직 클러스터에 할당되지 않은 포인트를 선택합니다.\n선택한 포인트에서 epsilon 거리 이내의 이웃 포인트를 찾습니다.\n이웃 포인트의 수가 MinPts 이상이면, 새로운 클러스터를 생성하고 해당 포인트와 그 이웃들을 해당 클러스터에 할당합니다.\n이웃 포인트의 이웃들을 또한 순회하며, MinPts 이상의 이웃을 가지고 있는 포인트를 발견하면 해당 클러스터에 추가합니다. 이 과정을 반복하여 클러스터가 더 이상 확장되지 않을 때까지 수행합니다.\n모든 포인트를 순회할 때까지 1-4 단계를 반복합니다. 이 과정이 끝나면 클러스터가 생성되며, 어떠한 클러스터에도 속하지 않는 포인트는 노이즈로 간주됩니다.\n\nDBSCAN의 주요 장점은 클러스터의 개수를 사전에 지정할 필요가 없으며, 클러스터의 모양에 대한 가정이 없어 다양한 형태의 클러스터를 찾을 수 있다는 점입니다. 또한, 이 알고리즘은 노이즈를 구분하여 클러스터링에 영향을 미치지 않게 처리할 수 있습니다. 단점으로는 높은 차원의 데이터에 대한 성능 저하가 있으며, epsilon과 MinPts와 같은 하이퍼파라미터를 선택하는 것이 어려울 수 있다는 점입니다.\n\n\nDBSCAN 예시\nlibrary(tidyverse)\nlibrary(reticulate)\n\n# Get Smile Data\ncircle_df &lt;- data.frame(\n  t = seq(0, 2 * pi, length.out = 100),\n  x = 0,\n  y = 0,\n  r = 1\n)\ncircle_df$x &lt;- circle_df$r * cos(circle_df$t) + rnorm(100,0,0.03)\ncircle_df$y &lt;- circle_df$r * sin(circle_df$t) + rnorm(100,0,0.03)\n\neyes_df &lt;- data.frame(\n  x = rep(c(-0.3, 0.3),50) + rnorm(50,0,0.03),\n  y = rep(c(0.4, 0.4),50) + rnorm(50,0,0.03)\n)\n\nmouth_df &lt;- data.frame(\n  t = seq(-pi, 0, length.out = 100),\n  x = 0,\n  y = -0.3,\n  r = 0.5\n)\nmouth_df$x &lt;- mouth_df$r * cos(mouth_df$t) + rnorm(100,0,0.03)\nmouth_df$y &lt;- mouth_df$y + mouth_df$r * sin(mouth_df$t) + rnorm(100,0,0.03)\n\nsmile_df &lt;- rbind(circle_df[,2:3],eyes_df,mouth_df[,2:3])\n# Run DBSCAN\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\n\nX = r.smile_df\n\ndbscan = DBSCAN(eps=0.15, min_samples=10)\ndbscan.fit(X)\nlabels = dbscan.labels_\nlabels\n# Plot result\nsmile_df &lt;- smile_df |&gt; mutate(label = as.factor(py$labels))\nggplot(smile_df,aes(x = x, y = y, color = label)) + geom_point()"
  },
  {
    "objectID": "posts/dimension-reduction/index.html",
    "href": "posts/dimension-reduction/index.html",
    "title": "Dimension Reduction (차원 축소)",
    "section": "",
    "text": "차원의 저주란..\n차원의 저주(curse of dimensionality)는 고차원 데이터에서 발생하는 여러 문제를 총체적으로 설명하는 용어입니다. 데이터의 차원이 증가할수록, 데이터 포인트 간의 거리가 점점 멀어져서 데이터가 희소해지는 현상이 발생합니다. 이러한 현상은 다음과 같은 문제를 야기합니다:\n\n거리 측정의 어려움: 고차원 공간에서는 데이터 포인트 간의 거리가 크게 증가하므로, 가까운 이웃을 찾거나 클러스터링하는 것이 어렵습니다.\n계산 복잡도 증가: 고차원 데이터에서 모델을 학습하거나 예측을 수행하는 데 필요한 계산량이 크게 증가합니다.\n과적합 위험 증가: 데이터의 차원이 증가할수록, 모델이 학습 데이터에 과도하게 적합되어 일반화 성능이 떨어지는 과적합(overfitting)이 발생하기 쉽습니다.\n특성 선택의 어려움: 고차원 데이터에서는 어떤 특성이 중요한지 판단하거나 적절한 특성 조합을 찾는 것이 어려워집니다.\n\n\n\n차원 축소란..\n차원 축소(dimension reduction)는 고차원 데이터를 저차원으로 변환하는 과정으로, 데이터 과학 및 머신러닝에서 중요한 역할을 합니다. 차원 축소의 필요성은 다음과 같습니다:\n\n데이터 시각화: 고차원 데이터를 2D 또는 3D로 줄여서 데이터의 패턴이나 구조를 쉽게 이해할 수 있게 합니다.\n계산 효율성: 저차원 데이터는 계산량이 줄어들어 머신러닝 모델의 학습 및 예측 속도가 빨라집니다.\n잡음 제거: 차원 축소는 데이터의 중요한 정보를 유지하면서 불필요한 변동성이나 잡음을 제거하는 데 도움이 됩니다.\n과적합 방지: 차원의 저주(curse of dimensionality)로 인해 고차원 데이터에는 과적합(overfitting)이 발생하기 쉽습니다. 차원 축소를 통해 과적합을 방지할 수 있습니다.\n\n\n\n주요 차원 축소 기법\n\nPCA (주성분 분석, Principal Component Analysis): 데이터의 분산을 최대한 보존하는 새로운 축을 찾아 고차원 데이터를 저차원으로 변환합니다. 선형적인 데이터 구조에 적합합니다. Principal Component Analysis\nNMF (음수 미포함 행렬 인수분해, Non-negative Matrix Factorization): 원래의 데이터 행렬을 음수가 아닌 두 개의 행렬의 곱으로 분해합니다. 이 방법은 데이터가 음수가 아닌 특성을 가질 때 유용하며, 데이터의 희소성을 고려할 수 있습니다.\nt-SNE (t-분포 확률적 임베딩, t-Distributed Stochastic Neighbor Embedding): 고차원 데이터의 점들 간 거리를 저차원에서의 확률 분포로 보존하려고 시도하는 비선형 차원 축소 기법입니다. 데이터의 군집 구조나 매니폴드 구조를 잘 보존합니다. t-SNE\nUMAP (Uniform Manifold Approximation and Projection)은 고차원 데이터를 저차원 공간으로 축소하는 비선형 차원 축소 기법 중 하나입니다. UMAP은 t-SNE와 유사하게 데이터의 군집 구조를 보존하면서 고차원 데이터를 시각화하거나 차원 축소하는 데 사용됩니다. 그러나 UMAP은 기하학적 특성을 더 잘 보존하고, 계산 효율성이 더 높아 대규모 데이터셋에도 적용 가능하다는 장점이 있습니다. UMAP"
  },
  {
    "objectID": "posts/dual-number/index.html",
    "href": "posts/dual-number/index.html",
    "title": "이원수와 전진자동미분",
    "section": "",
    "text": "이원수(Dual Number)란..\n이원수(dual number)는 복소수의 일반화로 볼 수 있는 수 체계입니다. 듀얼 넘버는 다음과 같은 형태로 표현됩니다:\n\\[\n\\alpha + {\\beta}{\\epsilon}\n\\]\n여기서 a와 b는 실수이며, ε는 듀얼 단위로, ε^2 = 0 (ε ≠ 0)을 만족합니다. 복소수에서 허수 단위 i와 비슷한 역할을 합니다.\n이원수를 활용한 미분은 오일러의 수학적 아이디어를 기반으로 합니다. 이원수의 성질을 이용하면, 함수의 미분을 정확하게 계산하는데 사용할 수 있습니다.\n이원수를 사용하여 함수 \\(f(x)\\)의 도함수를 구하는 방법은 다음과 같습니다:\n\n함수 \\(f(x)\\)를 이원수 형태로 확장합니다. 즉, \\(x = \\alpha + {\\beta}{\\epsilon}\\)로 설정합니다.\n함수 \\(f(x)\\)에 듀얼 넘버를 대입하여, \\(f(\\alpha + {\\beta}{\\epsilon})\\)를 계산합니다.\n계산된 결과에서 ε에 대한 계수를 찾습니다. 이 계수는 함수 \\(f(x)\\)의 도함수 값이 됩니다.\n\n예를 들어, \\(f(x)=x^2\\)를 미분해 보겠습니다.\n\\(f(x) = (\\alpha + {\\beta}{\\epsilon})^2 = \\alpha^2 + 2\\alpha\\beta\\epsilon + \\beta^2\\epsilon^2\\)\n위 식에서 \\(\\epsilon^2 = 0\\)이므로, 결과는 다음과 같습니다:\n\\(f(\\alpha + \\beta\\epsilon) = (\\alpha + {\\beta}{\\epsilon})^2 = \\alpha^2 + 2\\alpha\\beta\\epsilon\\)\n여기서 ε에 대한 계수는 2a입니다. 따라서 \\(f'(x) = 2x\\)가 됩니다. 이 결과는 기대한 대로 함수 \\(f(x)=x^2\\)의 도함수 값입니다.\n위를 일반화 할 경우, \\(f(\\alpha) + f'(\\alpha)\\cdot\\beta\\epsilon\\)으로 정리할 수 있습니다."
  },
  {
    "objectID": "posts/dummy-variables/index.html",
    "href": "posts/dummy-variables/index.html",
    "title": "N-1 더미 변수를 활용하는 이유",
    "section": "",
    "text": "Sample Data 생성\nlibrary(tidyverse)\n\nset.seed(42)\nsmp_dt &lt;- tibble(Season = rep(c(\"Spring\", \"Summer\", \"Fall\", \"Winter\"),2),\nSales = as.integer(runif(8,min = 300, max = 1000)))\n\n# A tibble: 8 × 2\n  Season Sales\n  &lt;chr&gt;  &lt;int&gt;\n1 Spring   940\n2 Summer   955\n3 Fall     500\n4 Winter   881\n5 Spring   749\n6 Summer   663\n7 Fall     815\n8 Winter   394\n\n\n직관적으로..\n단순히 생각하였을 때, n개의 더미 변수를 사용할 경우, Spring ~ Winter까지의 열 벡터를 합하면, Intercept 열이 되어, 선형독립을 만족치 못한다. 그렇기에 역행렬이 구해지지 않는 것이 당연하다고 볼 수 있다. 하기 예제로 실제로 그러한지 확인해보자.\n      Spring Summer Fall Winter Intercept Sales\n[1,]      1      0    0      0         1   940\n[2,]      0      1    0      0         1   955\n[3,]      0      0    1      0         1   500\n[4,]      0      0    0      1         1   881\n[5,]      1      0    0      0         1   749\n[6,]      0      1    0      0         1   663\n[7,]      0      0    1      0         1   815\n[8,]      0      0    0      1         1   394\n\n\nn 더미 변수의 경우..\nsmp_dt_dummy_1 &lt;-\ntibble( \n      Spring = c(1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L),\n      Summer = c(0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L),\n        Fall = c(0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L),\n      Winter = c(0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L),\n      Intercept = rep(1, 8),\n      Sales = smp_dt$Sales\n)\nsmp_dt_dummy_1_mat &lt;- smp_dt_dummy_1 |&gt; as.matrix()\nsmp_dt_dummy_1_mat\n\n      Spring Summer Fall Winter Intercept Sales\n[1,]      1      0    0      0         1   940\n[2,]      0      1    0      0         1   955\n[3,]      0      0    1      0         1   500\n[4,]      0      0    0      1         1   881\n[5,]      1      0    0      0         1   749\n[6,]      0      1    0      0         1   663\n[7,]      0      0    1      0         1   815\n[8,]      0      0    0      1         1   394\nLeast Square Formula 에 따라 \\((A^TA)^{-1} \\cdot A^Tb\\) 를 계산하고자 할 때, \\(Det(A^TA)\\)의 값이 0이 되어, 역행렬을 구할 수 없다. 에러 메시지를 통해 해당 행렬이 선형적으로 종속되어있음을 확인할 수 있다.\nsolve(t(smp_dt_dummy_1_mat[,1:5]) %*% smp_dt_dummy_1_mat[,1:5])\n %*% t(smp_dt_dummy_1_mat[,1:5]) %*% (smp_dt_dummy_1_mat[,6])\n#---------------------------------------------------------------------\nError in solve.default(t(smp_dt_dummy_1_mat[, 1:5]) %*% smp_dt_dummy_1_mat[,  : \n  Lapack routine dgesv: system is exactly singular: U[5,5] = 0\n#------------------------------------------------------------------------\ndet(t(smp_dt_dummy_1_mat[,1:5]) %*% smp_dt_dummy_1_mat[,1:5])\n[1] 0\n\n\nn-1 더미 변수의 경우..\nsmp_dt_dummy_2 &lt;-\n  tibble(\n      Spring = c(1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L),\n      Summer = c(0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L),\n        Fall = c(0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L),\n      Intercept = rep(1, 8),\n      Sales = smp_dt$Sales\n  )\n\nsmp_dt_dummy_2\n\n      Spring Summer Fall Intercept Sales\n[1,]      1      0    0         1   940\n[2,]      0      1    0         1   955\n[3,]      0      0    1         1   500\n[4,]      0      0    0         1   881\n[5,]      1      0    0         1   749\n[6,]      0      1    0         1   663\n[7,]      0      0    1         1   815\n[8,]      0      0    0         1   394\n반면 n-1 dummy variables의 경우, Least Square Formula의 계산이 제대로 된다.\nsolve(t(smp_dt_dummy_2_mat[,1:4]) %*% smp_dt_dummy_2_mat[,1:4]) %*% \nt(smp_dt_dummy_2_mat[,1:4]) %*% (smp_dt_dummy_2_mat[,5])\n\n           [,1]\nSpring    207.0\nSummer    171.5\nFall       20.0\nIntercept 637.5\n\nlm(Sales ~., smp_dt_dummy_2)\n\nCall:\nlm(formula = Sales ~ ., data = smp_dt_dummy_2)\n\nCoefficients:\n(Intercept)       Spring       Summer         Fall\n      637.5        207.0        171.5         20.0"
  },
  {
    "objectID": "posts/ensemble-learning/index.html",
    "href": "posts/ensemble-learning/index.html",
    "title": "Ensemble Learning",
    "section": "",
    "text": "Ensemble Learning은..\n앙상블 학습(Ensemble learning)은 여러 개의 기본 학습 모델(base learners)을 결합하여 더 강력한 예측 모델을 만드는 머신러닝 방법입니다. 앙상블 학습의 핵심 아이디어는 개별 모델의 예측 결과를 종합하여, 더 나은 예측 성능을 달성하고자 하는 것입니다. 앙상블 학습은 일반적으로 높은 성능과 안정성을 제공하며, 과적합(overfitting)을 방지하는 효과도 있습니다.\n앙상블 학습에는 여러 가지 전략이 있으며, 대표적인 방법으로는 다음과 같은 것들이 있습니다:\n\n배깅(Bagging, Bootstrap Aggregating): 배깅은 여러 개의 기본 학습 모델을 병렬로 훈련시키고, 그 결과를 평균(회귀) 또는 투표(분류) 방식으로 종합합니다. 학습 데이터셋을 여러 개의 부트스트랩(bootstrap) 샘플로 생성하여 각 모델을 별도로 학습시킵니다. 이를 통해 모델의 분산을 줄이고 과적합을 방지할 수 있습니다. 대표적인 예로 랜덤 포레스트(Random Forest)가 있습니다. Random Forest\n부스팅(Boosting): 부스팅은 기본 학습 모델을 순차적으로 훈련시키면서, 이전 모델의 오차를 보완하는 방식으로 작동합니다. 각 모델의 가중치를 조절하여, 오차가 큰 데이터 포인트에 더 집중하게 합니다. 이를 통해 모델의 편향을 줄이고 성능을 향상시킬 수 있습니다. 대표적인 예로 에이다부스트(AdaBoost), 그레디언트 부스팅(Gradient Boosting), XGBoost, LightGBM 등이 있습니다. Gradient Boosting\n\n앙상블 학습은 서로 다른 알고리즘 또는 하이퍼파라미터를 사용한 다양한 모델을 결합하여 전체적인 성능을 향상시키는 데에 초점을 맞춥니다. 이러한 접근 방식은 각 모델의 장점을 활용하고, 서로 다른 모델이 가진 단점을 상쇄하여 더 안정적이고 일반화된 예측을 수행할 수 있게 합니다.\n앙상블 학습의 몇 가지 주요 이점은 다음과 같습니다:\n\n성능 향상: 앙상블 학습은 여러 모델의 예측력을 결합하여 더 높은 성능의 모델을 만듭니다. 이는 각 모델이 가진 지식과 전문성을 종합하고, 다양한 문제 해결 방식을 고려할 수 있기 때문입니다.\n과적합 방지: 앙상블 학습은 여러 모델을 결합함으로써 과적합을 완화할 수 있습니다. 각 모델은 서로 다른 관점에서 데이터를 학습하므로, 단일 모델이 가진 과적합 문제를 상쇄하는 효과가 있습니다.\n일반화 능력 향상: 앙상블 학습은 다양한 모델을 통합함으로써 더 일반적인 패턴과 관계를 파악할 수 있습니다. 이로 인해 앙상블 모델은 새로운 데이터에 대한 예측 능력이 향상됩니다.\n안정성: 앙상블 학습은 여러 모델을 사용하기 때문에, 단일 모델의 결함이나 노이즈에 영향을 받지 않는 안정적인 예측을 수행할 수 있습니다."
  },
  {
    "objectID": "posts/entropy/index.html",
    "href": "posts/entropy/index.html",
    "title": "Entropy 쉽게 이해하기",
    "section": "",
    "text": "Entropy란..\n엔트로피(entropy)는 정보 이론에서 확률 변수의 불확실성을 측정하는 지표입니다. 엔트로피는 어떤 정보를 표현하기 위해 필요한 평균 비트 수를 나타내며, 이를 통해 정보의 복잡성이나 압축 가능성을 파악할 수 있습니다. 높은 엔트로피는 높은 불확실성을 의미하며, 낮은 엔트로피는 낮은 불확실성을 의미합니다.\n쉽게 설명하자면, 한 그룹에서의 놀라움의 기대값이라고 볼 수 있습니다. 여기서 놀라움이란, \\(log(\\frac{1}{p(x)})\\)로 표현됩니다. 로또에 당첨이 될 확률이 매우 낮기에, 당첨되면 놀라움이 큰 것과 같은 맥락으로 이해할 수 있습니다.\n90%의 확률로 앞면이 나오는 동전을 100번 던지는 것을 예로 들어보겠습니다:\n\n\n\n\n앞\n뒤\n\n\n\n\n확률\n0.9\n0.1\n\n\n놀라움\n0.15\n3.32\n\n\n\n100번을 던질 경우의 놀라움은 \\((0.9 \\times 100 \\times 1.5) + (0.1 \\times 100 \\times 3.32)\\) 입니다. 이 놀라움의 기대값 즉 Entropy는 \\(\\frac{(0.9 \\times 100 \\times 1.5) + (0.1 \\times 100 \\times 3.32)}{100}\\)로, 0.47입니다.\n다만, 여기서 던진 횟수는 약분이 되기에 위의 Entropy를 일반화하면 아래와 같이 표기됩니다:\n\\[\n\\sum log(\\frac{1}{p(x)}) \\cdot p(x)\n\\]"
  },
  {
    "objectID": "posts/error-metrics/index.html",
    "href": "posts/error-metrics/index.html",
    "title": "1종 오류 2종 오류 헷갈리지 않기",
    "section": "",
    "text": "1종 오류 / 2종 오류\n쉽게 설명하자면, 1종 오류는 경솔한 것 / 2종 오류는 답답한 것으로 표현할 수 있습니다.\n코로나 검사를 예를 들어봅시다. 코로나에 걸리지 않은 사람을 코로나로 진단하는 것, 즉 경솔함은 1종 오류로 판단할 수 있습니다.\n코로나에 걸린 사람을 코로나에 걸리지 않았다고 진단하는 것, 즉 답답함은 2종 오류로 판단됩니다.\n\n즉 상기 그림에서, FP는 1종 오류로 FN은 2종 오류입니다."
  },
  {
    "objectID": "posts/gpt-prompt/index.html",
    "href": "posts/gpt-prompt/index.html",
    "title": "GPT 템플릿",
    "section": "",
    "text": "GPT에게 할 수 있는 Command\n\nKeep going : 컨티뉴보다 이쪽이 더 확실\nMake a list of : 목록을 작성하라. 아래쪽의 ‘쓰다’ 만큼 넓은 범위를 참조함. 글머리 기호 붙여서 알기 쉽게 정리해줌.\nExplain : 모든 주제에 대한 간단한 설명을 얻기 위해 사용\nImprove : 컨텐츠를 제공하고 GPT에 정확도를 향상시키거나 컨텐츠를 추가하도록 요청\nPlan : 목표를 더 작은 단계로 분류하도록 요청\nSummarize : 요약하도록 요청\nExpand : 이전 답변에 대한 자세한 내용을 설명하도록 요청. 또는 내가 위나 아래에 제공한 문장 또는 장면을 더 자세히 풀어서 설명해줌.\n\n\n\n언어 모델이 선택한 최적의 프롬프트\n\n“Let’s work this out in a step by step way to be sure we have the right answer.”\n출처: [2211.01910] Large Language Models Are Human-Level Prompt Engineers\n\n\nGPT 코딩\nAct as CODEX (“COding DEsign eXpert”), an expert coder with experience in multiple coding languages. Always follow the coding best practices by writing clean, modular code with proper security measures and leveraging design patterns. You can break down your code into parts whenever possible to avoid breaching the chatgpt output character limit. Write code part by part when I send “continue”. If you reach the character limit, I will send “continue” and then you should continue without repeating any previous code. Do not assume anything from your side; please ask me a numbered list of essential questions before starting. If you have trouble fixing a bug, ask me for the latest code snippets for reference from the official documentation. I am using [MacOS], [VSCode] and prefer [brew] package manager. Start a conversation as “CODEX: Hi, what are we coding today?”\n출처: GPT-4 CODEX: Coding Design Expert; A Secret Prompt To Rule Them All | by 𝚃𝚑𝚎 𝙻𝚊𝚝𝚎𝚜𝚝 𝙽𝚘𝚠 ~ 𝙰𝙸 | Mar, 2023 | Artificial Intelligence in Plain English"
  },
  {
    "objectID": "posts/gradient-boosting/index.html",
    "href": "posts/gradient-boosting/index.html",
    "title": "Gradient Boosting",
    "section": "",
    "text": "Gradient Boosting이란..\n그래디언트 부스팅(Gradient Boosting)은 앙상블 학습 기법 중 하나로, 일련의 약한 학습기(보통 결정 트리)를 순차적으로 학습시켜 강력한 모델을 만드는 방법입니다. 그래디언트 부스팅은 손실 함수의 그래디언트(기울기)를 최소화하는 방향으로 모델을 업데이트하는 과정을 반복하며, 이를 통해 각 학습기가 이전 학습기의 오차를 줄이는 방향으로 학습됩니다.\n그래디언트 부스팅의 주요 단계는 다음과 같습니다:\n\n초기 예측: 모든 데이터 포인트에 대해 동일한 초기 예측 값을 설정합니다. 이 값은 일반적으로 평균(회귀) 또는 가장 빈번한 클래스(분류)로 설정됩니다.\n약한 학습기 학습: 첫 번째 약한 학습기(일반적으로 결정 트리)를 학습시키고, 예측 오차를 계산합니다.\n손실 함수 및 그래디언트 계산: 손실 함수(예: 평균 제곱 오차, 로그 손실 등)를 사용하여 예측 오차를 측정하고, 그래디언트를 계산합니다.\n새로운 약한 학습기 학습: 이전 학습기의 오차를 줄이는 방향으로 새로운 약한 학습기를 학습시킵니다. 이 과정은 그래디언트에 음의 가중치를 부여하여 이루어집니다.\n학습기 결합: 모든 약한 학습기의 예측을 가중치를 적용하여 결합합니다. 이렇게 하여 최종 예측이 개선됩니다.\n수렴 여부 확인: 손실 함수 값이 더 이상 개선되지 않거나 지정된 반복 횟수에 도달할 때까지 단계 2-5를 반복합니다.\n\n그래디언트 부스팅은 다양한 문제에서 높은 성능을 보이며, 회귀와 분류 문제 모두에 적용할 수 있습니다. XGBoost, LightGBM 및 CatBoost와 같은 그래디언트 부스팅 구현은 대용량 데이터셋에서도 빠르게 학습되도록 최적화되어 있으며, 고성능 그래디언트 부스팅 모델을 생성하는 데 널리 사용됩니다.\n\n\nHistGradientBoosting 예제\nr\n# load required library\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(reticulate)\n\n\npython\n# load required library\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n\n\npython\n# Split Data\ndata = load_breast_cancer()\nX, y = data.data, data.target\ncolname_x = data.feature_names\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\npython\n# Init Model\nmodel = HistGradientBoostingClassifier(random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n\nr\n# Calc Accuracy\nX_test &lt;- as_tibble(py$X_test,.name_repair)\ny_test &lt;- tibble(py$y_test)\ny_pred = tibble(py$y_pred)\n\nresult &lt;- X_test |&gt; bind_cols(y_test,y_pred)\ncolnames(result) &lt;- c(py$colname_x,\"truth\",\"estimate\")\nresult$truth &lt;- as.factor(result$truth)\nresult$estimate &lt;- as.factor(result$estimate)\n\nyardstick::accuracy(result,truth,estimate)\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.974"
  },
  {
    "objectID": "posts/hierarchical/index.html",
    "href": "posts/hierarchical/index.html",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Hierarchical Clustering은..\n데이터 포인트 간의 거리를 기반으로 클러스터를 형성하는데, 처음에는 각 데이터 포인트가 하나의 클러스터로 간주되며 가장 가까운 클러스터끼리 병합하는 과정을 반복하면서 최종적으로 하나의 클러스터로 합쳐집니다. 계층적 클러스터링은 덴드로그램(dendrogram)을 사용하여 클러스터 구조를 시각화할 수 있습니다.\n\n\nHierachical Clustring의 병합 방법 (Linkage Method)\n\n완전 연결법 (Complete Linkage): 클러스터 간 거리를 두 클러스터에 속하는 모든 데이터 포인트 쌍의 거리 중 최대값으로 정의하는 방법입니다. 이 방법은 비교적 밀집되고 잘 구분된 클러스터를 형성하는 경향이 있습니다. (method = “complete”)\n단일 연결법 (Single Linkage): 클러스터 간 거리를 두 클러스터에 속하는 모든 데이터 포인트 쌍의 거리 중 최소값으로 정의하는 방법입니다. 이 방법은 길고 늘어진 체인 형태의 클러스터를 형성할 수 있으며, 노이즈에 민감한 특성이 있습니다. (method = “single”)\n평균 연결법 (Average Linkage): 클러스터 간 거리를 두 클러스터에 속하는 모든 데이터 포인트 쌍의 거리의 평균값으로 정의하는 방법입니다. 이 방법은 노이즈에 상대적으로 덜 민감하며, 단일 연결법에 비해 더 균형 잡힌 클러스터를 형성하는 경향이 있습니다. (method = “average”)\n중심 연결법 (Centroid Linkage): 클러스터 간 거리를 두 클러스터의 중심(centroid, 즉 평균 벡터) 간의 거리로 정의하는 방법입니다. 이 방법은 보다 밀집된 클러스터를 형성하는 경향이 있지만, 거리 척도의 선택에 민감한 특성이 있습니다. (method = “centroid”)\n\n\n\nHierarchical Clustering 예제\n\n\n\nResult of Hierachical clustering\n\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(cluster)\n\n# Extract numeric columns\niris_numeric &lt;- iris[, 1:4]\n\n# Standardize the data\niris_std &lt;- scale(iris_numeric)\n\n# Compute Euclidean distance matrix\ndist_matrix &lt;- dist(iris_std, method = \"euclidean\")\n\n# Perform hierarchical clustering\nhc &lt;- hclust(dist_matrix, method = \"complete\")\n\n# Plot dendrogram\nplot(hc, labels = iris$Species, cex = 0.6, hang = -1)\n\n# Cut the tree into clusters\nk &lt;- 3 # Number of clusters\ncluster_assignments &lt;- cutree(hc, k)\n\n# Add cluster assignments to the Iris dataset\n\niris &lt;- iris |&gt; mutate(Cluster = case_when(\n  cluster_assignments == 1 ~ \"setosa\",\n  cluster_assignments == 2 ~ \"versicolor\",\n  .default = \"virginica\"\n))\n\niris$Cluster &lt;- as.factor(iris$Cluster)\n\niris |&gt; yardstick::conf_mat(Species, Cluster)\niris |&gt; yardstick::accuracy(Species, Cluster)\nTruth\nPrediction   setosa versicolor virginica\n  setosa         49          0         0\n  versicolor      1         21         2\n  virginica       0         29        48\n\naccuracy multiclass     0.787"
  },
  {
    "objectID": "posts/k-means/index.html",
    "href": "posts/k-means/index.html",
    "title": "K-means clustering",
    "section": "",
    "text": "image of kmeans\n\n\n\nK-means 알고리즘은..\nCentroid Based Clustering 기법 중 하나로, 데이터 포인트들을 K개의 군집으로 나누는 비지도 학습(unsupervised learning) 방법입니다. K-means 알고리즘은 다음과 같은 과정을 거쳐 데이터를 군집화합니다.\n\n초기화: K개의 초기 중심점(centroid)을 무작위로 선택하거나, 데이터 포인트에서 무작위로 추출합니다.\n할당: 각 데이터 포인트를 가장 가까운 중심점에 할당하여, K개의 군집을 생성합니다. 일반적으로 유클리디안 거리(Euclidean distance)를 사용하여 거리를 측정합니다.\n업데이트: 각 군집의 중심점을 새롭게 계산합니다. 새로운 중심점은 해당 군집에 속한 데이터 포인트들의 평균 위치입니다.\n수렴 여부 확인: 중심점의 위치가 더 이상 변하지 않거나, 미리 정한 반복 횟수에 도달할 때까지 2단계와 3단계를 반복합니다.\n\nK-means 알고리즘은 간단하고 이해하기 쉬워 널리 사용되지만, 몇 가지 단점도 가지고 있습니다. 주요 단점으로는 다음과 같습니다.\n\nK값을 미리 설정해야 하는데, 이는 최적의 군집 개수를 알기 어려운 경우 문제가 될 수 있습니다.\n초기 중심점 선택에 따라 결과가 달라질 수 있어, 여러 번 실행하여 최적의 결과를 찾아야 할 수도 있습니다.\n군집의 모양이 원형이 아닌 경우나 군집 크기가 다른 경우, 성능이 저하될 수 있습니다.\n\n이러한 단점에도 불구하고 K-means 알고리즘은 군집화에 사용되는 대표적인 방법 중 하나로 널리 적용되고 있습니다.\nlibrary(cluster)\n\niris_data &lt;- iris[, -5]\n\n# Apply K-means (K=3)\nset.seed(42)\nkmeans_result &lt;- kmeans(iris_data, centers = 3, nstart = 20)\n\ntable(iris$Species, kmeans_result$cluster)\nclusplot(iris_data, kmeans_result$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)\n              1  2  3\n  setosa     50  0  0\n  versicolor  0 48  2\n  virginica   0 14 36"
  },
  {
    "objectID": "posts/mutual-information/index.html",
    "href": "posts/mutual-information/index.html",
    "title": "상호 정보량 (Mutual Informaion)",
    "section": "",
    "text": "상호 정보량은..\n상호 정보량(mutual information, MI)은 정보 이론(Information Theory)에서 두 확률 변수 간의 의존성을 측정하는 방법입니다. 상호 정보량은 두 확률 변수가 서로 얼마나 많은 정보를 공유하는지를 나타내며, 그 값이 클수록 두 변수 사이의 의존성이 높다고 볼 수 있습니다. 상호 정보량은 엔트로피와 조건부 엔트로피를 기반으로 한 수식으로 계산됩니다.\n두 확률 변수 X와 Y가 주어졌을 때, 상호 정보량은 다음과 같이 정의됩니다:\n\\[\nMI(X; Y) = \\displaystyle \\sum_{x \\in X }\\sum_{y \\in Y}p(x,y)log[\\frac{p(x,y)}{p(x),p(y)}]\n\\]\n\n\nR-squared와의 차이\n상호 정보량(mutual information, MI)과 결정계수(R-squared)는 두 변수 간의 관계를 측정하는 데 사용되는 통계적인 지표입니다. 그러나 이들은 서로 다른 가정과 계산 방법을 기반으로 하며, 각각 다른 측면을 강조합니다.\n\nMI는 두 변수 간의 일반적인 의존성을 측정하는 반면, R-squared는 두 변수 간의 선형 관계를 측정합니다. MI는 선형, 비선형, 모노토닉 등 모든 종류의 관계를 고려하지만, R-squared는 선형 관계에만 국한됩니다.\nMI는 양수 또는 0의 값을 가질 수 있으며, 값이 클수록 두 변수가 많은 정보를 공유한다는 것을 의미합니다. R-squared의 값은 0과 1 사이에 있으며, 값이 1에 가까울수록 선형 관계가 강하다는 것을 나타냅니다."
  },
  {
    "objectID": "posts/pca/index.html",
    "href": "posts/pca/index.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "좌측 3D 모델을 2차원으로 옮긴 모습.\n\nPCA는..\n주성분 분석(PCA, Principal Component Analysis)은 고차원의 데이터를 저차원의 데이터로 축소하거나 변환하는 데 사용되는 통계적 기법입니다. 이 방법은 데이터의 분산(variance)을 최대한 보존하면서 데이터를 새로운 좌표계로 변환합니다. PCA는 다음과 같은 과정을 거칩니다.\n\n데이터의 공분산 행렬(covariance matrix)을 계산합니다.\n공분산 행렬의 고유값과 고유벡터를 찾습니다.\n고유값이 큰 순서대로 해당 고유벡터를 정렬합니다. 이렇게 정렬된 고유벡터가 주성분(principal components)이며, 데이터의 분산을 나타냅니다.\n원하는 차원의 수만큼 주성분을 선택합니다. 예를 들어, 3차원 데이터를 2차원으로 축소하려면 가장 큰 두 개의 고유값에 해당하는 고유벡터를 선택합니다.\n선택한 주성분에 데이터를 투영하여 저차원의 데이터를 얻습니다.\n\nPCA의 주요 목적은 다음과 같습니다:\n\n차원 축소: 고차원의 데이터를 저차원의 데이터로 변환하여 데이터의 복잡성을 줄이고 계산 비용을 절감합니다.\n시각화: 고차원의 데이터를 2D 또는 3D로 표현하여 데이터의 패턴, 클러스터 또는 이상치를 쉽게 시각적으로 확인할 수 있게 합니다.\n노이즈 제거: 데이터의 주요 정보를 보존하면서 노이즈를 제거하여 데이터를 깨끗하게 만듭니다.\n특성 선택 및 추출: 데이터에서 가장 중요한 특성을 선택하거나 새로운 특성을 추출하여 데이터를 더 효과적으로 분석할 수 있습니다.\n\nPCA는 데이터 전처리, 시각화, 기계 학습 및 패턴 인식 등 다양한 분야에서 널리 사용되는 기법입니다. 하지만 주성분 분석에는 몇 가지 한계가 있습니다. 예를 들어, PCA는 선형적인 관계를 가정하기 때문에 비선형적인 데이터 구조를 제대로 표현하지 못할 수 있습니다. 이러한 경우에는 커널 PCA, t-SNE, UMAP 등 다른 차원 축소 기법을 고려할 수 있습니다.\n\n\nPCA 예제\n# Load required library\nlibrary(Rvcg)\nlibrary(rgl)\nlibrary(tidyverse)\n\n# Load 3D Dataset\nstl_mesh &lt;- vcgImport(\"D:/Utah_teapot_(solid).stl\")\n\n# Convert stl file to df\nvertices &lt;- cbind(stl_mesh$vb[1,],stl_mesh$vb[2,],stl_mesh$vb[3,])\n\n# Compress data in 2D Using PCA\npca_result &lt;- prcomp(vertices, center = TRUE, scale. = TRUE)\nreduced_data &lt;- pca_result$x[, 1:2]\nreduced_df &lt;- data.frame(reduced_data)\ncolnames(reduced_df) &lt;- c(\"PC1\", \"PC2\")\n\n# Visualize 2D data\n\nggplot(reduced_df, aes(x = PC2, y = PC1)) +\n  geom_point() +\n  theme_minimal() +\n  ggtitle(\"Teapot 3D Data Reduced to 2D using PCA\")\n\n\nPCA from Scratch\n# 예제 데이터 생성\nset.seed(42)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100, sd = 0.5)\nz &lt;- -x + y + rnorm(100, sd = 0.5)\ndata &lt;- data.frame(x, y, z)\n\n# 1. 데이터의 공분산 행렬 계산\ndata_cov &lt;- cov(data)\n\n# 2. 공분산 행렬의 고유값과 고유벡터 찾기\neigen_result &lt;- eigen(data_cov)\n\n# 고유값\neigen_values &lt;- eigen_result$values\n# 고유벡터\neigen_vectors &lt;- eigen_result$vectors\n\n# 3. 원하는 차원의 수만큼 주성분 선택 (여기서는 2차원으로 축소)\nnum_dimensions &lt;- 2\nselected_eigen_vectors &lt;- sorted_eigen_vectors[, 1:num_dimensions]\n\n# 4. 선택한 주성분에 데이터를 투영하여 저차원의 데이터 얻기\ndata_centered &lt;- scale(data, center = TRUE, scale = FALSE)\nreduced_data &lt;- data_centered %*% selected_eigen_vectors\n\n# 결과 출력\nreduced_data &lt;- data.frame(reduced_data)\ncolnames(reduced_data) &lt;- c(\"PC1\", \"PC2\")\nprint(reduced_data)"
  },
  {
    "objectID": "posts/random-forest/index.html",
    "href": "posts/random-forest/index.html",
    "title": "Random Forest",
    "section": "",
    "text": "Random Forest는..\n랜덤 포레스트는 배깅(Bagging)의 한 형태로서, 여러 개의 결정 트리를 조합하여 예측 결과를 도출하는 앙상블 기법입니다. 이 방법은 결정 트리의 가장 큰 문제점 중 하나인 과적합(overfitting)을 효과적으로 해결하고, 높은 예측 성능을 보여줍니다.\n랜덤 포레스트의 기본 아이디어는 다음과 같습니다:\n\n원본 데이터셋에서 부트스트랩 샘플링을 통해 여러 개의 샘플을 생성합니다. 이때, 중복 허용으로 원본 데이터셋과 같은 크기의 샘플을 만듭니다.\n각 샘플로부터 결정 트리를 학습시킵니다. 이 과정에서 무작위로 선택된 특성(feature)의 부분집합을 사용해 노드를 분할하는 최적의 분할을 찾습니다. 이러한 무작위성은 결정 트리 간의 상관관계를 낮추고, 다양성을 높여 과적합을 방지합니다.\n테스트 데이터셋에 대한 예측을 수행할 때, 각 결정 트리의 예측을 모아 최종 예측 결과를 도출합니다. 분류 문제의 경우 다수결 투표(Majority Voting) 방식을 사용해 최종 예측 클래스를 결정하며, 회귀 문제의 경우 각 트리의 예측값의 평균을 사용합니다.\n\n\n\nFashion MNIST 데이터를 활용한 Random Forest 예제\n [출처] : Fashion MNIST | Kaggle\nlibrary(tidymodels)\n\n## Load Dataset\nfmnist &lt;- arrow::read_feather(\"fashion-mnist.feather\")\nfmnist$label &lt;- as.factor(fmnist$label)\n\nset.seed(42)\nsplit_index &lt;- initial_split(fmnist,3/4,label)\ntrain_tb &lt;- training(split_index)\ntest_tb &lt;- testing(split_index)\n\n## Make workflow\n\nfmnist_recipe &lt;- recipe(label ~ ., data = train_tb) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; \n  prep()\n\nfmnist_rf &lt;- rand_forest(trees = 100, mtry = sqrt(784)) |&gt; \n  set_engine(\"ranger\", importance = \"impurity\") |&gt; \n  set_mode(\"classification\")\n\nfmnist_wf &lt;- workflow() |&gt;\n  add_recipe(fmnist_recipe) |&gt; \n  add_model(fmnist_rf)\n\nfmnist_wf_trained &lt;- fmnist_wf |&gt; fit(train_tb)\n\n# Predict with test dat\npredictions &lt;- predict(fmnist_wf_trained, test_tb) |&gt; \n  bind_cols(test_tb |&gt;  select(label))\n\naccuracy(predictions, truth = label, estimate = .pred_class)\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.882"
  },
  {
    "objectID": "posts/regularization/index.html",
    "href": "posts/regularization/index.html",
    "title": "Regularization (정규화 기법)",
    "section": "",
    "text": "모델의 정규화란…\n정규화 기법은 모델의 복잡도를 줄이고 일반화 성능을 향상시키기 위해 사용됩니다. 일반적인 정규화 기법으로는 릿지 회귀(Ridge Regression), 라쏘 회귀(Lasso Regression), 그리고 엘라스틱넷 회귀(Elastic Net Regression)가 있습니다.\n\n릿지 회귀 (Ridge Regression) - L2 정규화: 릿지 회귀는 선형 회귀에 L2 정규화를 추가한 것입니다. L2 정규화는 회귀 계수의 제곱합에 비례하는 패널티를 손실 함수에 추가합니다. 이 패널티는 모델의 계수를 작게 만들어 과적합을 방지하며, 일반화 성능을 향상시킵니다. 하이퍼파라미터인 람다(lambda)는 정규화 항의 강도를 조절합니다.\n라쏘 회귀 (Lasso Regression) - L1 정규화: 라쏘 회귀는 선형 회귀에 L1 정규화를 추가한 것입니다. L1 정규화는 회귀 계수의 절댓값의 합에 비례하는 패널티를 손실 함수에 추가합니다. 라쏘 회귀는 계수를 정확히 0으로 만들어 희소한 모델을 생성하며, 이를 통해 변수 선택이 이루어집니다. 라쏘 회귀는 릿지 회귀와 마찬가지로 람다(lambda) 하이퍼파라미터를 사용하여 정규화 항의 강도를 조절합니다.\n엘라스틱넷 회귀 (Elastic Net Regression) - L1과 L2 정규화의 조합: 엘라스틱넷 회귀는 L1 정규화와 L2 정규화를 모두 사용하는 방법입니다. 이 방법은 라쏘 회귀의 변수 선택 기능과 릿지 회귀의 일반화 성능을 모두 활용할 수 있습니다. 엘라스틱넷 회귀에서는 람다(lambda)와 알파(alpha) 두 가지 하이퍼파라미터를 사용하여 정규화 항의 강도와 L1, L2 정규화의 비율을 조절합니다.\n\n [출처] Lasso and Ridge Regression in Python Tutorial | DataCamp\n\n\n적절한 정규화 기법의 선택법\n\n상관 관계가 높은 특성이 많은 경우: 엘라스틱넷 회귀가 더 적합한 선택일 수 있습니다. L1 정규화는 변수 선택에 도움이 되지만, 상관 관계가 높은 특성들 중 하나만 선택하고 다른 특성들을 제외할 위험이 있습니다. 이러한 경우, L1과 L2 정규화의 조합인 엘라스틱넷 회귀가 더 적절한 성능을 보일 수 있습니다.\n특성 수가 관측치 수보다 많은 경우: 릿지 회귀가 적합한 선택일 수 있습니다. 라쏘 회귀는 최대 관측치 수만큼의 변수를 선택하므로, 이 경우에는 릿지 회귀가 더 나은 일반화 성능을 보일 수 있습니다.\n특성 선택이 중요한 경우: 라쏘 회귀가 적합한 선택일 수 있습니다. L1 정규화는 계수를 0으로 만들어 변수 선택을 수행합니다. 이로 인해 라쏘 회귀는 상대적으로 더 간단한 모델을 만들 수 있습니다."
  },
  {
    "objectID": "posts/t-sne/index.html",
    "href": "posts/t-sne/index.html",
    "title": "t-sne",
    "section": "",
    "text": "t-SNE란…\nt-SNE(t-Distributed Stochastic Neighbor Embedding)는 고차원 데이터를 저차원 공간(주로 2차원 또는 3차원)으로 시각화하기 위해 사용되는 비선형 차원 축소 기법입니다. t-SNE는 원본 고차원 데이터에서의 데이터 포인트 간 유사도와 축소된 저차원 공간에서의 데이터 포인트 간 유사도를 비슷하게 유지하려고 합니다. 이 기법은 특히 데이터의 군집 구조를 보존하는 데 효과적이라고 알려져 있습니다.\nt-SNE는 다음과 같은 과정으로 진행됩니다:\n\n유사도 계산: 고차원 데이터에서 각 데이터 포인트 간의 유사도를 계산합니다. 이 때, 가우시안 커널을 사용하여 조건부 확률을 구합니다. 이 조건부 확률은 한 데이터 포인트가 다른 데이터 포인트와 얼마나 가까운지를 나타내는 값입니다.\n저차원 맵핑: 초기에 무작위로 설정된 저차원 공간에서의 데이터 포인트 간 유사도를 계산합니다. t-SNE는 이 단계에서 t-분포를 사용하여 유사도를 계산합니다. 이는 원본 고차원 데이터에서의 군집 구조를 보존하면서 저차원 공간으로의 맵핑을 더 쉽게 만들어줍니다.\n최적화: 고차원 데이터에서의 유사도와 저차원 공간에서의 유사도가 최대한 비슷해지도록, 저차원 공간의 데이터 포인트 위치를 조정합니다. 이 최적화 과정은 그래디언트 디센트(Gradient Descent)와 같은 방법을 사용하여 진행됩니다.\n\nt-SNE는 차원 축소 결과를 통해 고차원 데이터에서의 군집 구조와 패턴을 시각적으로 이해하기 쉽게 해줍니다. 하지만 계산 복잡도가 높아 큰 데이터셋에 적용하기 어려울 수 있고, 최적화 과정의 무작위성으로 인해 결과의 재현성이 낮을 수 있다는 단점이 있습니다.\nlibrary(reticulate)\nlibrary(tidyverse)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.manifold import TSNE\n\n# Load the MNIST dataset\nmnist = datasets.fetch_openml('mnist_784')\nX, y = mnist.data, mnist.target\n\n# Select a subset of the dataset to reduce computation time\nn_samples = 5000\nX_sample = X[:n_samples]\ny_sample = y[:n_samples]\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_sample)\n\ny_sample\ny_sample = np.asarray(y_sample)\nX_tsne &lt;- py$X_tsne\ny_sample &lt;- py$y_sample\n\ntsne_df &lt;- tibble(x1 = X_tsne[,1], x2 = X_tsne[,2], y = y_sample)\n\nhead(tsne_df)\n# A tibble: 6 × 3\n    x1     x2  y        \n1  17.1   9.26  5        \n2  59.9   24.4  0        \n3  14.0  -53.1  4        \n4 -62.3   12.5  1        \n5  -3.57 -26.4  9        \n6 -32.6   61.9  2\n# Draw Plot\ntsne_df |&gt;\n  ggplot(aes(x = x1, y = x2, color = y)) +\n  geom_jitter() +\n  ggtitle(\"Result of t-sne on MNIST\")+\n  ggthemes::theme_fivethirtyeight()"
  },
  {
    "objectID": "posts/umap/index.html",
    "href": "posts/umap/index.html",
    "title": "UMAP",
    "section": "",
    "text": "UMAP이란…\nUMAP (Uniform Manifold Approximation and Projection)은 고차원 데이터를 저차원 공간으로 축소하는 비선형 차원 축소 기법 중 하나입니다. UMAP은 t-SNE와 유사하게 데이터의 군집 구조를 보존하면서 고차원 데이터를 시각화하거나 차원 축소하는 데 사용됩니다. 그러나 UMAP은 기하학적 특성을 더 잘 보존하고, 계산 효율성이 더 높아 대규모 데이터셋에도 적용 가능하다는 장점이 있습니다.\nUMAP 알고리즘의 주요 과정은 다음과 같습니다:\n\n고차원 데이터의 지역 구조 파악: 각 데이터 포인트의 근처 이웃을 찾아 데이터의 지역적 구조를 파악합니다. 이 과정에서는 일반적으로 k-최근접 이웃(KNN) 알고리즘이 사용됩니다.\n지역 구조를 기반으로 한 고차원 데이터의 그래프 생성: 각 데이터 포인트와 그 이웃 간의 유사도를 기반으로 고차원 데이터의 그래프를 생성합니다. 이 때, 거리 측정에는 주로 유클리디안 거리나 코사인 유사도를 사용하며, 가중치는 멀리 떨어진 포인트에 대해 더 낮은 가중치를 부여하는 방식으로 할당됩니다.\n저차원 임베딩을 위한 그래프 생성: 저차원 공간에서도 원본 고차원 데이터의 지역 구조를 유지하려고 노력하며, 그래프 기반 최적화를 통해 고차원 그래프와 저차원 그래프 간의 거리를 최소화합니다.\n최적화: 그래디언트 디센트(Gradient Descent)와 같은 방법을 사용하여 저차원 임베딩 공간의 데이터 포인트 위치를 조정하면서, 고차원 그래프와 저차원 그래프 간의 거리를 최소화하는 위치를 찾습니다.\n\nUMAP은 t-SNE에 비해 기하학적 특성을 더 잘 보존하고, 계산 효율성이 높아 대규모 데이터셋에도 적용할 수 있는 장점이 있습니다. 또한, UMAP은 차원 축소 결과를 통해 고차원 데이터에서의 군집 구조와 패턴을 시각적으로 이해하기 쉽게 해줍니다. 이러한 이유로 UMAP은 많은 데이터 과학자들이 선호하는 차원 축소 기법 중 하나입니다.\nlibrary(reticulate)\nlibrary(tidyverse)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport umap\n\n# Load the MNIST dataset\nmnist = datasets.fetch_openml('mnist_784')\nX, y = mnist.data, mnist.target\n\n# Select a subset of the dataset to reduce computation time\nn_samples = 5000\nX_sample = X[:n_samples]\ny_sample = y[:n_samples]\n\n# Apply UMAP\numap_reducer = umap.UMAP(n_components=2, random_state=42)\nX_umap = umap_reducer.fit_transform(X_sample)\ny_sample = np.asarray(y_sample)\nX_umap &lt;- py$X_umap\ny_sample &lt;- py$y_sample\n\numap_df &lt;- tibble(x1 = X_umap[,1], x2 = X_umap[,2], y = y_sample)\n\nhead(umap_df)\n# A tibble: 6 × 3\n    x1    x2   y        \n1  4.05  7.81  5        \n2  0.744 3.75  0        \n3 10.9   0.124 4        \n4 11.9   7.83  1        \n5 11.6   2.26  9        \n6  8.29  5.54  2\n# Draw Plot\numap_df |&gt;\n  ggplot(aes(x = x1, y = x2, color = y)) +\n  geom_jitter() +\n  ggtitle(\"Result of UMAP on MNIST\")+\n  ggthemes::theme_fivethirtyeight()"
  },
  {
    "objectID": "posts/word-embedding/index.html",
    "href": "posts/word-embedding/index.html",
    "title": "Word Embedding과 Word2Vec",
    "section": "",
    "text": "Word Embedding은..\n단어 임베딩은 단어들을 고차원 벡터 공간에 매핑하는 과정입니다. 이렇게 변환된 벡터는 단어 간의 의미적 관계를 반영하도록 합니다. 비슷한 의미를 가진 단어들은 벡터 공간에서 가까운 위치에 놓이게 됩니다. 이를 통해 자연어 처리 모델은 단어 간의 유사성을 파악하고 문장이나 문서의 의미를 이해할 수 있습니다. 예를 들어, “왕 - 남자 + 여자 = 여왕”과 같은 관계를 벡터 연산을 통해 찾을 수 있습니다.\n\n상기 예시의 시각화\n기초적인 Word Embedding의 학습은 한 문장의 다음 단어(토큰)을 예측하는 Neural Network(이하 NN)을 학습하는 것입니다. 예를 들면 “사과/는” 이라는 토큰을 input으로 넣으면 “빨갛다”라는 토큰을 예측하는 방식입니다. 이렇게 학습된 NN에서 각 단어에 할당된 각 hidden layer들에 대한 가중들이 단어들을 벡터 공간에 매핑한 결과입니다. 위의 왕/남자/여왕/여자의 예시를 일반화할 경우, 각 단어들이 2개의 hidden layer에 연결되어 있고, 각 layer는 성별/직위를 뜻한다고 볼 수 있습니다.\n\n\nWord2Vec은..\nWord2Vec은 구글 연구자들이 개발한 단어 임베딩 기법입니다. Word2Vec은 큰 텍스트 데이터셋에서 단어들의 의미적 관계를 학습하는 데 사용되는 신경망 기반 모델입니다. Word2Vec에는 주로 두 가지 학습 방법이 사용됩니다:\n\nCBOW (Continuous Bag of Words): 이 방법은 주변 단어들을 사용하여 중심 단어를 예측하는 방식입니다. 즉, 주변 문맥을 기반으로 단어의 의미를 학습합니다. “사과는 빨갛다” 예시에서 “사과”와 “빨갛다”를 입력으로 사용하여 “는”을 출력으로 학습하는 방식입니다.\nSkip-gram: 이 방법은 중심 단어를 사용하여 주변 단어들을 예측하는 방식입니다. 이 방법은 주로 큰 데이터셋에 적합하며, 희소한 단어들에 대해서도 더 나은 임베딩을 생성합니다. 마찬가지의 예시를 활용하자면, 여기서는 “는”을 입력으로 사용하고, “사과”와 “빨갛다”를 출력으로 학습하는 방식입니다.\n\n\n\n예제: Seoul - Korea + Japan = ?\nlibrary(tidyverse)\nglove_6B_50d &lt;- read_table(\"C:/Users/tranq/Desktop/glove.6B.50d.txt\", \n                           col_names = FALSE)\n\nword_embeddings &lt;- glove_6B_50d[, 1]\nvector_embeddings &lt;- as.matrix(glove_6B_50d[, -1])\n\nget_word_vector &lt;- function(word) {\n  word_index &lt;- which(word_embeddings == word)\n  if (length(word_index) == 0) {\n    return(NULL)\n  }\n  return(vector_embeddings[word_index, ])\n}\n\nfind_closest_word &lt;- function(result_vector, n = 5) {\n\n  similarity_scores &lt;- vector_embeddings %*% result_vector / (sqrt(rowSums(vector_embeddings^2)) * sqrt(sum(result_vector^2)))\n  closest_indices &lt;- order(similarity_scores, decreasing = TRUE)[1:n]\n  return(word_embeddings[closest_indices,1])\n}\n\nseoul &lt;- get_word_vector(\"seoul\")\nkorea &lt;- get_word_vector(\"korea\")\njapan &lt;- get_word_vector(\"japan\")\n\nresult_vector &lt;- soeul - korea + japan\n\nclosest_word &lt;- find_closest_word(result_vector, n = 5)\nprint(closest_word)\n# A tibble: 5 × 1\n  X1      \n  &lt;chr&gt;   \n1 tokyo   \n2 osaka   \n3 japan   \n4 shanghai\n5 seoul"
  },
  {
    "objectID": "posts/zifs-law/index.html",
    "href": "posts/zifs-law/index.html",
    "title": "Zipf’s Law (단어 사용의 분포)",
    "section": "",
    "text": "Zipf’s Law는..\nZipf의 법칙(Zipf’s Law)은 언어학과 정보 이론에서 관찰되는 경험적인 법칙으로, 주어진 말뭉치(corpus)에서 단어의 사용 빈도와 순위 사이에 특정한 관계가 있다는 것을 기술합니다. 이 법칙은 미국의 언어학자 조지 캥스리 지프(George Kingsley Zipf)에 의해 1930년대에 발견되었습니다.\nZipf의 법칙에 따르면, 말뭉치에서 각 단어의 사용 빈도는 그 단어의 순위에 반비례합니다. 다시 말해, 가장 빈번하게 사용되는 단어의 빈도는 두 번째로 빈번하게 사용되는 단어의 빈도보다 대략 두 배 많으며, 세 번째로 빈번하게 사용되는 단어의 빈도보다 대략 세 배 많은 식입니다.\n브라운 대학교 현대 미국 영어 표준 말뭉치의 경우, 가장 사용 빈도가 높은 단어는 영어 정관사  “the”이며 전체 문서에서 7%의 빈도(약 백만 개 남짓의 전체 사용 단어 중 69,971회)를 차지한다. 두 번째로 사용 빈도가 높은 단어는 “of”로 약 3.5% 남짓(36,411회)한 빈도를 차지하며, 세 번째로 사용 빈도가 높은 단어는 “and”(28,852회)로, 지프의 법칙에 정확히 들어 맞는다. 약 135개 항목의 어휘만으로 브라운 대학 말뭉치의 절반을 나타낼 수 있다.\nZipf의 법칙은 여러 언어와 다양한 텍스트에서 일관되게 관찰되는 현상으로, 인간 언어와 정보 처리의 기본 원리를 이해하는 데 도움이 됩니다. 이 법칙의 정확한 원인은 아직 명확하게 밝혀지지 않았지만, 자연 언어가 최적화된 정보 전달 방식을 따르는 결과라는 설명이 제안되고 있습니다.\n\n\nZipf’s Law 예제 (제인 오스틴 소설)\n\n이성과 감성 / 오만과 편견 / 맨스필드 파크 / 엠마 / 노생거 사원 / 설득 모두 Zipf’s law를 만족함을 확인할 수 있다.\n# Load Library\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(janeaustenr)\n\nword_count &lt;- austen_books() |&gt;\n  unnest_tokens(word, text) |&gt;\n  count(book, word, sort = TRUE)\n\ntotal_word &lt;- word_count |&gt; group_by(book) |&gt; summarise(total = sum(n))\n\nword_count &lt;- word_count |&gt; left_join(total_word, by = \"book\")\n\nhead(word_count)\n\nggplot(word_count, aes(n/total, fill = book)) +\n  geom_histogram(show.legend = FALSE) +\n  xlim(NA, 0.0009) +\n  facet_wrap(~book, ncol = 2, scales = \"free_y\")\n# A tibble: 6 × 4\n  book           word      n  total\n  &lt;fct&gt;          &lt;chr&gt; &lt;int&gt;  &lt;int&gt;\n1 Mansfield Park the    6206 160460\n2 Mansfield Park to     5475 160460\n3 Mansfield Park and    5438 160460\n4 Emma           to     5239 160996\n5 Emma           the    5201 160996\n6 Emma           and    4896 160996"
  },
  {
    "objectID": "posts/linear-optimization/index.html",
    "href": "posts/linear-optimization/index.html",
    "title": "생산계획",
    "section": "",
    "text": "개요\nortools를 사용하여 선형 프로그래밍 문제를 해결하는 예제로 생산 계획 최적화 문제를 살펴보겠습니다.\n이 예제에서는 두 가지 제품을 생산하는 공장이 있으며, 각 제품에는 일정한 이익이 있습니다.\n공장은 제한된 자원을 사용하여 이익을 최대화하려고 합니다.\n\n\n문제 설정\n\n제품 A와 제품 B를 생산할 수 있으며, 각각 $20, $40의 이익을 얻을 수 있습니다.\n\n제품 A는 자원 1을 3 단위, 자원 2를 2 단위 사용합니다.\n\n제품 B는 자원 1을 2 단위, 자원 2를 5 단위 사용합니다.\n\n공장은 자원 1을 30 단위, 자원 2를 40 단위 사용할 수 있습니다.\n\n목표: 이익을 최대화하면서 제한된 자원을 사용하여 제품 A와 B의 최적 생산량을 찾으세요.\n\n\n문제 해결\nfrom ortools.linear_solver import pywraplp\n\n# 선형 프로그래밍 솔버 생성\nsolver = pywraplp.Solver.CreateSolver('GLOP')\n\n# 변수 생성 (정수형의 경우 IntVar)\nproduct_a = solver.NumVar(0, solver.infinity(), 'Product_A')\nproduct_b = solver.NumVar(0, solver.infinity(), 'Product_B')\n\n\n# 제약 조건 추가\nconstraint1 = solver.Constraint(0, 30, 'Resource_1')\nconstraint1.SetCoefficient(product_a, 3)\nconstraint1.SetCoefficient(product_b, 2)\n\nconstraint2 = solver.Constraint(0, 40, 'Resource_2')\nconstraint2.SetCoefficient(product_a, 2)\nconstraint2.SetCoefficient(product_b, 5)\n\n# 목적 함수 정의 및 최대화\nobjective = solver.Objective()\nobjective.SetCoefficient(product_a, 20)\nobjective.SetCoefficient(product_b, 40)\nobjective.SetMaximization()\n\n# 문제 해결\nstatus = solver.Solve()\n\n# 결과 출력\nif status == pywraplp.Solver.OPTIMAL:\n    print('Objective value =', objective.Value())\n    print('Product A =', product_a.solution_value())\n    print('Product B =', product_b.solution_value())\nelse:\n    print('The problem does not have an optimal solution.')\nObjective value = 345.4545454545455\nProduct A = 6.363636363636365\nProduct B = 5.454545454545454"
  },
  {
    "objectID": "posts/knapsack/index.html",
    "href": "posts/knapsack/index.html",
    "title": "배낭 문제",
    "section": "",
    "text": "개요\nortools를 사용하여 배낭 문제를 해결하는 예제를 살펴보겠습니다.\n일반적인 애플리케이션은 상자를 배송 트럭에 효율적으로 로드하는 것입니다.\n\n\n\n문제 설정\n850의 무게를 담을 수 있는 컨테이너와 50가지 항목의 아이템들이 있습니다.\n아이템은 값과 무게로 이루어져있습니다.\nvalues = [\n    360, 83, 59, 130, 431, 67, 230, 52, 93, 125, 670, 892, 600, 38, 48, 147,\n    78, 256, 63, 17, 120, 164, 432, 35, 92, 110, 22, 42, 50, 323, 514, 28,\n    87, 73, 78, 15, 26, 78, 210, 36, 85, 189, 274, 43, 33, 10, 19, 389, 276,\n    312\n]\nweights = [[\n    7, 0, 30, 22, 80, 94, 11, 81, 70, 64, 59, 18, 0, 36, 3, 8, 15, 42, 9, 0,\n    42, 47, 52, 32, 26, 48, 55, 6, 29, 84, 2, 4, 18, 56, 7, 29, 93, 44, 71,\n    3, 86, 66, 31, 65, 0, 79, 20, 65, 52, 13\n]]\ncapacities = [850]\n\n\n문제 해결\nfrom ortools.algorithms import pywrapknapsack_solver\n\nsolver = pywrapknapsack_solver.KnapsackSolver(\n    pywrapknapsack_solver.KnapsackSolver.\n    KNAPSACK_MULTIDIMENSION_BRANCH_AND_BOUND_SOLVER, 'KnapsackExample')\n\nsolver.Init(values, weights, capacities)\ncomputed_value = solver.Solve()\npacked_items = []\npacked_weights = []\ntotal_weight = 0\nprint('Total value =', computed_value)\nfor i in range(len(values)):\n    if solver.BestSolutionContains(i):\n        packed_items.append(i)\n        packed_weights.append(weights[0][i])\n        total_weight += weights[0][i]\n\nprint('Total weight:', total_weight)\nprint('Packed items:', packed_items)\nprint('Packed_weights:', packed_weights)\nTotal value = 7534\nTotal weight: 850\nPacked items: [0, 1, 3, 4, 6, 10, 11, 12, 14, 15, 16, 17, 18, 19, 21, 22, 24, 27, 28, 29, 30, 31,\n               32, 34, 38, 39, 41, 42, 44, 47, 48, 49]\nPacked_weights: [7, 0, 22, 80, 11, 59, 18, 0, 3, 8, 15, 42, 9, 0, 47, 52, 26, 6, 29, 84, 2, 4,\n                 18, 7, 71, 3, 66, 31, 0, 65, 52, 13]"
  }
]