---
title: "Entropy 쉽게 이해하기"
author: "Jinwook Chang"
date: "2023-04-27"
date-modified: "2023-04-27"
categories: [DataScience, Math, Statistics]
---

![](front.png)

### Entropy란..

엔트로피(entropy)는 정보 이론에서 확률 변수의 불확실성을 측정하는 지표입니다.
엔트로피는 어떤 정보를 표현하기 위해 필요한 평균 비트 수를 나타내며, 이를 통해 정보의 복잡성이나 압축 가능성을 파악할 수 있습니다.
높은 엔트로피는 높은 불확실성을 의미하며, 낮은 엔트로피는 낮은 불확실성을 의미합니다.

쉽게 설명하자면, 한 그룹에서의 놀라움의 기대값이라고 볼 수 있습니다.
여기서 놀라움이란, $log(\frac{1}{p(x)})$로 표현됩니다.
로또에 당첨이 될 확률이 매우 낮기에, 당첨되면 놀라움이 큰 것과 같은 맥락으로 이해할 수 있습니다.

90%의 확률로 앞면이 나오는 동전을 100번 던지는 것을 예로 들어보겠습니다:

|  | 앞 | 뒤 |
| --- | --- | --- |
| 확률 | 0.9 | 0.1 |
| 놀라움 | 0.15 | 3.32 |

100번을 던질 경우의 놀라움은 $(0.9 \times 100 \times 1.5) + (0.1 \times 100 \times 3.32)$ 입니다.
이 놀라움의 기대값 즉 Entropy는  $\frac{(0.9 \times 100 \times 1.5) + (0.1 \times 100 \times 3.32)}{100}$로,  0.47입니다.

다만, 여기서 던진 횟수는 약분이 되기에 위의 Entropy를 일반화하면 아래와 같이 표기됩니다:

$$
\sum log(\frac{1}{p(x)}) \cdot p(x)
$$