---
title: "LangChain Memory"
author: "Jinwook Chang"
date: "2023-06-05"
date-modified: "2023-06-05"
categories: [AI, NLP, LangChain, Script]
---

![](../../others/prompt.png)


[DeepLearning.AI](https://www.deeplearning.ai/)에서 제공하는 LangChian for LLM Application Development 강좌를 요약한 글입니다.

목차 :

* [Prompt Template](../langchain-prompt-template/)
* [Output Parser](../langchain-output-parser/)
* [Memory](../langchain-memory/)
* [Chain](../langchain-chains/)
* [Q&A](../langchain-qna)
* [Agent](../langchain-agent)

LangChain은 큰 언어 모델(Large Language Models, LLMs)을 사용하여 애플리케이션을 구축하는 개발자들을 돕기 위한 라이브러리입니다. \
이번 자료에서는 LangChain의 Memory기능을 활용해보도록 하겠습니다. \
이때까지의 방식이 하나의 질문에 하나의 대답을 얻는 방식이었다면, 이전의 대답에 대해 다시 질문할 수 있도록 하는 것이 **Memory**의 주요 기능입니다. \
ChatGPT가 구체적인 사례로 들 수 있습니다.

이제 LangChain의 메모리 기능을 활용해보겠습니다.

### Install and Load Libraries

```python
#!pip install python-dotenv
#!pip install openai
#!pip install --upgrade langchain
#!pip install tiktoken

import os
import openai
api_key = "YOUR_API_KEY"
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
```

### Using ConversationBufferMemory

```python
llm = ChatOpenAI(temperature=0.0,openai_api_key=api_key)
memory = ConversationBufferMemory()

# memory = ConversationBufferWindowMemory(k=1)
# only memorize k input

conversation = ConversationChain(
    llm=llm, 
    memory = memory,
    verbose=True
)
```

```python
conversation.predict(input="안녕하세요, 제 이름은 장진욱입니다.")
conversation.predict(input="1 + 1이 뭐니?")
conversation.predict(input="내 이름이 뭐니?")
```

```python
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: 안녕하세요, 제 이름은 장진욱입니다.
AI:

> Finished chain.


> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: 안녕하세요, 제 이름은 장진욱입니다.
AI: 안녕하세요, 장진욱님. 저는 인공지능입니다. 어떤 도움이 필요하신가요?
Human: 1 + 1이 뭐니?
AI:

> Finished chain.


> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: 안녕하세요, 제 이름은 장진욱입니다.
AI: 안녕하세요, 장진욱님. 저는 인공지능입니다. 어떤 도움이 필요하신가요?
Human: 1 + 1이 뭐니?
AI: 1 + 1은 2입니다. 이는 기본적인 수학 연산 중 하나입니다.
Human: 내 이름이 뭐니?
AI:

> Finished chain.
당신의 이름은 장진욱입니다. 이전에 말씀하셨죠.
```

`ConversationChain`에 모델, 메모리를 부여한 뒤 `input`을 넣으면 chain이라는 말과 같이 순차적으로 질문에 대해 답하게 됩니다.
위의 사례에서 볼 수 있듯이, 첫 질문에 이름을 알려주고 세번째 질문에 나의 이름에 대해 질문하면 첫 질문을 기억하여 답해줍니다.

인위적으로 Memory에 데이터 삽입도 가능합니다.

```python
memory = ConversationBufferMemory()
memory.save_context({"input": "안녕?"}, 
                    {"output": "응 그래 무슨 일이야?"})
memory.save_context({"input": "아니 뭐, 그냥 심심해서"}, 
                    {"output": "ㅋㅋ"})
memory.load_memory_variables({})
```

```python
{'history': 'Human: 안녕?\nAI: 응 그래 무슨 일이야?\nHuman: 아니 뭐, 그냥 심심해서\nAI: ㅋㅋ'}
```

```python
conversation = ConversationChain(
    llm=llm, 
    memory = memory,
    verbose=False
)
conversation.predict(input="그래서 이번 주말에 뭐해?")
```

```python
저는 주말에는 일을 하지 않습니다. 하지만, 일반적으로 주말에는 쉬는 시간을 가지고, 취미를 즐기거나 친구들과 만나는 등의 활동을 합니다. 당신은 무엇을 계획하고 있나요?
```



### 토큰 제한 문제

LLM 모델에서 주로 사용 transformer 구조의 한계 상, 이해할 수 있는 문맥의 길이 \(context length\)가 제한되어 있습니다.
\(GPT 3.5의 경우 4096 token\) \
이를 해결하기 위해 `ConversationTokenBufferMemory`와 `ConversationSummaryBufferMemory`를 활용할 수 있습니다.

`ConversationTokenBufferMemory`는 최대 토큰에 제한을 두어, 최대 토큰 개수 만큼의 최근 대화 목록만큼 기억합니다.

```python
from langchain.memory import ConversationTokenBufferMemory

memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)
memory.save_context({"input": "AI is what?!"},
                    {"output": "Amazing!"})
memory.save_context({"input": "Backpropagation is what?"},
                    {"output": "Beautiful!"})
memory.save_context({"input": "Chatbots are what?"}, 
                    {"output": "Charming!"})
memory.load_memory_variables({})
```

```python
{'history': 'AI: Beautiful!\nHuman: Chatbots are what?\nAI: Charming!'}
```

`ConversationSummaryBufferMemory`의 경우 대화 내용이 최대 토큰을 넘길 경우, 이전 대화 내용을 요약하여 토큰을 절약합니다.

```python
from langchain.memory import ConversationSummaryBufferMemory

# create a long string
schedule = """오전 8시에 제품 팀과 회의가 있기에 파워포인트 프레젠테이션을 준비해야 해. \
오전 9시부터 오후 12시까지 LangChain을 이용한 작업이 있을꺼야. \
정오에는 한 시간 고객 함께 이탈리안 레스토랑에서 점심 식사 미팅이 있고 \
점식 먹은 이후에 LLM 데모를 보여야 하니 노트북 좀 챙겨와 줘.
"""


memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)
memory.save_context({"input": "ㅎㅇ"}, {"output": "요즘 어때?"})
memory.save_context({"input": "뭐 별거 없지"},
                    {"output": "ㅇㅇ"})
memory.save_context({"input": "오늘 스케줄이 어떻게 돼?"}, 
                    {"output": f"{schedule}"})

memory.load_memory_variables({})                 
```

```python
{'history': "System: The human and AI engage in a conversation about the human's schedule. The AI informs the human of a morning meeting with the product team and a LangChain project from 9am to 12pm. There is also a lunch meeting with a customer at an Italian restaurant and a LLM demo to present after lunch. The AI requests the human to bring a laptop for the demo."}
```

위 메모리를 토대로 input을 넣어보겠습니다.

```python
conversation = ConversationChain(
    llm=llm, 
    memory = memory,
    verbose=True
)

conversation.predict(input="데모에서는 뭘 보여주는게 좋을까?, 한국어로 답해 줘")
```

```python


> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
System: The human and AI discuss the human's schedule, which includes a morning meeting with the product team and a LangChain project from 9am to 12pm, a lunch meeting with a customer at an Italian restaurant, and a LLM demo to present after lunch. The AI requests the human to bring a laptop for the demo. The human asks what would be good to show in the demo.
AI: Well, for the LLM demo, it would be best to showcase the key features and benefits of the product. This could include the user interface, the ease of use, and the various functionalities that the LLM offers. Additionally, it would be helpful to highlight any recent updates or improvements that have been made to the product. Is there anything specific you would like me to focus on?
Human: 데모에서는 뭘 보여주는게 좋을까?, 한국어로 답해 줘
AI:

> Finished chain.
LLM 제품의 주요 기능과 이점을 보여주는 것이 가장 좋을 것입니다. 이는 사용자 인터페이스, 사용의 용이성 및 LLM이 제공하는 다양한 기능을 포함할 수 있습니다. 또한 제품에 최근에 이루어진 업데이트 또는 개선 사항을 강조하는 것이 도움이 될 것입니다. 특별히 강조하고 싶은 내용이 있으신가요?
```


