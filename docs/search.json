[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Experienced Supply Chain Manager\nData Scientist\nSamsung Electronics (Jul 2019 ~ )\nMS in Data Science\nBS in Industrial Engineering\nBA in Sports Industry Management"
  },
  {
    "objectID": "about.html#jinwook-chang-å¼µé­æ—­",
    "href": "about.html#jinwook-chang-å¼µé­æ—­",
    "title": "About",
    "section": "",
    "text": "Experienced Supply Chain Manager\nData Scientist\nSamsung Electronics (Jul 2019 ~ )\nMS in Data Science\nBS in Industrial Engineering\nBA in Sports Industry Management"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Note of myself",
    "section": "",
    "text": "Kullback-Leibler Divergence ì‰½ê²Œ ì´í•´í•˜ê¸°\n\n\n\nDataScience\n\n\nMath\n\n\nStatistics\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2023 MIT 6.S191 03 MNIST\n\n\n\nDataScience\n\n\nDeepLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2023 MIT 6.S191 02 Music Generation with RNNs\n\n\n\nDataScience\n\n\nDeepLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2023 MIT 6.S191 01 Intro to TensorFlow\n\n\n\nDataScience\n\n\nDeepLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2023 MIT 6.S191 Deep Learning\n\n\n\nDataScience\n\n\nDeepLearning\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nì¼ˆë¦¬ ê¸°ì¤€\n\n\n\nDataScience\n\n\nScript\n\n\nOptimization\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\në”¥ëŸ¬ë‹ì„ ìœ„í•œ Multi variables Linear Regression\n\n\n\nDataScience\n\n\nRegression\n\n\nDeepLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\në”¥ëŸ¬ë‹ì„ ìœ„í•œ Simple Linear Regression\n\n\n\nDataScience\n\n\nRegression\n\n\nDeepLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\në°°ë‚­ ë¬¸ì œ (ì—¬ëŸ¬ ë°°ë‚­)\n\n\n\nDataScience\n\n\nOptimization\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\në°°ë‚­ ë¬¸ì œ\n\n\n\nDataScience\n\n\nOptimization\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nìƒì‚°ê³„íš\n\n\n\nDataScience\n\n\nOptimization\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1ì¢… ì˜¤ë¥˜ 2ì¢… ì˜¤ë¥˜ í—·ê°ˆë¦¬ì§€ ì•Šê¸°\n\n\n\nDataScience\n\n\nStatistics\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGPT í…œí”Œë¦¿\n\n\n\nAI\n\n\nGPT\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEntropy ì‰½ê²Œ ì´í•´í•˜ê¸°\n\n\n\nDataScience\n\n\nMath\n\n\nStatistics\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nìƒí˜¸ ì •ë³´ëŸ‰ (Mutual Informaion)\n\n\n\nDataScience\n\n\nMetric\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddingê³¼ Word2Vec\n\n\n\nDataScience\n\n\nNLP\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nì´ì›ìˆ˜ì™€ ì „ì§„ìë™ë¯¸ë¶„\n\n\n\nDataScience\n\n\nOptimization\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nì¸ê³¼ê´€ê³„ì˜ ì¡°ê±´ê³¼ íƒìƒ‰\n\n\n\nDataScience\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDBSCAN\n\n\n\nDataScience\n\n\nClustering\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Boosting\n\n\n\nDataScience\n\n\nEnsembleLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest\n\n\n\nDataScience\n\n\nEnsembleLearning\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEnsemble Learning\n\n\n\nDataScience\n\n\nMainTheme\n\n\nEnsembleLearning\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nK-means clustering\n\n\n\nDataScience\n\n\nClustering\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nt-sne\n\n\n\nDataScience\n\n\nDimensionReduction\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUMAP\n\n\n\nDataScience\n\n\nDimensionReduction\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\nMainTheme\n\n\nDataScience\n\n\nClustering\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDimension Reduction (ì°¨ì› ì¶•ì†Œ)\n\n\n\nMainTheme\n\n\nDataScience\n\n\nDimensionReduction\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nN-1 ë”ë¯¸ ë³€ìˆ˜ë¥¼ í™œìš©í•˜ëŠ” ì´ìœ \n\n\n\nDataScience\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRegularization (ì •ê·œí™” ê¸°ë²•)\n\n\n\nDataScience\n\n\nOptimization\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCovariance Matrix\n\n\n\nDataScience\n\n\nMath\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHierarchical Clustering\n\n\n\nDataScience\n\n\nClustering\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis\n\n\n\nDataScience\n\n\nDimensionReduction\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAlpha max plus Beta min (Fast 2D Eculedian Distance)\n\n\n\nDataScience\n\n\nOptimization\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBenfordâ€™s Law\n\n\n\nDataScience\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nZipfâ€™s Law (ë‹¨ì–´ ì‚¬ìš©ì˜ ë¶„í¬)\n\n\n\nDataScience\n\n\nNLP\n\n\nScript\n\n\n\n\n\n\n\nJinwook Chang\n\n\nApr 9, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/alpha-max-plus-beta-min/index.html",
    "href": "posts/alpha-max-plus-beta-min/index.html",
    "title": "Alpha max plus Beta min (Fast 2D Eculedian Distance)",
    "section": "",
    "text": "Alpha max plus Beta min ì•Œê³ ë¦¬ì¦˜ì€ ë‘ ì œê³±ì˜ í•©ì˜ ì œê³±ê·¼ì„ ê³ ì†ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ê³„ì‚°ì€ ì§ê° ì‚¼ê°í˜•ì˜ ë¹—ë³€ ê¸¸ì´ë¥¼ êµ¬í•˜ê±°ë‚˜, 2ì°¨ì› ë²¡í„°ì˜ ë†ˆ, ë˜ëŠ” ì‹¤ìˆ˜ì™€ í—ˆìˆ˜ë¶€ë¥¼ ê°€ì§„ ë³µì†Œìˆ˜ z = a + biì˜ í¬ê¸°ë¥¼ êµ¬í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ìœ ìš©í•œ ê¸°ëŠ¥ì…ë‹ˆë‹¤.\nì´ ì•Œê³ ë¦¬ì¦˜ì€ ê³„ì‚° ë¹„ìš©ì´ ë†’ì€ ì œê³± ë° ì œê³±ê·¼ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ëŒ€ì‹  ë¹„êµ, ê³±ì…ˆ ë° ë§ì…ˆê³¼ ê°™ì€ ê°„ë‹¨í•œ ì—°ì‚°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Î±ì™€ Î² ë§¤ê°œë³€ìˆ˜ë¥¼ ì ì ˆí•˜ê²Œ ì„ íƒí•¨ìœ¼ë¡œì¨ ê³±ì…ˆ ì—°ì‚°ì„ ì´ì§„ ìë¦¿ìˆ˜ì˜ ë‹¨ìˆœí•œ ì‹œí”„íŠ¸ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ê³ ì† ë””ì§€í„¸ íšŒë¡œ êµ¬í˜„ì— íŠ¹íˆ ì í•©í•©ë‹ˆë‹¤.\nì•ŒíŒŒ ë§¥ìŠ¤ í”ŒëŸ¬ìŠ¤ ë² íƒ€ ë¯¼ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\\[\n\\sqrt{A^2 + B^2} â‰ˆ Î± \\cdot max(A, B) + Î² \\cdot min(A, B)\n\\]\nì—¬ê¸°ì„œ Aì™€ BëŠ” ì£¼ì–´ì§„ ë‘ ìˆ˜ì´ê³ , max(A, B)ëŠ” Aì™€ B ì¤‘ í° ê°’ì„ ë‚˜íƒ€ë‚´ë©°, min(A, B)ëŠ” ì‘ì€ ê°’ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Î±ì™€ Î²ëŠ” ê·¼ì‚¬ì¹˜ë¥¼ ìµœì í™”í•˜ëŠ” ì„ íƒëœ ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤.\nì´ ì•Œê³ ë¦¬ì¦˜ì€ ë‘ ì œê³±ì˜ í•©ì˜ ì œê³±ê·¼ì„ ì§ì ‘ ê³„ì‚°í•˜ëŠ” ê²ƒì— ë¹„í•´ ë¹ ë¥´ê³  ê³„ì‚°ì ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ëŒ€ì•ˆì„ ì œê³µí•˜ë©´ì„œë„ í•©ë¦¬ì ì¸ ì •í™•ë„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n\n\n\nUntitled"
  },
  {
    "objectID": "posts/alpha-max-plus-beta-min/index.html#benchmark-of-alpha-max-plus-beta-min",
    "href": "posts/alpha-max-plus-beta-min/index.html#benchmark-of-alpha-max-plus-beta-min",
    "title": "Alpha max plus Beta min (Fast 2D Eculedian Distance)",
    "section": "Benchmark of Alpha max plus Beta min",
    "text": "Benchmark of Alpha max plus Beta min\n# ê¸°ë³¸ ì œê³±ê·¼ ê³„ì‚° í•¨ìˆ˜\nsqrt_sum_of_squares &lt;- function(a, b) {\n  return(sqrt(a^2 + b^2))\n}\n\n# ì•ŒíŒŒ ë§¥ìŠ¤ í”ŒëŸ¬ìŠ¤ ë² íƒ€ ë¯¼ ê·¼ì‚¬ í•¨ìˆ˜\nalpha_max_plus_beta_min &lt;- function(a, b, alpha, beta) {\n  return(alpha * max(a, b) + beta * min(a, b))\n}\n\n# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±\nnum_tests &lt;- 10000\ntest_cases &lt;- data.frame(a = runif(num_tests), b = runif(num_tests))\n\n# íŒŒë¼ë¯¸í„° ì„¤ì •\nalpha &lt;- 0.960\nbeta &lt;- 0.397\n\n# ê¸°ë³¸ ì œê³±ê·¼ ê³„ì‚° ì‹œê°„ ì¸¡ì •\nstart_time &lt;- Sys.time()\nfor (i in 1:num_tests) {\n  sqrt_sum_of_squares(test_cases$a[i], test_cases$b[i])\n}\nend_time &lt;- Sys.time()\nsqrt_time &lt;- end_time - start_time\n\n# ì•ŒíŒŒ ë§¥ìŠ¤ í”ŒëŸ¬ìŠ¤ ë² íƒ€ ë¯¼ ê³„ì‚° ì‹œê°„ ì¸¡ì •\nstart_time &lt;- Sys.time()\nfor (i in 1:num_tests) {\n  alpha_max_plus_beta_min(test_cases$a[i], test_cases$b[i], alpha, beta)\n}\nend_time &lt;- Sys.time()\napprox_time &lt;- end_time - start_time\n\n# ê²°ê³¼ ì¶œë ¥\ncat(\"ê¸°ë³¸ ì œê³±ê·¼ ê³„ì‚° ì‹œê°„: \", sqrt_time, \"\\n\")\ncat(\"ì•ŒíŒŒ ë§¥ìŠ¤ í”ŒëŸ¬ìŠ¤ ë² íƒ€ ë¯¼ ê·¼ì‚¬ ì‹œê°„: \", approx_time, \"\\n\")\n\n## ê¸°ë³¸ ì œê³±ê·¼ ê³„ì‚° ì‹œê°„:  0.09882092\n## ì•ŒíŒŒ ë§¥ìŠ¤ í”ŒëŸ¬ìŠ¤ ë² íƒ€ ë¯¼ ê·¼ì‚¬ ì‹œê°„:  0.03761482"
  },
  {
    "objectID": "posts/benfords-law/index.html",
    "href": "posts/benfords-law/index.html",
    "title": "Benfordâ€™s Law",
    "section": "",
    "text": "ë²¤í¬ë“œì˜ ë²•ì¹™(Benfordâ€™s Law), ë˜ëŠ” ì²« ë²ˆì§¸ ìˆ«ì ë²•ì¹™(First-Digit Law)ì€ ë‹¤ì–‘í•œ ì‹¤ìƒí™œ ë°ì´í„° ì§‘í•©ì—ì„œ íŠ¹ì • ìˆ«ìê°€ ì²˜ìŒ ìë¦¬ì— ë‚˜íƒ€ë‚  í™•ë¥ ì— ê´€í•œ ê²½í—˜ì  ë²•ì¹™ì…ë‹ˆë‹¤. ì´ ë²•ì¹™ì€ 1881ë…„ì— ê´€ì°°ë˜ì—ˆìœ¼ë‚˜, ë¬¼ë¦¬í•™ì í”„ë­í¬ ë²¤í¬ë“œ(Frank Benford)ê°€ 1938ë…„ì— ì´ ë²•ì¹™ì„ ë” ë„ë¦¬ ì•Œë ¸ê¸° ë•Œë¬¸ì— ê·¸ì˜ ì´ë¦„ì´ ë¶™ì—ˆìŠµë‹ˆë‹¤.\në²¤í¬ë“œì˜ ë²•ì¹™ì— ë”°ë¥´ë©´, ì²« ë²ˆì§¸ ìˆ«ìê°€ nì¸ ë°ì´í„°ì˜ ë¹„ìœ¨ì€ ë‹¤ìŒ ê³µì‹ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\nP(n) = log10(n + 1) - log10(n) = log10(1 + 1/n)\nì´ ê³µì‹ì— ë”°ë¥´ë©´, ì²« ë²ˆì§¸ ìˆ«ìê°€ 1ì¼ í™•ë¥ ì€ ì•½ 30.1%, 2ì¼ í™•ë¥ ì€ ì•½ 17.6%, 9ì¼ í™•ë¥ ì€ ì•½ 4.6%ì…ë‹ˆë‹¤.\në²¤í¬ë“œì˜ ë²•ì¹™ì€ ì¸êµ¬, ê²½ì œ, ê³¼í•™, ì§€ë¦¬ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ë°ì´í„°ì— ì ìš©ë©ë‹ˆë‹¤. ë²¤í¬ë“œì˜ ë²•ì¹™ì€ ë°ì´í„°ì˜ ìˆ«ì ë¶„í¬ë¥¼ ë¶„ì„í•˜ëŠ” ë° ë„ì›€ì´ ë˜ë©°, ì´ìƒì¹˜ íƒì§€, ë¶€ì • í–‰ìœ„ ê°ì§€, ë°ì´í„° ì˜¤ë¥˜ í™•ì¸ ë“±ì— í™œìš©ë©ë‹ˆë‹¤.\nì˜ˆë¥¼ ë“¤ì–´ íšŒê³„ ë¶„ì•¼ì—ì„œ ë²¤í¬ë“œì˜ ë²•ì¹™ì„ ì´ìš©í•´ ë¶€ì • íšŒê³„ë¥¼ ì°¾ì•„ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë²¤í¬ë“œì˜ ë²•ì¹™ì— ë”°ë¥´ë©´, ê¸ˆìœµ ë°ì´í„°ì˜ ì²« ìë¦¬ ìˆ«ì ë¶„í¬ëŠ” íŠ¹ì •í•œ íŒ¨í„´ì„ ë”°ë¥´ëŠ”ë°, ì´ íŒ¨í„´ê³¼ í¬ê²Œ ë²—ì–´ë‚œ ë°ì´í„°ê°€ ë°œê²¬ë˜ë©´ ë¶€ì • í–‰ìœ„ì˜ ê°€ëŠ¥ì„±ì„ ì˜ì‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n[1 â‰¤ k â‰¤ 9 ì—ì„œì˜ kì˜ ë¶„í¬ P_k %ì˜ ê·¸ë˜í”„]"
  },
  {
    "objectID": "posts/benfords-law/index.html#benfords-law-ì˜ˆì œ",
    "href": "posts/benfords-law/index.html#benfords-law-ì˜ˆì œ",
    "title": "Benfordâ€™s Law",
    "section": "Benfordâ€™s Law ì˜ˆì œ",
    "text": "Benfordâ€™s Law ì˜ˆì œ\n# Load required packages\nlibrary(gapminder)\nlibrary(tidyverse)\n\n# Load data from the gapminder package\ndata &lt;- gapminder\n\n# Filter the dataset to get the most recent population data for each country\nlatest_data &lt;- data |&gt; filter(year == max(year))\n\n# Extract the first digits from the population data\nfirst_digits &lt;- as.character(latest_data$pop) |&gt; str_sub(1,1) |&gt; as.integer()\n\n# Calculate the frequencies of the first digits\nobserved_freq &lt;- as.vector(table(first_digits) / length(first_digits))\n\n# Compute the expected frequencies according to Benford's Law\ndigits &lt;- 1:9\nbenford_freq &lt;- log10(1 + 1/digits)\n\n# Compare the observed and expected frequencies\ncomparison &lt;- tibble(\n  Digit = digits,\n  Observed_Frequency = observed_freq,\n  Benford_Frequency = benford_freq\n)\n\n# Reshape the data into a tidy format\ncomparison_tidy &lt;- comparison |&gt;\n  pivot_longer(cols = c(Observed_Frequency, Benford_Frequency), names_to = \"Type\", values_to = \"Frequency\")\n\n# Create a bar plot with ggplot2\nggplot(comparison_tidy, aes(x = factor(Digit), y = Frequency, fill = Type)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Comparing Observed and Benford's Law Frequencies\",\n       x = \"First Digit\",\n       y = \"Frequency\",\n       fill = \"Frequency Type\") +\n  theme_minimal()\n\n# Perform a chi-squared goodness-of-fit test\nobserved_counts &lt;- table(first_digits)\nchisq_test &lt;- chisq.test(observed_counts, p = benford_freq)\n\n# Print the results of the chi-squared test\ncat(\"Chi-squared test statistic:\", chisq_test$statistic, \"\\n\")\ncat(\"Degrees of freedom:\", chisq_test$parameter, \"\\n\")\ncat(\"p-value:\", chisq_test$p.value, \"\\n\")\n\n# Check if the observed data follows Benford's Law (using a significance level of 0.05)\nif (chisq_test$p.value &gt; 0.05) {\n  cat(\"The data follows Benford's Law.\\n\")\n} else {\n  cat(\"The data does not follow Benford's Law.\\n\")\n}\n\n\n\nfigure_1.png\n\n\nChi-squared test for given probabilities\n\ndata:  observed_counts\nX-squared = 4.9697, df = 8, p-value = 0.7608\n\n# The data follows Benford's Law."
  },
  {
    "objectID": "posts/causal-relationship/index.html",
    "href": "posts/causal-relationship/index.html",
    "title": "ì¸ê³¼ê´€ê³„ì˜ ì¡°ê±´ê³¼ íƒìƒ‰",
    "section": "",
    "text": "ì˜êµ­ì˜ ì² í•™ì ë°ì´ë¹„ë“œ í„ì´ ê·¸ì˜ ì €ì„œ â€œì¸ê°„ ì§€ì„±ì˜ ì¡°ì‚¬â€ì—ì„œ ì¸ê°„ ì§€ì‹ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ì¸ì‹í•˜ëŠ”ë° ìˆì–´ í•„ìš”í•œ ì„¸ ê°€ì§€ ì¡°ê±´ì„ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. ì¡°ê±´ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤ :\n\nì¸ì ‘ì„±(contiguity) : ì¸ê³¼ ê´€ê³„ì— ìˆëŠ” ë‘ ì‚¬ê±´ì´ ê³µê°„ì ìœ¼ë¡œë‚˜ ì‹œê°„ì ìœ¼ë¡œ ê°€ê¹Œì›Œì•¼ í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–´ë–¤ ì›ì¸ì´ ê²°ê³¼ë¥¼ ì´ˆë˜í•˜ë ¤ë©´, ê·¸ ì›ì¸ê³¼ ê²°ê³¼ëŠ” ê³µê°„ì ìœ¼ë¡œ ì¸ì ‘í•´ ìˆì–´ì•¼ í•˜ë©°, ì‹œê°„ì ìœ¼ë¡œë„ ê°€ê¹Œìš´ ì‹œì ì—ì„œ ë°œìƒí•´ì•¼ í•©ë‹ˆë‹¤. ì´ ê°œë…ì€ ì¸ê³¼ ê´€ê³„ë¥¼ ë¶„ì„í•  ë•Œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\nì‹œê°„ì  ì„ í–‰(temporal precedence) : ì¸ê³¼ ê´€ê³„ì— ìˆëŠ” ë‘ ì‚¬ê±´ì´ ì‹œê°„ ìˆœì„œì— ë”°ë¼ ë°œìƒí•´ì•¼ í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì›ì¸ì´ í•­ìƒ ê²°ê³¼ì— ì„ í–‰í•´ì•¼ í•©ë‹ˆë‹¤. ì¦‰, ì›ì¸ì´ ë¨¼ì € ë°œìƒí•˜ê³  ê·¸ ë‹¤ìŒì— ê²°ê³¼ê°€ ë°œìƒí•©ë‹ˆë‹¤.\nì§€ì†ì  ì—°ê´€(constant conjunction) : ì§€ì†ì  ì—°ê´€ì€ ì¸ê³¼ ê´€ê³„ì— ìˆëŠ” ë‘ ì‚¬ê±´ì´ ì¼ì •í•˜ê²Œ ë°œìƒí•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ê°œë…ì€ ë‘ ì‚¬ê±´ ì‚¬ì´ì— ì¸ê³¼ ê´€ê³„ê°€ ìˆë‹¤ê³  ì¶”ë¡ í•  ë•Œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–´ë–¤ ì›ì¸ì´ í•­ìƒ íŠ¹ì •í•œ ê²°ê³¼ì™€ í•¨ê»˜ ë°œìƒí•˜ë©´, ê·¸ ë‘ ì‚¬ê±´ ì‚¬ì´ì— ì¸ê³¼ ê´€ê³„ê°€ ìˆë‹¤ê³  ì¶”ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\në§ˆì°¬ê°€ì§€ë¡œ ì˜êµ­ì˜ ì² í•™ìì¸ ì¡´ ìŠ¤íŠœì–´íŠ¸ ë°€ì˜ ê²½ìš°, ì¸ê³¼ê´€ê³„ë¥¼ íƒìƒ‰í•˜ëŠ” ë°ì— ìˆì–´ 5ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” â€™Millâ€™s Methodâ€™ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤. ë‹¤ì„¯ ê°€ì§€ ë°©ë²•ì€ í•˜ê¸°ì™€ ê°™ìŠµë‹ˆë‹¤:\n\në™ì‹œ ë°œìƒì˜ ë°©ë²• (Method of Agreement): ì„œë¡œ ë‹¤ë¥¸ ìƒí™©ì—ì„œ ê²°ê³¼ê°€ ë°œìƒí•  ë•Œ ê³µí†µì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ìš”ì†Œë¥¼ ì¸ê³¼ ìš”ì¸ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.\nì°¨ì´ì˜ ë°©ë²• (Method of Difference): ê²°ê³¼ê°€ ë°œìƒí•˜ëŠ” ìƒí™©ê³¼ ë°œìƒí•˜ì§€ ì•ŠëŠ” ìƒí™©ì—ì„œì˜ ìœ ì¼í•œ ì°¨ì´ì ì„ ì¸ê³¼ ìš”ì¸ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.\nê³µë³€ì˜ ë°©ë²• (Method of Concomitant Variation): ì¸ê³¼ ìš”ì¸ê³¼ ê²°ê³¼ê°€ í•¨ê»˜ ë³€í•˜ëŠ” ì •ë„ë¥¼ ê´€ì°°í•˜ì—¬ ì¸ê³¼ ê´€ê³„ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\nì”ì—¬ì˜ ë°©ë²• (Method of Residues): ì´ë¯¸ ì•Œë ¤ì§„ ì¸ê³¼ ìš”ì¸ì„ ì œì™¸í•œ í›„, ë‚¨ì•„ìˆëŠ” ê²°ê³¼ì™€ ìš”ì¸ ì‚¬ì´ì˜ ì¸ê³¼ ê´€ê³„ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\nì—­ì„¤ì  í•©ë¦¬í™”ì˜ ë°©ë²• (Method of Contra-positive Reasoning): ì¸ê³¼ ìš”ì¸ì´ ì—†ì„ ë•Œ ê²°ê³¼ë„ ë°œìƒí•˜ì§€ ì•ŠëŠ” ê²ƒì„ ê´€ì°°í•˜ì—¬ ì¸ê³¼ ê´€ê³„ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "front_image.jpg\n\n\n\nClusteringì€..\ní´ëŸ¬ìŠ¤í„°ë§(clustering)ì€ ë°ì´í„° ë§ˆì´ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë¹„ì§€ë„ í•™ìŠµ(unsupervised learning) ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„°ë§ì˜ ëª©ì ì€ ì£¼ì–´ì§„ ë°ì´í„°ì…‹ì—ì„œ ìœ ì‚¬í•œ íŠ¹ì„±ì„ ê°€ì§„ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì„ ê·¸ë£¹í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë ‡ê²Œ ê·¸ë£¹í™”ëœ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì˜ ì§‘í•©ì„ í´ëŸ¬ìŠ¤í„°(cluster)ë¼ê³  í•©ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„°ë§ì€ ê³ ê° ì„¸ë¶„í™”, ì´ë¯¸ì§€ ë¶„ë¥˜, ë¬¸ì„œ êµ°ì§‘í™”, ì´ìƒì¹˜ íƒì§€, ì¶”ì²œ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ë°ì´í„°ì˜ êµ¬ì¡°ì™€ íŒ¨í„´ì„ ë°œê²¬í•˜ê³ , ìƒˆë¡œìš´ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ê±°ë‚˜ ì˜ì‚¬ ê²°ì •ì„ ë•ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ì£¼ìš” ê¸°ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\nì¤‘ì‹¬ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§(Centroid based clustering): ê° í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ì„ ì •ì˜í•˜ê³ , ê° ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì— í• ë‹¹í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•©ë‹ˆë‹¤. ì¤‘ì‹¬ê³¼ ë°ì´í„° í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ì˜ ì œê³±í•©ì„ ìµœì†Œí™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë°˜ë³µì ìœ¼ë¡œ ìµœì í™”í•©ë‹ˆë‹¤. K-means clustering\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): ë°ì´í„° í¬ì¸íŠ¸ì˜ ë°€ë„ì— ê¸°ë°˜í•œ í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•ìœ¼ë¡œ, ë°€ë„ê°€ ë†’ì€ ì˜ì—­ì„ í´ëŸ¬ìŠ¤í„°ë¡œ ì¸ì‹í•˜ê³ , ë°€ë„ê°€ ë‚®ì€ ì˜ì—­ì€ ë…¸ì´ì¦ˆë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ í´ëŸ¬ìŠ¤í„°ì˜ ê°œìˆ˜ë¥¼ ë¯¸ë¦¬ ì§€ì •í•  í•„ìš”ê°€ ì—†ìœ¼ë©°, ë…¸ì´ì¦ˆì— ëŒ€í•œ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. DBSCAN\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§(Hierarchical clustering): ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ë‚˜ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„° í¬ì¸íŠ¸ë‚˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ë³‘í•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë´ë“œë¡œê·¸ë¨(dendrogram)ì´ë¼ëŠ” ë‚˜ë¬´ í˜•íƒœì˜ êµ¬ì¡°ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ê³¼ì •ì„ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Hierarchical Clustering\nìŠ¤í™íŠ¸ëŸ¼ í´ëŸ¬ìŠ¤í„°ë§(Spectral clustering): ë°ì´í„°ì˜ ìœ ì‚¬ë„ í–‰ë ¬ì„ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ê³ , ê·¸ë˜í”„ì˜ ìŠ¤í™íŠ¸ëŸ¼(ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°)ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë¹„ì„ í˜• êµ¬ì¡°ë¥¼ ê°€ì§„ ë°ì´í„°ì— ì í•©í•˜ë©°, í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ì–‘ì´ ë³µì¡í•œ ê²½ìš°ì—ë„ ì˜ ì‘ë™í•©ë‹ˆë‹¤.\n\n\n\në°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ëŠ”..\n\nìœ í´ë¦¬ë””ì–¸ ê±°ë¦¬ (Euclidean Distance): ë‘ ì  ì‚¬ì´ì˜ ì§ì„  ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ê±°ë¦¬ ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤. ì´ ê±°ë¦¬ëŠ” ê¸°í•˜í•™ì  ê³µê°„ì—ì„œ ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ëŠ” L2 ë…¸ë¦„(norm)ìœ¼ë¡œë„ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.\në§¨í•˜íƒ„ ê±°ë¦¬ (Manhattan Distance): ê° ì¶•ì— ë”°ë¼ ìˆ˜ì§ìœ¼ë¡œ ì´ë™í•˜ì—¬ ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, L1 ë…¸ë¦„(norm)ìœ¼ë¡œë„ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ì´ ê±°ë¦¬ ë©”íŠ¸ë¦­ì€ ê·¸ë¦¬ë“œ ê¸°ë°˜ì˜ ë°ì´í„°ì—ì„œ ì¢…ì¢… ì‚¬ìš©ë©ë‹ˆë‹¤.\nì½”ì‚¬ì¸ ìœ ì‚¬ë„ (Cosine Similarity): ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ê°ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ê°’ì˜ ë²”ìœ„ëŠ” -1ì—ì„œ 1ê¹Œì§€ì´ë©°, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë²¡í„° ê°„ì˜ ë°©í–¥ì´ ìœ ì‚¬í•¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ ë©”íŠ¸ë¦­ì€ í…ìŠ¤íŠ¸ ë¬¸ì„œì™€ ê°™ì´ ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤.\nMahalanobis distance(ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬): ë‹¤ì°¨ì› ê³µê°„ì—ì„œ ë‘ ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ ê±°ë¦¬ ì¸¡ì • ë°©ë²•ì€ ê° ì°¨ì›ì˜ ìŠ¤ì¼€ì¼(scale)ê³¼ ìƒí˜¸ê°„ì˜ ê³µë¶„ì‚°(covariance)ì„ ê³ ë ¤í•˜ì—¬ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì¦‰, ê° ì°¨ì›ì˜ ì¤‘ìš”ì„±ì„ ê³ ë ¤í•˜ì—¬ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì— íŠ¹ì´í•œ ë°ì´í„°ë‚˜ ì´ìƒì¹˜(outlier)ì— ëœ ë¯¼ê°í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/covariance/index.html",
    "href": "posts/covariance/index.html",
    "title": "Covariance Matrix",
    "section": "",
    "text": "ê³µë¶„ì‚° í–‰ë ¬ì€â€¦\nê³µë¶„ì‚° í–‰ë ¬(covariance matrix)ì€ ë³€ìˆ˜ë“¤ ê°„ì˜ ê³µë¶„ì‚°ì„ ìš”ì†Œë¡œ ê°–ëŠ” ì •ë°©í–‰ë ¬(square matrix)ì…ë‹ˆë‹¤. ê³µë¶„ì‚°ì€ ë‘ ë³€ìˆ˜ê°€ í•¨ê»˜ ë³€í•˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ê°’ìœ¼ë¡œ, í•˜ë‚˜ì˜ ë³€ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nê³µë¶„ì‚° í–‰ë ¬ì—ì„œ ëŒ€ê°ì„  ìš”ì†ŒëŠ” ê° ë³€ìˆ˜ì˜ ë¶„ì‚°(variance)ì„ ë‚˜íƒ€ë‚´ë©°, ì´ëŠ” í•´ë‹¹ ë³€ìˆ˜ê°€ ì–¼ë§ˆë‚˜ í¼ì ¸ ìˆëŠ”ì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ë¹„ëŒ€ê°ì„  ìš”ì†ŒëŠ” ì„œë¡œ ë‹¤ë¥¸ ë‘ ë³€ìˆ˜ ê°„ì˜ ê³µë¶„ì‚°ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nnê°œì˜ ë³€ìˆ˜ë¥¼ ê°€ì§„ ë°ì´í„° ì„¸íŠ¸ì˜ ê³µë¶„ì‚° í–‰ë ¬ì€ n x n í¬ê¸°ì˜ ì •ë°©í–‰ë ¬ì´ë©°, í–‰ë ¬ì˜ (i, j) ìœ„ì¹˜ì— ìˆëŠ” ìš”ì†ŒëŠ” ë³€ìˆ˜ iì™€ ë³€ìˆ˜ jì˜ ê³µë¶„ì‚°ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nì˜ˆë¥¼ ë“¤ì–´, ë°ì´í„° ì„¸íŠ¸ì— ë³€ìˆ˜ Xì™€ ë³€ìˆ˜ Yê°€ ìˆë‹¤ê³  ê°€ì •í•˜ë©´ ê³µë¶„ì‚° í–‰ë ¬ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nCov(X, X)   Cov(X, Y)\nCov(Y, X)   Cov(Y, Y)\n\nì—¬ê¸°ì„œ Cov(X, X)ëŠ” ë³€ìˆ˜ Xì˜ ë¶„ì‚°, Cov(Y, Y)ëŠ” ë³€ìˆ˜ Yì˜ ë¶„ì‚°, Cov(X, Y)ì™€ Cov(Y, X)ëŠ” ë³€ìˆ˜ Xì™€ ë³€ìˆ˜ Yì˜ ê³µë¶„ì‚°ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nê³µë¶„ì‚° í–‰ë ¬ì€ ë°ì´í„°ì˜ ì„ í˜• ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ë³´ë¥¼ í¬í•¨í•˜ë©°, ì£¼ì„±ë¶„ ë¶„ì„(PCA)ê³¼ ê°™ì€ í†µê³„ ë° ê¸°ê³„ í•™ìŠµ ê¸°ë²•ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ê³µë¶„ì‚° í–‰ë ¬ì„ ì‚¬ìš©í•˜ë©´ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê³  ë³€ìˆ˜ ê°„ì˜ ìƒê´€ ê´€ê³„ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/dbscan/index.html",
    "href": "posts/dbscan/index.html",
    "title": "DBSCAN",
    "section": "",
    "text": "Result of DBSCAN\n\n\n\nDBSCANì€..\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)ì€ ë°€ë„ ê¸°ë°˜ì˜ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ë¡œ, 1996ë…„ì— Martin Ester, Hans-Peter Kriegel, JÃ¶rg Sander, Xiaowei Xuì— ì˜í•´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. DBSCANì€ ë°€ë„ ê¸°ë°˜ì˜ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ì–‘ì´ ì›í˜•ì´ ì•„ë‹Œ ê²½ìš°ì—ë„ ìœ ì—°í•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ë…¸ì´ì¦ˆë¥¼ í¬í•¨í•œ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ê°•ì¸í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\nDBSCAN ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ê°œë…ì„ í™œìš©í•©ë‹ˆë‹¤:\n\nì´ì›ƒ ë°˜ê²½(epsilon, Îµ): ë°ì´í„° í¬ì¸íŠ¸ì—ì„œ ì£¼ì–´ì§„ ê±°ë¦¬ ë‚´ì— ìˆëŠ” ì´ì›ƒ í¬ì¸íŠ¸ë¥¼ ì°¾ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ê±°ë¦¬ì…ë‹ˆë‹¤.\nìµœì†Œ í¬ì¸íŠ¸(MinPts): ë°€ì§‘ ì˜ì—­ì´ë¼ê³  ê°„ì£¼ë˜ê¸° ìœ„í•´ ì£¼ì–´ì§„ ì´ì›ƒ ë°˜ê²½ ë‚´ì— ì¡´ì¬í•´ì•¼ í•˜ëŠ” í¬ì¸íŠ¸ì˜ ìµœì†Œ ê°œìˆ˜ì…ë‹ˆë‹¤.\n\nì•Œê³ ë¦¬ì¦˜ì˜ ë™ì‘ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\në°ì´í„°ì…‹ ë‚´ì˜ ëª¨ë“  í¬ì¸íŠ¸ë¥¼ ìˆœíšŒí•˜ë©°, ì•„ì§ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹ë˜ì§€ ì•Šì€ í¬ì¸íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\nì„ íƒí•œ í¬ì¸íŠ¸ì—ì„œ epsilon ê±°ë¦¬ ì´ë‚´ì˜ ì´ì›ƒ í¬ì¸íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\nì´ì›ƒ í¬ì¸íŠ¸ì˜ ìˆ˜ê°€ MinPts ì´ìƒì´ë©´, ìƒˆë¡œìš´ í´ëŸ¬ìŠ¤í„°ë¥¼ ìƒì„±í•˜ê³  í•´ë‹¹ í¬ì¸íŠ¸ì™€ ê·¸ ì´ì›ƒë“¤ì„ í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹í•©ë‹ˆë‹¤.\nì´ì›ƒ í¬ì¸íŠ¸ì˜ ì´ì›ƒë“¤ì„ ë˜í•œ ìˆœíšŒí•˜ë©°, MinPts ì´ìƒì˜ ì´ì›ƒì„ ê°€ì§€ê³  ìˆëŠ” í¬ì¸íŠ¸ë¥¼ ë°œê²¬í•˜ë©´ í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì¶”ê°€í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ í´ëŸ¬ìŠ¤í„°ê°€ ë” ì´ìƒ í™•ì¥ë˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤.\nëª¨ë“  í¬ì¸íŠ¸ë¥¼ ìˆœíšŒí•  ë•Œê¹Œì§€ 1-4 ë‹¨ê³„ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤. ì´ ê³¼ì •ì´ ëë‚˜ë©´ í´ëŸ¬ìŠ¤í„°ê°€ ìƒì„±ë˜ë©°, ì–´ë– í•œ í´ëŸ¬ìŠ¤í„°ì—ë„ ì†í•˜ì§€ ì•ŠëŠ” í¬ì¸íŠ¸ëŠ” ë…¸ì´ì¦ˆë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.\n\nDBSCANì˜ ì£¼ìš” ì¥ì ì€ í´ëŸ¬ìŠ¤í„°ì˜ ê°œìˆ˜ë¥¼ ì‚¬ì „ì— ì§€ì •í•  í•„ìš”ê°€ ì—†ìœ¼ë©°, í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ì–‘ì— ëŒ€í•œ ê°€ì •ì´ ì—†ì–´ ë‹¤ì–‘í•œ í˜•íƒœì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë˜í•œ, ì´ ì•Œê³ ë¦¬ì¦˜ì€ ë…¸ì´ì¦ˆë¥¼ êµ¬ë¶„í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨ì ìœ¼ë¡œëŠ” ë†’ì€ ì°¨ì›ì˜ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ ì €í•˜ê°€ ìˆìœ¼ë©°, epsilonê³¼ MinPtsì™€ ê°™ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\n\n\nDBSCAN ì˜ˆì‹œ\nlibrary(tidyverse)\nlibrary(reticulate)\n\n# Get Smile Data\ncircle_df &lt;- data.frame(\n  t = seq(0, 2 * pi, length.out = 100),\n  x = 0,\n  y = 0,\n  r = 1\n)\ncircle_df$x &lt;- circle_df$r * cos(circle_df$t) + rnorm(100,0,0.03)\ncircle_df$y &lt;- circle_df$r * sin(circle_df$t) + rnorm(100,0,0.03)\n\neyes_df &lt;- data.frame(\n  x = rep(c(-0.3, 0.3),50) + rnorm(50,0,0.03),\n  y = rep(c(0.4, 0.4),50) + rnorm(50,0,0.03)\n)\n\nmouth_df &lt;- data.frame(\n  t = seq(-pi, 0, length.out = 100),\n  x = 0,\n  y = -0.3,\n  r = 0.5\n)\nmouth_df$x &lt;- mouth_df$r * cos(mouth_df$t) + rnorm(100,0,0.03)\nmouth_df$y &lt;- mouth_df$y + mouth_df$r * sin(mouth_df$t) + rnorm(100,0,0.03)\n\nsmile_df &lt;- rbind(circle_df[,2:3],eyes_df,mouth_df[,2:3])\n# Run DBSCAN\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\n\nX = r.smile_df\n\ndbscan = DBSCAN(eps=0.15, min_samples=10)\ndbscan.fit(X)\nlabels = dbscan.labels_\nlabels\n# Plot result\nsmile_df &lt;- smile_df |&gt; mutate(label = as.factor(py$labels))\nggplot(smile_df,aes(x = x, y = y, color = label)) + geom_point()"
  },
  {
    "objectID": "posts/dimension-reduction/index.html",
    "href": "posts/dimension-reduction/index.html",
    "title": "Dimension Reduction (ì°¨ì› ì¶•ì†Œ)",
    "section": "",
    "text": "ì°¨ì›ì˜ ì €ì£¼ë€..\nì°¨ì›ì˜ ì €ì£¼(curse of dimensionality)ëŠ” ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ë°œìƒí•˜ëŠ” ì—¬ëŸ¬ ë¬¸ì œë¥¼ ì´ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ëŠ” ìš©ì–´ì…ë‹ˆë‹¤. ë°ì´í„°ì˜ ì°¨ì›ì´ ì¦ê°€í• ìˆ˜ë¡, ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ê°€ ì ì  ë©€ì–´ì ¸ì„œ ë°ì´í„°ê°€ í¬ì†Œí•´ì§€ëŠ” í˜„ìƒì´ ë°œìƒí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í˜„ìƒì€ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë¥¼ ì•¼ê¸°í•©ë‹ˆë‹¤:\n\nê±°ë¦¬ ì¸¡ì •ì˜ ì–´ë ¤ì›€: ê³ ì°¨ì› ê³µê°„ì—ì„œëŠ” ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ê°€ í¬ê²Œ ì¦ê°€í•˜ë¯€ë¡œ, ê°€ê¹Œìš´ ì´ì›ƒì„ ì°¾ê±°ë‚˜ í´ëŸ¬ìŠ¤í„°ë§í•˜ëŠ” ê²ƒì´ ì–´ë µìŠµë‹ˆë‹¤.\nê³„ì‚° ë³µì¡ë„ ì¦ê°€: ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ëª¨ë¸ì„ í•™ìŠµí•˜ê±°ë‚˜ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ë° í•„ìš”í•œ ê³„ì‚°ëŸ‰ì´ í¬ê²Œ ì¦ê°€í•©ë‹ˆë‹¤.\nê³¼ì í•© ìœ„í—˜ ì¦ê°€: ë°ì´í„°ì˜ ì°¨ì›ì´ ì¦ê°€í• ìˆ˜ë¡, ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì í•©ë˜ì–´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê³¼ì í•©(overfitting)ì´ ë°œìƒí•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.\níŠ¹ì„± ì„ íƒì˜ ì–´ë ¤ì›€: ê³ ì°¨ì› ë°ì´í„°ì—ì„œëŠ” ì–´ë–¤ íŠ¹ì„±ì´ ì¤‘ìš”í•œì§€ íŒë‹¨í•˜ê±°ë‚˜ ì ì ˆí•œ íŠ¹ì„± ì¡°í•©ì„ ì°¾ëŠ” ê²ƒì´ ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤.\n\n\n\nì°¨ì› ì¶•ì†Œë€..\nì°¨ì› ì¶•ì†Œ(dimension reduction)ëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ, ë°ì´í„° ê³¼í•™ ë° ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ì°¨ì› ì¶•ì†Œì˜ í•„ìš”ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\në°ì´í„° ì‹œê°í™”: ê³ ì°¨ì› ë°ì´í„°ë¥¼ 2D ë˜ëŠ” 3Dë¡œ ì¤„ì—¬ì„œ ë°ì´í„°ì˜ íŒ¨í„´ì´ë‚˜ êµ¬ì¡°ë¥¼ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\nê³„ì‚° íš¨ìœ¨ì„±: ì €ì°¨ì› ë°ì´í„°ëŠ” ê³„ì‚°ëŸ‰ì´ ì¤„ì–´ë“¤ì–´ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµ ë° ì˜ˆì¸¡ ì†ë„ê°€ ë¹¨ë¼ì§‘ë‹ˆë‹¤.\nì¡ìŒ ì œê±°: ì°¨ì› ì¶•ì†ŒëŠ” ë°ì´í„°ì˜ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë¶ˆí•„ìš”í•œ ë³€ë™ì„±ì´ë‚˜ ì¡ìŒì„ ì œê±°í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.\nê³¼ì í•© ë°©ì§€: ì°¨ì›ì˜ ì €ì£¼(curse of dimensionality)ë¡œ ì¸í•´ ê³ ì°¨ì› ë°ì´í„°ì—ëŠ” ê³¼ì í•©(overfitting)ì´ ë°œìƒí•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ì°¨ì› ì¶•ì†Œë¥¼ í†µí•´ ê³¼ì í•©ì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\nì£¼ìš” ì°¨ì› ì¶•ì†Œ ê¸°ë²•\n\nPCA (ì£¼ì„±ë¶„ ë¶„ì„, Principal Component Analysis): ë°ì´í„°ì˜ ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ìƒˆë¡œìš´ ì¶•ì„ ì°¾ì•„ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì„ í˜•ì ì¸ ë°ì´í„° êµ¬ì¡°ì— ì í•©í•©ë‹ˆë‹¤. Principal Component Analysis\nNMF (ìŒìˆ˜ ë¯¸í¬í•¨ í–‰ë ¬ ì¸ìˆ˜ë¶„í•´, Non-negative Matrix Factorization): ì›ë˜ì˜ ë°ì´í„° í–‰ë ¬ì„ ìŒìˆ˜ê°€ ì•„ë‹Œ ë‘ ê°œì˜ í–‰ë ¬ì˜ ê³±ìœ¼ë¡œ ë¶„í•´í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë°ì´í„°ê°€ ìŒìˆ˜ê°€ ì•„ë‹Œ íŠ¹ì„±ì„ ê°€ì§ˆ ë•Œ ìœ ìš©í•˜ë©°, ë°ì´í„°ì˜ í¬ì†Œì„±ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nt-SNE (t-ë¶„í¬ í™•ë¥ ì  ì„ë² ë”©, t-Distributed Stochastic Neighbor Embedding): ê³ ì°¨ì› ë°ì´í„°ì˜ ì ë“¤ ê°„ ê±°ë¦¬ë¥¼ ì €ì°¨ì›ì—ì„œì˜ í™•ë¥  ë¶„í¬ë¡œ ë³´ì¡´í•˜ë ¤ê³  ì‹œë„í•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì…ë‹ˆë‹¤. ë°ì´í„°ì˜ êµ°ì§‘ êµ¬ì¡°ë‚˜ ë§¤ë‹ˆí´ë“œ êµ¬ì¡°ë¥¼ ì˜ ë³´ì¡´í•©ë‹ˆë‹¤. t-SNE\nUMAP (Uniform Manifold Approximation and Projection)ì€ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ì¶•ì†Œí•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. UMAPì€ t-SNEì™€ ìœ ì‚¬í•˜ê²Œ ë°ì´í„°ì˜ êµ°ì§‘ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ê±°ë‚˜ ì°¨ì› ì¶•ì†Œí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ UMAPì€ ê¸°í•˜í•™ì  íŠ¹ì„±ì„ ë” ì˜ ë³´ì¡´í•˜ê³ , ê³„ì‚° íš¨ìœ¨ì„±ì´ ë” ë†’ì•„ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ë„ ì ìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. UMAP"
  },
  {
    "objectID": "posts/dl-multi-variables-linear-regression/index.html",
    "href": "posts/dl-multi-variables-linear-regression/index.html",
    "title": "ë”¥ëŸ¬ë‹ì„ ìœ„í•œ Multi variables Linear Regression",
    "section": "",
    "text": "ì´ì „ì˜ Simple Linear Regressionì˜ˆì œì— ì´ì–´, ë‹¤ë³€ìˆ˜ë¥¼ í†µí•´ ê²°ê³¼ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì— ëŒ€í•´ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\ní€´ì¦ˆ1, í€´ì¦ˆ2, ì¤‘ê°„ê³ ì‚¬ì˜ ì„±ì ìœ¼ë¡œ ê¸°ë§ê³ ì‚¬ì˜ ì„±ì ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ì˜ˆì‹œë¡œ í•˜ê² ìŠµë‹ˆë‹¤.\n\n\n# A tibble: 5 Ã— 4\n  quiz1 quiz2 midterm final\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    73    80      75   152\n2    93    88      95   185\n3    89    91      90   180\n4    96    98     100   196\n5    73    66      70   142\n\n\nì´ì „ê³¼ ë‹¬ë¦¬ ë²¡í„° ê¸°ë°˜ìœ¼ë¡œ ì‹ì„ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\\[ H(x) = XW + b \\] ìœ„ì˜ ì‚¬ë¡€ë¡œ ì‹ì„ í’€ì–´ì“°ë©´, í•˜ê¸°ì™€ ê°™ìŠµë‹ˆë‹¤.\nx1, x2, x3 ëŠ” quiz1, quiz2, midtermì˜ ì ìˆ˜ (í–‰ ë²¡í„°) w1, w2, w3 ëŠ” quiz1, quiz2, midetermì— ëŒ€í•œ ê³„ìˆ˜ (ì—´ ë²¡í„°)\n\\[ H(x) = x_1w_1 + x_2w_2 + x_3w_3 + b \\]\nì´ ì„ ê³¼ ì‹¤ì œ ë°ì´í„°ì˜ ì˜¤ì°¨ëŠ” ì´ì „ê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[ cost(W,b) = \\frac{1}{m}\\sum_{i=1}^{m}(H(x^{(i)}) - y^{(i)})^2 \\]\në§ˆì°¬ê°€ì§€ë¡œ, cost(ì˜¤ì°¨)ë¥¼ ìµœì†Œí™” í•  ìˆ˜ ìˆëŠ” W,bì˜ ìŒì„ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜¤ì°¨ë¥¼ ìµœì†Œí™” í•˜ëŠ” ê°’ì„ ì°¾ê¸° ìœ„í•´, Gradient Descentë¥¼ í†µí•´ í™•ì¸í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\nì´ˆê¸° ê°’ W,bì— ëŒ€í•œ ê°ê°ì˜ í¸ë¯¸ë¶„ ê°’ì„ êµ¬í•©ë‹ˆë‹¤.\ní¸ë¯¸ë¶„ ê°’ì— learning rate \\(\\alpha\\)ë¥¼ ê³±í•´ì¤ë‹ˆë‹¤.\nW, bì— ê³±í•´ì¤€ ê°’ì„ ëº€ í›„ W, b ê°’ì„ ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤.\ncostê°€ ë§Œì¡±í• ë§Œí•œ ìˆ˜ì¤€ì´ ë  ë•Œ ê¹Œì§€ 1 ~ 3ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n\nì˜ˆì‹œ ë°ì´í„°ë¥¼ ì•„ë˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ í†µí•´ \\(XW + b\\)ë¥¼ êµ¬í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\n# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\nlibrary(tidyverse)\nlibrary(glue)\n\n# Gradient Descent í•¨ìˆ˜ ì •ì˜\ngradient_descent &lt;- function(X, y, alpha, iterations) {\n  n &lt;- length(y)\n  X &lt;- cbind(1, as.matrix(X)) # add intercept term to X matrix\n  m &lt;- ncol(X)\n  y &lt;- as.matrix(y)\n  W &lt;- matrix(rnorm(m), ncol = 1)\n  \n  for (i in 1:iterations) {\n    y_predicted &lt;- X %*% W\n    cost &lt;- y_predicted - y\n    W &lt;- W - alpha*(1/n)*t(X) %*% (y_predicted - y)\n  }\n  return(W)\n}\n\n\n# Gradient Descent ì‹¤í–‰\nresult &lt;- gradient_descent(example[,-4], example[4], alpha = 0.00001, iterations = 2000)\n\n# ê²°ê³¼ ì¶œë ¥\ncat(glue(\"ì ˆí¸ : {result[1,]}\nquiz1 : {result[2,]}\nquiz2 : {result[3,]}\nmidterm : {result[4,]}\"))\n\nì ˆí¸ : 0.616439443438877\nquiz1 : 1.79538749134914\nquiz2 : 0.628151238241493\nmidterm : -0.405326284524674\n\ncat(\"ì˜ˆì¸¡: \\n\")\n\nì˜ˆì¸¡: \n\nprint(cbind(1,as.matrix(example[,-4])) %*% result)\n\n        final\n[1,] 151.5324\n[2,] 184.3588\n[3,] 181.0883\n[4,] 193.9998\n[5,] 144.7649\n\ncat(\"ì‹¤ì œ: \\n\")\n\nì‹¤ì œ: \n\nprint(as.matrix(example[,4]))\n\n     final\n[1,]   152\n[2,]   185\n[3,]   180\n[4,]   196\n[5,]   142"
  },
  {
    "objectID": "posts/dl-simple-linear-regression/index.html",
    "href": "posts/dl-simple-linear-regression/index.html",
    "title": "ë”¥ëŸ¬ë‹ì„ ìœ„í•œ Simple Linear Regression",
    "section": "",
    "text": "í•˜ê¸°ì™€ ê°™ì´ ê³µë¶€í•œ ì‹œê°„ê³¼ ì„±ì ì˜ ë°ì´í„°ê°€ ìˆì„ ë•Œ, ì‹œê°„ì„ í†µí•´ ì„±ì ë¥¼ ì˜ˆì¸¡í•˜ê³ ì í•©ë‹ˆë‹¤. ë‘ ë³€ìˆ˜ì˜ ê´€ê³„ê°€ ì„ í˜•ì ì´ë¼ ê°€ì •í•  ë•Œ, í•˜ê¸°ì˜ íŒŒë€ìƒ‰ ì„ ê³¼ ê°™ì´ ì ìˆ˜ë¥¼ ê°€ì¥ ì˜ ì˜ˆì¸¡í•˜ëŠ” ì„ ì„ ì°¾ëŠ” ê²ƒì´ ì´ë²ˆ í¬ìŠ¤íŠ¸ì˜ ëª©ì ì…ë‹ˆë‹¤.\n\n\n# A tibble: 4 Ã— 2\n  hours score\n  &lt;dbl&gt; &lt;dbl&gt;\n1    10    90\n2     9    80\n3     3    50\n4     2    30\n\n\n\n\n\n\n\nìœ„ì˜ ì„ ì„ ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ í‘œê¸°í•˜ë„ë¡ í•´ë´…ì‹œë‹¤.\n\\[ H(x) = Wx + b \\] ì´ ì„ ê³¼ ì‹¤ì œ ë°ì´í„°ì˜ ì˜¤ì°¨ëŠ” í•˜ê¸°ì™€ ê°™ì´ í‘œê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[ cost(W,b) = \\frac{1}{m}\\sum_{i=1}^{m}(H(x^{(i)}) - y^{(i)})^2 \\]\nì—¬ê¸°ì„œ ìš°ë¦¬ì˜ ëª©ì ì€ cost(ì˜¤ì°¨)ë¥¼ ìµœì†Œí™” í•  ìˆ˜ ìˆëŠ” W,bì˜ ìŒì„ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜¤ì°¨ë¥¼ ìµœì†Œí™” í•˜ëŠ” ê°’ì„ ì°¾ê¸° ìœ„í•´, Gradient Descentë¥¼ í†µí•´ í™•ì¸í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\nì´ˆê¸° ê°’ W,bì— ëŒ€í•œ ê°ê°ì˜ í¸ë¯¸ë¶„ ê°’ì„ êµ¬í•©ë‹ˆë‹¤.\ní¸ë¯¸ë¶„ ê°’ì— learning rate \\(\\alpha\\)ë¥¼ ê³±í•´ì¤ë‹ˆë‹¤.\nW, bì— ê³±í•´ì¤€ ê°’ì„ ëº€ í›„ W, b ê°’ì„ ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤.\ncostê°€ ë§Œì¡±í• ë§Œí•œ ìˆ˜ì¤€ì´ ë  ë•Œ ê¹Œì§€ 1 ~ 3ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n\n\\[ W := W - \\alpha\\frac{\\partial}{\\partial W}cost(W,b) \\] \\[ b := b - \\alpha\\frac{\\partial}{\\partial b}cost(W,b) \\] ì˜ˆì‹œ ë°ì´í„°ë¥¼ ì•„ë˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ í†µí•´ \\(Wx + b\\)ë¥¼ êµ¬í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\n# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\nlibrary(tidyverse)\nlibrary(glue)\n\n# Gradient Descent í•¨ìˆ˜ ì •ì˜\ngradient_descent_simple_lr &lt;- function(X, y, alpha = 0.005, iterations = 3000) {\n  m &lt;- length(X)\n  b &lt;- runif(1) # ì ˆí¸ ì´ˆê¸°í™”\n  w &lt;- runif(1) # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n  \n  for (i in 1:iterations) {\n    y_hat &lt;- w * X + b\n    error &lt;- y - y_hat\n    b_gradient &lt;- sum(error) * (-2 / m)\n    w_gradient &lt;- sum(error * X) * (-2 / m)\n    b &lt;- b - alpha * b_gradient\n    w &lt;- w - alpha * w_gradient\n  }\n  \n  return(list(\"b\" = b, \"W\" = w))\n}\n\n# Gradient Descent ì‹¤í–‰\nresult &lt;- gradient_descent_simple_lr(example$hours, example$score)\n\n# ê²°ê³¼ ì¶œë ¥\n\ncat(glue(\"ì ˆí¸ : {result$b} \\nê°€ì¤‘ì¹˜ : {result$W}\"))\n\nì ˆí¸ : 22.889648962238 \nê°€ì¤‘ì¹˜ : 6.6012872777775\n\nmod &lt;- lm(score ~ hours, example)\ncat(mod$coefficients)\n\n22.9 6.6"
  },
  {
    "objectID": "posts/dual-number/index.html",
    "href": "posts/dual-number/index.html",
    "title": "ì´ì›ìˆ˜ì™€ ì „ì§„ìë™ë¯¸ë¶„",
    "section": "",
    "text": "ì´ì›ìˆ˜(Dual Number)ë€..\nì´ì›ìˆ˜(dual number)ëŠ” ë³µì†Œìˆ˜ì˜ ì¼ë°˜í™”ë¡œ ë³¼ ìˆ˜ ìˆëŠ” ìˆ˜ ì²´ê³„ì…ë‹ˆë‹¤. ë“€ì–¼ ë„˜ë²„ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ í‘œí˜„ë©ë‹ˆë‹¤:\n\\[\n\\alpha + {\\beta}{\\epsilon}\n\\]\nì—¬ê¸°ì„œ aì™€ bëŠ” ì‹¤ìˆ˜ì´ë©°, ÎµëŠ” ë“€ì–¼ ë‹¨ìœ„ë¡œ, Îµ^2 = 0 (Îµ â‰  0)ì„ ë§Œì¡±í•©ë‹ˆë‹¤. ë³µì†Œìˆ˜ì—ì„œ í—ˆìˆ˜ ë‹¨ìœ„ iì™€ ë¹„ìŠ·í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\nì´ì›ìˆ˜ë¥¼ í™œìš©í•œ ë¯¸ë¶„ì€ ì˜¤ì¼ëŸ¬ì˜ ìˆ˜í•™ì  ì•„ì´ë””ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì´ì›ìˆ˜ì˜ ì„±ì§ˆì„ ì´ìš©í•˜ë©´, í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ì •í™•í•˜ê²Œ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ì›ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ \\(f(x)\\)ì˜ ë„í•¨ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\ní•¨ìˆ˜ \\(f(x)\\)ë¥¼ ì´ì›ìˆ˜ í˜•íƒœë¡œ í™•ì¥í•©ë‹ˆë‹¤. ì¦‰, \\(x = \\alpha + {\\beta}{\\epsilon}\\)ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\ní•¨ìˆ˜ \\(f(x)\\)ì— ë“€ì–¼ ë„˜ë²„ë¥¼ ëŒ€ì…í•˜ì—¬, \\(f(\\alpha + {\\beta}{\\epsilon})\\)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\nê³„ì‚°ëœ ê²°ê³¼ì—ì„œ Îµì— ëŒ€í•œ ê³„ìˆ˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ì´ ê³„ìˆ˜ëŠ” í•¨ìˆ˜ \\(f(x)\\)ì˜ ë„í•¨ìˆ˜ ê°’ì´ ë©ë‹ˆë‹¤.\n\nì˜ˆë¥¼ ë“¤ì–´, \\(f(x)=x^2\\)ë¥¼ ë¯¸ë¶„í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n\\(f(x) = (\\alpha + {\\beta}{\\epsilon})^2 = \\alpha^2 + 2\\alpha\\beta\\epsilon + \\beta^2\\epsilon^2\\)\nìœ„ ì‹ì—ì„œ \\(\\epsilon^2 = 0\\)ì´ë¯€ë¡œ, ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\\(f(\\alpha + \\beta\\epsilon) = (\\alpha + {\\beta}{\\epsilon})^2 = \\alpha^2 + 2\\alpha\\beta\\epsilon\\)\nì—¬ê¸°ì„œ Îµì— ëŒ€í•œ ê³„ìˆ˜ëŠ” 2aì…ë‹ˆë‹¤. ë”°ë¼ì„œ \\(f'(x) = 2x\\)ê°€ ë©ë‹ˆë‹¤. ì´ ê²°ê³¼ëŠ” ê¸°ëŒ€í•œ ëŒ€ë¡œ í•¨ìˆ˜ \\(f(x)=x^2\\)ì˜ ë„í•¨ìˆ˜ ê°’ì…ë‹ˆë‹¤.\nìœ„ë¥¼ ì¼ë°˜í™” í•  ê²½ìš°, \\(f(\\alpha) + f'(\\alpha)\\cdot\\beta\\epsilon\\)ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/dummy-variables/index.html",
    "href": "posts/dummy-variables/index.html",
    "title": "N-1 ë”ë¯¸ ë³€ìˆ˜ë¥¼ í™œìš©í•˜ëŠ” ì´ìœ ",
    "section": "",
    "text": "Sample Data ìƒì„±\nlibrary(tidyverse)\n\nset.seed(42)\nsmp_dt &lt;- tibble(Season = rep(c(\"Spring\", \"Summer\", \"Fall\", \"Winter\"),2),\nSales = as.integer(runif(8,min = 300, max = 1000)))\n\n# A tibble: 8 Ã— 2\n  Season Sales\n  &lt;chr&gt;  &lt;int&gt;\n1 Spring   940\n2 Summer   955\n3 Fall     500\n4 Winter   881\n5 Spring   749\n6 Summer   663\n7 Fall     815\n8 Winter   394\n\n\nì§ê´€ì ìœ¼ë¡œ..\në‹¨ìˆœíˆ ìƒê°í•˜ì˜€ì„ ë•Œ, nê°œì˜ ë”ë¯¸ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ê²½ìš°, Spring ~ Winterê¹Œì§€ì˜ ì—´ ë²¡í„°ë¥¼ í•©í•˜ë©´, Intercept ì—´ì´ ë˜ì–´, ì„ í˜•ë…ë¦½ì„ ë§Œì¡±ì¹˜ ëª»í•œë‹¤. ê·¸ë ‡ê¸°ì— ì—­í–‰ë ¬ì´ êµ¬í•´ì§€ì§€ ì•ŠëŠ” ê²ƒì´ ë‹¹ì—°í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. í•˜ê¸° ì˜ˆì œë¡œ ì‹¤ì œë¡œ ê·¸ëŸ¬í•œì§€ í™•ì¸í•´ë³´ì.\n      Spring Summer Fall Winter Intercept Sales\n[1,]      1      0    0      0         1   940\n[2,]      0      1    0      0         1   955\n[3,]      0      0    1      0         1   500\n[4,]      0      0    0      1         1   881\n[5,]      1      0    0      0         1   749\n[6,]      0      1    0      0         1   663\n[7,]      0      0    1      0         1   815\n[8,]      0      0    0      1         1   394\n\n\nn ë”ë¯¸ ë³€ìˆ˜ì˜ ê²½ìš°..\nsmp_dt_dummy_1 &lt;-\ntibble( \n      Spring = c(1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L),\n      Summer = c(0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L),\n        Fall = c(0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L),\n      Winter = c(0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L),\n      Intercept = rep(1, 8),\n      Sales = smp_dt$Sales\n)\nsmp_dt_dummy_1_mat &lt;- smp_dt_dummy_1 |&gt; as.matrix()\nsmp_dt_dummy_1_mat\n\n      Spring Summer Fall Winter Intercept Sales\n[1,]      1      0    0      0         1   940\n[2,]      0      1    0      0         1   955\n[3,]      0      0    1      0         1   500\n[4,]      0      0    0      1         1   881\n[5,]      1      0    0      0         1   749\n[6,]      0      1    0      0         1   663\n[7,]      0      0    1      0         1   815\n[8,]      0      0    0      1         1   394\nLeast Square Formula ì— ë”°ë¼ \\((A^TA)^{-1} \\cdot A^Tb\\) ë¥¼ ê³„ì‚°í•˜ê³ ì í•  ë•Œ, \\(Det(A^TA)\\)ì˜ ê°’ì´ 0ì´ ë˜ì–´, ì—­í–‰ë ¬ì„ êµ¬í•  ìˆ˜ ì—†ë‹¤. ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ í†µí•´ í•´ë‹¹ í–‰ë ¬ì´ ì„ í˜•ì ìœ¼ë¡œ ì¢…ì†ë˜ì–´ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\nsolve(t(smp_dt_dummy_1_mat[,1:5]) %*% smp_dt_dummy_1_mat[,1:5])\n %*% t(smp_dt_dummy_1_mat[,1:5]) %*% (smp_dt_dummy_1_mat[,6])\n#---------------------------------------------------------------------\nError in solve.default(t(smp_dt_dummy_1_mat[, 1:5]) %*% smp_dt_dummy_1_mat[,  : \n  Lapack routine dgesv: system is exactly singular: U[5,5] = 0\n#------------------------------------------------------------------------\ndet(t(smp_dt_dummy_1_mat[,1:5]) %*% smp_dt_dummy_1_mat[,1:5])\n[1] 0\n\n\nn-1 ë”ë¯¸ ë³€ìˆ˜ì˜ ê²½ìš°..\nsmp_dt_dummy_2 &lt;-\n  tibble(\n      Spring = c(1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L),\n      Summer = c(0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L),\n        Fall = c(0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L),\n      Intercept = rep(1, 8),\n      Sales = smp_dt$Sales\n  )\n\nsmp_dt_dummy_2\n\n      Spring Summer Fall Intercept Sales\n[1,]      1      0    0         1   940\n[2,]      0      1    0         1   955\n[3,]      0      0    1         1   500\n[4,]      0      0    0         1   881\n[5,]      1      0    0         1   749\n[6,]      0      1    0         1   663\n[7,]      0      0    1         1   815\n[8,]      0      0    0         1   394\në°˜ë©´ n-1 dummy variablesì˜ ê²½ìš°, Least Square Formulaì˜ ê³„ì‚°ì´ ì œëŒ€ë¡œ ëœë‹¤.\nsolve(t(smp_dt_dummy_2_mat[,1:4]) %*% smp_dt_dummy_2_mat[,1:4]) %*% \nt(smp_dt_dummy_2_mat[,1:4]) %*% (smp_dt_dummy_2_mat[,5])\n\n           [,1]\nSpring    207.0\nSummer    171.5\nFall       20.0\nIntercept 637.5\n\nlm(Sales ~., smp_dt_dummy_2)\n\nCall:\nlm(formula = Sales ~ ., data = smp_dt_dummy_2)\n\nCoefficients:\n(Intercept)       Spring       Summer         Fall\n      637.5        207.0        171.5         20.0"
  },
  {
    "objectID": "posts/ensemble-learning/index.html",
    "href": "posts/ensemble-learning/index.html",
    "title": "Ensemble Learning",
    "section": "",
    "text": "Ensemble Learningì€..\nì•™ìƒë¸” í•™ìŠµ(Ensemble learning)ì€ ì—¬ëŸ¬ ê°œì˜ ê¸°ë³¸ í•™ìŠµ ëª¨ë¸(base learners)ì„ ê²°í•©í•˜ì—¬ ë” ê°•ë ¥í•œ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“œëŠ” ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²•ì…ë‹ˆë‹¤. ì•™ìƒë¸” í•™ìŠµì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ê°œë³„ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬, ë” ë‚˜ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³ ì í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì•™ìƒë¸” í•™ìŠµì€ ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ì„±ëŠ¥ê³¼ ì•ˆì •ì„±ì„ ì œê³µí•˜ë©°, ê³¼ì í•©(overfitting)ì„ ë°©ì§€í•˜ëŠ” íš¨ê³¼ë„ ìˆìŠµë‹ˆë‹¤.\nì•™ìƒë¸” í•™ìŠµì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ ì „ëµì´ ìˆìœ¼ë©°, ëŒ€í‘œì ì¸ ë°©ë²•ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì´ ìˆìŠµë‹ˆë‹¤:\n\në°°ê¹…(Bagging, Bootstrap Aggregating): ë°°ê¹…ì€ ì—¬ëŸ¬ ê°œì˜ ê¸°ë³¸ í•™ìŠµ ëª¨ë¸ì„ ë³‘ë ¬ë¡œ í›ˆë ¨ì‹œí‚¤ê³ , ê·¸ ê²°ê³¼ë¥¼ í‰ê· (íšŒê·€) ë˜ëŠ” íˆ¬í‘œ(ë¶„ë¥˜) ë°©ì‹ìœ¼ë¡œ ì¢…í•©í•©ë‹ˆë‹¤. í•™ìŠµ ë°ì´í„°ì…‹ì„ ì—¬ëŸ¬ ê°œì˜ ë¶€íŠ¸ìŠ¤íŠ¸ë©(bootstrap) ìƒ˜í”Œë¡œ ìƒì„±í•˜ì—¬ ê° ëª¨ë¸ì„ ë³„ë„ë¡œ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ë¶„ì‚°ì„ ì¤„ì´ê³  ê³¼ì í•©ì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€í‘œì ì¸ ì˜ˆë¡œ ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest)ê°€ ìˆìŠµë‹ˆë‹¤. Random Forest\në¶€ìŠ¤íŒ…(Boosting): ë¶€ìŠ¤íŒ…ì€ ê¸°ë³¸ í•™ìŠµ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í›ˆë ¨ì‹œí‚¤ë©´ì„œ, ì´ì „ ëª¨ë¸ì˜ ì˜¤ì°¨ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤. ê° ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ì—¬, ì˜¤ì°¨ê°€ í° ë°ì´í„° í¬ì¸íŠ¸ì— ë” ì§‘ì¤‘í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ í¸í–¥ì„ ì¤„ì´ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€í‘œì ì¸ ì˜ˆë¡œ ì—ì´ë‹¤ë¶€ìŠ¤íŠ¸(AdaBoost), ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…(Gradient Boosting), XGBoost, LightGBM ë“±ì´ ìˆìŠµë‹ˆë‹¤. Gradient Boosting\n\nì•™ìƒë¸” í•™ìŠµì€ ì„œë¡œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ë˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œ ë‹¤ì–‘í•œ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì „ì²´ì ì¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°ì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ ê° ëª¨ë¸ì˜ ì¥ì ì„ í™œìš©í•˜ê³ , ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì´ ê°€ì§„ ë‹¨ì ì„ ìƒì‡„í•˜ì—¬ ë” ì•ˆì •ì ì´ê³  ì¼ë°˜í™”ëœ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\nì•™ìƒë¸” í•™ìŠµì˜ ëª‡ ê°€ì§€ ì£¼ìš” ì´ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\nì„±ëŠ¥ í–¥ìƒ: ì•™ìƒë¸” í•™ìŠµì€ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ë ¥ì„ ê²°í•©í•˜ì—¬ ë” ë†’ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤. ì´ëŠ” ê° ëª¨ë¸ì´ ê°€ì§„ ì§€ì‹ê³¼ ì „ë¬¸ì„±ì„ ì¢…í•©í•˜ê³ , ë‹¤ì–‘í•œ ë¬¸ì œ í•´ê²° ë°©ì‹ì„ ê³ ë ¤í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\nê³¼ì í•© ë°©ì§€: ì•™ìƒë¸” í•™ìŠµì€ ì—¬ëŸ¬ ëª¨ë¸ì„ ê²°í•©í•¨ìœ¼ë¡œì¨ ê³¼ì í•©ì„ ì™„í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ëª¨ë¸ì€ ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì—ì„œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ë¯€ë¡œ, ë‹¨ì¼ ëª¨ë¸ì´ ê°€ì§„ ê³¼ì í•© ë¬¸ì œë¥¼ ìƒì‡„í•˜ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤.\nì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒ: ì•™ìƒë¸” í•™ìŠµì€ ë‹¤ì–‘í•œ ëª¨ë¸ì„ í†µí•©í•¨ìœ¼ë¡œì¨ ë” ì¼ë°˜ì ì¸ íŒ¨í„´ê³¼ ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ì•™ìƒë¸” ëª¨ë¸ì€ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ëŠ¥ë ¥ì´ í–¥ìƒë©ë‹ˆë‹¤.\nì•ˆì •ì„±: ì•™ìƒë¸” í•™ìŠµì€ ì—¬ëŸ¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ë‹¨ì¼ ëª¨ë¸ì˜ ê²°í•¨ì´ë‚˜ ë…¸ì´ì¦ˆì— ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ” ì•ˆì •ì ì¸ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/entropy/index.html",
    "href": "posts/entropy/index.html",
    "title": "Entropy ì‰½ê²Œ ì´í•´í•˜ê¸°",
    "section": "",
    "text": "Entropyë€..\nì—”íŠ¸ë¡œí”¼(entropy)ëŠ” ì •ë³´ ì´ë¡ ì—ì„œ í™•ë¥  ë³€ìˆ˜ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì—”íŠ¸ë¡œí”¼ëŠ” ì–´ë–¤ ì •ë³´ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ í•„ìš”í•œ í‰ê·  ë¹„íŠ¸ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì´ë¥¼ í†µí•´ ì •ë³´ì˜ ë³µì¡ì„±ì´ë‚˜ ì••ì¶• ê°€ëŠ¥ì„±ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë†’ì€ ì—”íŠ¸ë¡œí”¼ëŠ” ë†’ì€ ë¶ˆí™•ì‹¤ì„±ì„ ì˜ë¯¸í•˜ë©°, ë‚®ì€ ì—”íŠ¸ë¡œí”¼ëŠ” ë‚®ì€ ë¶ˆí™•ì‹¤ì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\nì‰½ê²Œ ì„¤ëª…í•˜ìë©´, í•œ ê·¸ë£¹ì—ì„œì˜ ë†€ë¼ì›€ì˜ ê¸°ëŒ€ê°’ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ë†€ë¼ì›€ì´ë€, \\(log(\\frac{1}{p(x)})\\)ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ë¡œë˜ì— ë‹¹ì²¨ì´ ë  í™•ë¥ ì´ ë§¤ìš° ë‚®ê¸°ì—, ë‹¹ì²¨ë˜ë©´ ë†€ë¼ì›€ì´ í° ê²ƒê³¼ ê°™ì€ ë§¥ë½ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n90%ì˜ í™•ë¥ ë¡œ ì•ë©´ì´ ë‚˜ì˜¤ëŠ” ë™ì „ì„ 100ë²ˆ ë˜ì§€ëŠ” ê²ƒì„ ì˜ˆë¡œ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤:\n\n\n\n\nì•\në’¤\n\n\n\n\ní™•ë¥ \n0.9\n0.1\n\n\në†€ë¼ì›€\n0.15\n3.32\n\n\n\n100ë²ˆì„ ë˜ì§ˆ ê²½ìš°ì˜ ë†€ë¼ì›€ì€ \\((0.9 \\times 100 \\times 1.5) + (0.1 \\times 100 \\times 3.32)\\) ì…ë‹ˆë‹¤. ì´ ë†€ë¼ì›€ì˜ ê¸°ëŒ€ê°’ ì¦‰ EntropyëŠ” \\(\\frac{(0.9 \\times 100 \\times 1.5) + (0.1 \\times 100 \\times 3.32)}{100}\\)ë¡œ, 0.47ì…ë‹ˆë‹¤.\në‹¤ë§Œ, ì—¬ê¸°ì„œ ë˜ì§„ íšŸìˆ˜ëŠ” ì•½ë¶„ì´ ë˜ê¸°ì— ìœ„ì˜ Entropyë¥¼ ì¼ë°˜í™”í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ í‘œê¸°ë©ë‹ˆë‹¤:\n\\[\n\\sum log(\\frac{1}{p(x)}) \\cdot p(x)\n\\]"
  },
  {
    "objectID": "posts/error-metrics/index.html",
    "href": "posts/error-metrics/index.html",
    "title": "1ì¢… ì˜¤ë¥˜ 2ì¢… ì˜¤ë¥˜ í—·ê°ˆë¦¬ì§€ ì•Šê¸°",
    "section": "",
    "text": "1ì¢… ì˜¤ë¥˜ / 2ì¢… ì˜¤ë¥˜\nì‰½ê²Œ ì„¤ëª…í•˜ìë©´, 1ì¢… ì˜¤ë¥˜ëŠ” ê²½ì†”í•œ ê²ƒ / 2ì¢… ì˜¤ë¥˜ëŠ” ë‹µë‹µí•œ ê²ƒìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì½”ë¡œë‚˜ ê²€ì‚¬ë¥¼ ì˜ˆë¥¼ ë“¤ì–´ë´…ì‹œë‹¤. ì½”ë¡œë‚˜ì— ê±¸ë¦¬ì§€ ì•Šì€ ì‚¬ëŒì„ ì½”ë¡œë‚˜ë¡œ ì§„ë‹¨í•˜ëŠ” ê²ƒ, ì¦‰ ê²½ì†”í•¨ì€ 1ì¢… ì˜¤ë¥˜ë¡œ íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì½”ë¡œë‚˜ì— ê±¸ë¦° ì‚¬ëŒì„ ì½”ë¡œë‚˜ì— ê±¸ë¦¬ì§€ ì•Šì•˜ë‹¤ê³  ì§„ë‹¨í•˜ëŠ” ê²ƒ, ì¦‰ ë‹µë‹µí•¨ì€ 2ì¢… ì˜¤ë¥˜ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.\n\nì¦‰ ìƒê¸° ê·¸ë¦¼ì—ì„œ, FPëŠ” 1ì¢… ì˜¤ë¥˜ë¡œ FNì€ 2ì¢… ì˜¤ë¥˜ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/gpt-prompt/index.html",
    "href": "posts/gpt-prompt/index.html",
    "title": "GPT í…œí”Œë¦¿",
    "section": "",
    "text": "GPTì—ê²Œ í•  ìˆ˜ ìˆëŠ” Command\n\nKeep going : ì»¨í‹°ë‰´ë³´ë‹¤ ì´ìª½ì´ ë” í™•ì‹¤\nMake a list of :Â ëª©ë¡ì„ ì‘ì„±í•˜ë¼.Â ì•„ë˜ìª½ì˜ â€˜ì“°ë‹¤â€™ ë§Œí¼ ë„“ì€ ë²”ìœ„ë¥¼ ì°¸ì¡°í•¨. ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ë¶™ì—¬ì„œ ì•Œê¸° ì‰½ê²Œ ì •ë¦¬í•´ì¤Œ.\nExplain :Â ëª¨ë“  ì£¼ì œì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…ì„ ì–»ê¸° ìœ„í•´ ì‚¬ìš©\nImprove :Â ì»¨í…ì¸ ë¥¼ ì œê³µí•˜ê³  GPTì— ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ê±°ë‚˜ ì»¨í…ì¸ ë¥¼ ì¶”ê°€í•˜ë„ë¡ ìš”ì²­\nPlan :Â ëª©í‘œë¥¼ ë” ì‘ì€ ë‹¨ê³„ë¡œ ë¶„ë¥˜í•˜ë„ë¡ ìš”ì²­\nSummarize : ìš”ì•½í•˜ë„ë¡ ìš”ì²­\nExpand :Â ì´ì „ ë‹µë³€ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì„ ì„¤ëª…í•˜ë„ë¡ ìš”ì²­. ë˜ëŠ” ë‚´ê°€ ìœ„ë‚˜ ì•„ë˜ì— ì œê³µí•œ ë¬¸ì¥ ë˜ëŠ” ì¥ë©´ì„ ë” ìì„¸íˆ í’€ì–´ì„œ ì„¤ëª…í•´ì¤Œ.\n\n\n\nì–¸ì–´ ëª¨ë¸ì´ ì„ íƒí•œ ìµœì ì˜ í”„ë¡¬í”„íŠ¸\n\nâ€œLetâ€™s work this out in a step by step way to be sure we have the right answer.â€\nì¶œì²˜: [2211.01910] Large Language Models Are Human-Level Prompt Engineers\n\n\nGPT ì½”ë”©\nAct as CODEX (â€œCOding DEsign eXpertâ€), an expert coder with experience in multiple coding languages. Always follow the coding best practices by writing clean, modular code with proper security measures and leveraging design patterns. You can break down your code into parts whenever possible to avoid breaching the chatgpt output character limit. Write code part by part when I send â€œcontinueâ€. If you reach the character limit, I will send â€œcontinueâ€ and then you should continue without repeating any previous code. Do not assume anything from your side; please ask me a numbered list of essential questions before starting. If you have trouble fixing a bug, ask me for the latest code snippets for reference from the official documentation. I am using [MacOS], [VSCode] and prefer [brew] package manager. Start a conversation as â€œCODEX: Hi, what are we coding today?â€\nì¶œì²˜: GPT-4 CODEX: Coding Design Expert; A Secret Prompt To Rule Them All | by ğšƒğš‘ğš ğ™»ğšŠğšğšğšœğš ğ™½ğš˜ğš  ~ ğ™°ğ™¸ | Mar, 2023 | Artificial Intelligence in Plain English"
  },
  {
    "objectID": "posts/gradient-boosting/index.html",
    "href": "posts/gradient-boosting/index.html",
    "title": "Gradient Boosting",
    "section": "",
    "text": "Gradient Boostingì´ë€..\nê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…(Gradient Boosting)ì€ ì•™ìƒë¸” í•™ìŠµ ê¸°ë²• ì¤‘ í•˜ë‚˜ë¡œ, ì¼ë ¨ì˜ ì•½í•œ í•™ìŠµê¸°(ë³´í†µ ê²°ì • íŠ¸ë¦¬)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œì¼œ ê°•ë ¥í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì€ ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸(ê¸°ìš¸ê¸°)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ë©°, ì´ë¥¼ í†µí•´ ê° í•™ìŠµê¸°ê°€ ì´ì „ í•™ìŠµê¸°ì˜ ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.\nê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì˜ ì£¼ìš” ë‹¨ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\nì´ˆê¸° ì˜ˆì¸¡: ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ ë™ì¼í•œ ì´ˆê¸° ì˜ˆì¸¡ ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤. ì´ ê°’ì€ ì¼ë°˜ì ìœ¼ë¡œ í‰ê· (íšŒê·€) ë˜ëŠ” ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤(ë¶„ë¥˜)ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.\nì•½í•œ í•™ìŠµê¸° í•™ìŠµ: ì²« ë²ˆì§¸ ì•½í•œ í•™ìŠµê¸°(ì¼ë°˜ì ìœ¼ë¡œ ê²°ì • íŠ¸ë¦¬)ë¥¼ í•™ìŠµì‹œí‚¤ê³ , ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\nì†ì‹¤ í•¨ìˆ˜ ë° ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°: ì†ì‹¤ í•¨ìˆ˜(ì˜ˆ: í‰ê·  ì œê³± ì˜¤ì°¨, ë¡œê·¸ ì†ì‹¤ ë“±)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ ì¸¡ì •í•˜ê³ , ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\nìƒˆë¡œìš´ ì•½í•œ í•™ìŠµê¸° í•™ìŠµ: ì´ì „ í•™ìŠµê¸°ì˜ ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ìƒˆë¡œìš´ ì•½í•œ í•™ìŠµê¸°ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì´ ê³¼ì •ì€ ê·¸ë˜ë””ì–¸íŠ¸ì— ìŒì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\ní•™ìŠµê¸° ê²°í•©: ëª¨ë“  ì•½í•œ í•™ìŠµê¸°ì˜ ì˜ˆì¸¡ì„ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ ê²°í•©í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì´ ê°œì„ ë©ë‹ˆë‹¤.\nìˆ˜ë ´ ì—¬ë¶€ í™•ì¸: ì†ì‹¤ í•¨ìˆ˜ ê°’ì´ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šê±°ë‚˜ ì§€ì •ëœ ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë‹¨ê³„ 2-5ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤.\n\nê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì€ ë‹¤ì–‘í•œ ë¬¸ì œì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, íšŒê·€ì™€ ë¶„ë¥˜ ë¬¸ì œ ëª¨ë‘ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. XGBoost, LightGBM ë° CatBoostì™€ ê°™ì€ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… êµ¬í˜„ì€ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì—ì„œë„ ë¹ ë¥´ê²Œ í•™ìŠµë˜ë„ë¡ ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, ê³ ì„±ëŠ¥ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ë° ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\n\n\nHistGradientBoosting ì˜ˆì œ\nr\n# load required library\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(reticulate)\n\n\npython\n# load required library\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n\n\npython\n# Split Data\ndata = load_breast_cancer()\nX, y = data.data, data.target\ncolname_x = data.feature_names\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\npython\n# Init Model\nmodel = HistGradientBoostingClassifier(random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n\nr\n# Calc Accuracy\nX_test &lt;- as_tibble(py$X_test,.name_repair)\ny_test &lt;- tibble(py$y_test)\ny_pred = tibble(py$y_pred)\n\nresult &lt;- X_test |&gt; bind_cols(y_test,y_pred)\ncolnames(result) &lt;- c(py$colname_x,\"truth\",\"estimate\")\nresult$truth &lt;- as.factor(result$truth)\nresult$estimate &lt;- as.factor(result$estimate)\n\nyardstick::accuracy(result,truth,estimate)\n# A tibble: 1 Ã— 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.974"
  },
  {
    "objectID": "posts/hierarchical/index.html",
    "href": "posts/hierarchical/index.html",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Hierarchical Clusteringì€..\në°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•˜ëŠ”ë°, ì²˜ìŒì—ëŠ” ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ë¡œ ê°„ì£¼ë˜ë©° ê°€ì¥ ê°€ê¹Œìš´ í´ëŸ¬ìŠ¤í„°ë¼ë¦¬ ë³‘í•©í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ì„œ ìµœì¢…ì ìœ¼ë¡œ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ë¡œ í•©ì³ì§‘ë‹ˆë‹¤. ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì€ ë´ë“œë¡œê·¸ë¨(dendrogram)ì„ ì‚¬ìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„° êµ¬ì¡°ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nHierachical Clustringì˜ ë³‘í•© ë°©ë²• (Linkage Method)\n\nì™„ì „ ì—°ê²°ë²• (Complete Linkage): í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ë¥¼ ë‘ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ” ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ ìŒì˜ ê±°ë¦¬ ì¤‘ ìµœëŒ€ê°’ìœ¼ë¡œ ì •ì˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë¹„êµì  ë°€ì§‘ë˜ê³  ì˜ êµ¬ë¶„ëœ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. (method = â€œcompleteâ€)\në‹¨ì¼ ì—°ê²°ë²• (Single Linkage): í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ë¥¼ ë‘ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ” ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ ìŒì˜ ê±°ë¦¬ ì¤‘ ìµœì†Œê°’ìœ¼ë¡œ ì •ì˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê¸¸ê³  ëŠ˜ì–´ì§„ ì²´ì¸ í˜•íƒœì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ë…¸ì´ì¦ˆì— ë¯¼ê°í•œ íŠ¹ì„±ì´ ìˆìŠµë‹ˆë‹¤. (method = â€œsingleâ€)\ní‰ê·  ì—°ê²°ë²• (Average Linkage): í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ë¥¼ ë‘ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ” ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ ìŒì˜ ê±°ë¦¬ì˜ í‰ê· ê°’ìœ¼ë¡œ ì •ì˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë…¸ì´ì¦ˆì— ìƒëŒ€ì ìœ¼ë¡œ ëœ ë¯¼ê°í•˜ë©°, ë‹¨ì¼ ì—°ê²°ë²•ì— ë¹„í•´ ë” ê· í˜• ì¡íŒ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. (method = â€œaverageâ€)\nì¤‘ì‹¬ ì—°ê²°ë²• (Centroid Linkage): í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ë¥¼ ë‘ í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬(centroid, ì¦‰ í‰ê·  ë²¡í„°) ê°„ì˜ ê±°ë¦¬ë¡œ ì •ì˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë³´ë‹¤ ë°€ì§‘ëœ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•˜ëŠ” ê²½í–¥ì´ ìˆì§€ë§Œ, ê±°ë¦¬ ì²™ë„ì˜ ì„ íƒì— ë¯¼ê°í•œ íŠ¹ì„±ì´ ìˆìŠµë‹ˆë‹¤. (method = â€œcentroidâ€)\n\n\n\nHierarchical Clustering ì˜ˆì œ\n\n\n\nResult of Hierachical clustering\n\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(cluster)\n\n# Extract numeric columns\niris_numeric &lt;- iris[, 1:4]\n\n# Standardize the data\niris_std &lt;- scale(iris_numeric)\n\n# Compute Euclidean distance matrix\ndist_matrix &lt;- dist(iris_std, method = \"euclidean\")\n\n# Perform hierarchical clustering\nhc &lt;- hclust(dist_matrix, method = \"complete\")\n\n# Plot dendrogram\nplot(hc, labels = iris$Species, cex = 0.6, hang = -1)\n\n# Cut the tree into clusters\nk &lt;- 3 # Number of clusters\ncluster_assignments &lt;- cutree(hc, k)\n\n# Add cluster assignments to the Iris dataset\n\niris &lt;- iris |&gt; mutate(Cluster = case_when(\n  cluster_assignments == 1 ~ \"setosa\",\n  cluster_assignments == 2 ~ \"versicolor\",\n  .default = \"virginica\"\n))\n\niris$Cluster &lt;- as.factor(iris$Cluster)\n\niris |&gt; yardstick::conf_mat(Species, Cluster)\niris |&gt; yardstick::accuracy(Species, Cluster)\nTruth\nPrediction   setosa versicolor virginica\n  setosa         49          0         0\n  versicolor      1         21         2\n  virginica       0         29        48\n\naccuracy multiclass     0.787"
  },
  {
    "objectID": "posts/intro-to-tensorflow/index.html",
    "href": "posts/intro-to-tensorflow/index.html",
    "title": "2023 MIT 6.S191 01 Intro to TensorFlow",
    "section": "",
    "text": "2023ë…„ MITì—ì„œ ê°•ì˜ëœ 6.S191 ê°•ì¢Œì˜ Labì˜ì´í•´ë¥¼ ë•ê¸°ìœ„í•´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\nLabì— ëŒ€í•´ì„œëŠ” ì´ê³³ì—ì„œ í™•ì¸ê°€ëŠ¥í•˜ë©°, í•´ë‹¹ ê°•ì¢ŒëŠ” ì—¬ê¸°ì„œ í™•ì¸ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\nLab1-Part1 Intro to Tensorflow\nLab1-Part2 Music Generation\nLab2-Part1 MNIST\nLab2-Part2 Face Detection\nLab3-Part1 Introduction Caspa\nLab3-Part2 Bias and Uncertainty\n\n\n0.1 Load Required Library\n%tensorflow_version 2.x\nimport tensorflow as tf\n\n# Download and import the MIT Introduction to Deep Learning package\n!pip install mitdeeplearning\nimport mitdeeplearning as mdl\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n1.1 ì™œ TensorFlowëŠ” TensorFlowì¸ê°€ìš”?\nTensorFlowëŠ” ë‹¤ì°¨ì› ë°°ì—´ë¡œ ìƒê°í•  ìˆ˜ ìˆëŠ” ë°ì´í„° êµ¬ì¡°ì¸ í…ì„œì˜ íë¦„(ë…¸ë“œ/ìˆ˜í•™ì  ì—°ì‚°)ì„ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— â€™TensorFlowâ€™ë¼ê³  ë¶ˆë¦½ë‹ˆë‹¤.\ní…ì„œëŠ” ë¬¸ìì—´ì´ë‚˜ ì •ìˆ˜ì™€ ê°™ì€ ê¸°ë³¸ ë°ì´í„° íƒ€ì…ì˜ nì°¨ì› ë°°ì—´ë¡œ í‘œí˜„ë˜ë©°, ë²¡í„°ì™€ í–‰ë ¬ì„ ë” ë†’ì€ ì°¨ì›ìœ¼ë¡œ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.\ní…ì„œì˜ í˜•íƒœëŠ” ì°¨ì› ìˆ˜ (ndim)ì™€ ê° ì°¨ì›ì˜ í¬ê¸°(rank)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë¨¼ì € Scalarë¥¼ ì˜ˆë¡œ 0-d í…ì„œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:\nsport = tf.constant(\"Tennis\", tf.string)\nnumber = tf.constant(1.41421356237, tf.float64)\n\nprint(f\"`sport` is a {tf.rank(sport).numpy()}-d Tensor\")\nprint(f\"`number` is a {tf.rank(number).numpy()}-d Tensor\")\n`sport` is a 0-d Tensor\n`number` is a 0-d Tensor\në²¡í„°ì™€ ë¦¬ìŠ¤íŠ¸ëŠ” 1-d í…ì„œì…ë‹ˆë‹¤.\nsports = tf.constant([\"Tennis\", \"Basketball\"], tf.string)\nnumbers = tf.constant([3.141592, 1.414213, 2.71821], tf.float64)\n\nprint(f\"`sports` is a {tf.rank(sports).numpy()}-d Tensor\")\nprint(f\"`numbers` is a {tf.rank(numbers).numpy()}-d Tensor\")\n`sports` is a 1-d Tensor\n`numbers` is a 1-d Tensor\në‹¤ìŒìœ¼ë¡œ 2ì°¨ì› í…ì„œ (í–‰ë ¬) ë° ê³ ì°¨ì› í…ì„œë¥¼ ìƒì„±í•´ ë´…ì‹œë‹¤.\nì˜ˆë¥¼ ë“¤ì–´, í–¥í›„ ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì»´í“¨í„° ë¹„ì „ê³¼ ê´€ë ¨ëœ ì‹¤ìŠµì—ì„œëŠ” 4-d í…ì„œë¥¼ ì‚¬ìš©í•˜ê²Œ ë  ê²ƒì…ë‹ˆë‹¤.\nì—¬ê¸°ì„œ ì°¨ì›ì€ ë°°ì¹˜ì— í¬í•¨ëœ ì˜ˆì œ ì´ë¯¸ì§€ì˜ ìˆ˜, ì´ë¯¸ì§€ ë†’ì´, ì´ë¯¸ì§€ ë„ˆë¹„, ì»¬ëŸ¬ ì±„ë„ ìˆ˜ì— í•´ë‹¹í•©ë‹ˆë‹¤.\nmatrix = tf.constant([[1,4],[3,2]],tf.float64)\nimages = tf.zeros([10,256,256,3],tf.float64) # 10 images with RGB color, 256 x 256 pixels\nprint(tf.rank(images).numpy().tolist())\nprint(tf.shape(images).numpy().tolist())\n4\n[10, 256, 256, 3]\në³´ì‹œë‹¤ì‹œí”¼ shapeëŠ” ê° í…ì„œ ì°¨ì›ì— ìˆëŠ” ìš”ì†Œì˜ ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\nì´ í•¨ìˆ˜ëŠ” ë§¤ìš° ìœ ìš©í•˜ë©° ìì£¼ ì‚¬ìš©í•˜ê²Œ ë  ê²ƒì…ë‹ˆë‹¤.\n\n\nTensorì˜ ê³„ì‚°\nTensorFlowì—ì„œ ê³„ì‚°ì„ ìƒê°í•˜ê³  ì‹œê°í™”í•˜ëŠ” í¸ë¦¬í•œ ë°©ë²•ì€ ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\nê·¸ë˜í”„ëŠ” ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” í…ì„œì™€ ì´ëŸ¬í•œ í…ì„œì— ì¼ì •í•œ ìˆœì„œë¡œ ì‘ìš©í•˜ëŠ” ìˆ˜í•™ì  ì—°ì‚°ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ í†µí•´ TensorFlowë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ ê³„ì‚°ì„ ì •ì˜í•´ ë³´ê² ìŠµë‹ˆë‹¤:\n\n# Node Aì™€ Bë¥¼ ì •ì˜í•˜ê³  ê°’ì„ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.\na = tf.constant(15)\nb = tf.constant(61)\n\n# Aì™€ Bë¥¼ ë”í•©ë‹ˆë‹¤\nc1 = tf.add(a,b)\nc2 = a + b # TensorFLowëŠ” `+`ë¥¼ ì˜¤ë²„ë¼ì´ë“œ í•˜ì—¬ `c1`ê³¼ ê°™ì´ ë™ì‘í•˜ê²Œ í•©ë‹ˆë‹¤.\nprint(c1)\nprint(c2)\ntf.Tensor(76, shape=(), dtype=int32)\ntf.Tensor(76, shape=(), dtype=int32)\ní…ì„œí”Œë¡œ ì—°ì‚°ìœ¼ë¡œ êµ¬ì„±ëœ ê³„ì‚° ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ê³  ì—°ì‚°ì„ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•œ ê²°ê³¼ ê°’ì´ 76ì¸ í…ì„œë¥¼ ìƒì„±í•œ ê²ƒì„ ë³´ì•˜ìŠµë‹ˆë‹¤.\nì´ì œ ì¡°ê¸ˆ ë” ë³µì¡í•œ ì˜ˆë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:\n\nì—¬ê¸°ì„œëŠ” ë‘ ê°œì˜ ì…ë ¥ a, bë¥¼ ë°›ì•„ ì¶œë ¥ eë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\nê·¸ë˜í”„ì˜ ê° ë…¸ë“œëŠ” ì¼ë¶€ ì…ë ¥ì„ ë°›ì•„ ì¼ë¶€ ì—°ì‚°ì„ ìˆ˜í–‰í•œ í›„ ê·¸ ì¶œë ¥ì„ ë‹¤ë¥¸ ë…¸ë“œë¡œ ì „ë‹¬í•˜ëŠ” ì—°ì‚°ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nì´ ê³„ì‚° í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ í…ì„œí”Œë¡œì—ì„œ ì •ì˜í•´ ë³´ê² ìŠµë‹ˆë‹¤:\n### Defining Tensor computations ###\n\n# Construct a simple computation function\ndef func(a,b):\n  c = tf.add(a,b)\n  d = tf.subtract(b,1)\n  e = tf.multiply(c,d)\n  return e\n\n# Consider example values for a,b\na, b = 1.5, 2.5\n# Execute the computation\ne_out = func(a,b)\nprint(e_out)\ntf.Tensor(6.0, shape=(), dtype=float32)\n\n\n1.3 TensorFlowì—ì„œì˜ Neural Network\në“œë””ì–´ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬í˜„í•  ì°¨ë¡€ì…ë‹ˆë‹¤!\nTensorFlowì—ì„œëŠ” Kerasë¼ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ìœ„í•œ ì§ê´€ì ì¸ High level APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë¨¼ì € í•˜ë‚˜ì˜ Dense Layerë¥¼ ê°€ì§„ single perceptron($ y=Ïƒ(Wx+b)$)ì„ êµ¬í˜„í•´ë³´ë„ë¡ í•´ë³´ê² ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ WëŠ” í–‰ë ¬ì˜ weights, bëŠ” bias, xëŠ” input, ÏƒëŠ” activation í•¨ìˆ˜, yëŠ” outputì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n\n### Defining a network Layer ###\n\n# n_output_nodes: number of output nodes\n# input_shape: shape of the input\n# x: input to the layer\n\nclass OurDenseLayer(tf.keras.layers.Layer):\n  def __init__(self, n_output_nodes):\n    super(OurDenseLayer, self).__init__()\n    self.n_output_nodes = n_output_nodes\n\n  def build(self, input_shape):\n    d = int(input_shape[-1])\n    # Define and initialize parameters: a weight matrix W and bias b\n    # Note that parameter initialization is random!\n    self.W = self.add_weight(\"weight\", shape=[d, self.n_output_nodes]) # note the dimensionality\n    self.b = self.add_weight(\"bias\", shape=[1, self.n_output_nodes]) # note the dimensionality\n\n  def call(self, x):\n    z = tf.add(tf.matmul(self.X,self.W),self.b)\n    y = tf.sigmoid(z)\n    return y\n\n# Since layer parameters are initialized randomly, we will set a random seed for reproducibility\ntf.random.set_seed(1)\nlayer = OurDenseLayer(3)\nlayer.build((1,2))\nx_input = tf.constant([[1,2.]], shape=(1,2))\ny = layer.call(x_input)\n\n# test the output!\nprint(y.numpy())\n[[0.45985997 0.646087   0.245385  ]]\ní¸ë¦¬í•˜ê²Œë„ í…ì„œí”Œë¡œì—ëŠ” ì‹ ê²½ë§ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì—¬ëŸ¬ ë ˆì´ì–´ê°€ ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\nì´ì œ ë‹¨ì¼ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ì‹ ê²½ë§ì„ ì •ì˜í•˜ëŠ” ëŒ€ì‹  Kerasì˜ Sequential ëª¨ë¸ê³¼ ë‹¨ì¼ Dense ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤.\nSequential APIë¥¼ ì‚¬ìš©í•˜ë©´ ë¹Œë”© ë¸”ë¡ì²˜ëŸ¼ ë ˆì´ì–´ë¥¼ ìŒ“ì•„ì„œ ì‹ ê²½ë§ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### Defining a neural network using the Sequential API ###\n\n# Import relevant packages\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\nn_output_nodes = 3\nmodel = Sequential()\ndense_layer = Dense(n_output_nodes, activation='sigmoid')\nmodel.add(dense_layer)\n\nx_input = tf.constant([[1,2.]], shape=(1,2))\nprint(model(x_input).numpy())\ntf.Tensor([[0.6124562  0.91317874 0.8520293 ]], shape=(1, 3), dtype=float32)\nSequentialëª¨ë¸ì„ ì •ì˜í•˜ëŠ” ëŒ€ì‹ ì— Model classì˜ sub-classë¥¼ ë§Œë“¤ì–´ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ë¥¼ í†µí•˜ì—¬ ì»¤ìŠ¤í…€ ë ˆì´ì–´, ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë‹, ì»¤ìŠ¤í…€ activationì„ ìœ ì—°í•˜ê²Œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n### Defining a model using subclassing ###\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense\n\nclass SubclassModel(tf.keras.Model):\n\n  # In __init__, we define the Model's layers\n  def __init__(self, n_output_nodes):\n    super(SubclassModel, self).__init__()\n    '''TODO: Our model consists of a single Dense layer. Define this layer.''' \n    self.dense_layer = Dense(n_output_nodes, activation='sigmoid')\n\n  # In the call function, we define the Model's forward pass.\n  def call(self, inputs):\n    return self.dense_layer(inputs)\nn_output_nodes = 3\nmodel = SubclassModel(n_output_nodes)\n\nx_input = tf.constant([[1,2.]], shape=(1,2))\n\nprint(model.call(x_input))\ntf.Tensor([[0.09956823 0.34443256 0.8754808 ]], shape=(1, 3), dtype=float32)\n\n\n1.4 TensorFlowì˜ ìë™ ë¯¸ë¶„\nìë™ ë¯¸ë¶„ì€ TensorFlowì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ ì¤‘ í•˜ë‚˜ì´ë©° ì—­ì „íŒŒë¥¼ ì´ìš©í•œ í›ˆë ¨ì˜ ê·¼ê°„ì…ë‹ˆë‹¤.\në¯¸ë¶„ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ tf.GradientTapeë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\në„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ Forward Passê°€ ì´ë£¨ì–´ì§€ë©´ ëª¨ë“  ì—°ì‚°ì´ â€œí…Œì´í”„â€ì— ê¸°ë¡ë˜ê³ , ë¯¸ë¶„ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ í…Œì´í”„ê°€ ì—­ë°©í–¥ìœ¼ë¡œ ì¬ìƒë©ë‹ˆë‹¤.\nê¸°ë³¸ì ìœ¼ë¡œ í…Œì´í”„ëŠ” ì—­ë°©í–¥ìœ¼ë¡œ ì¬ìƒëœ í›„ íê¸°ë˜ë¯€ë¡œ íŠ¹ì • tf.GradientTapeëŠ” í•˜ë‚˜ì˜ ë¯¸ë¶„ë§Œ ê³„ì‚°í•  ìˆ˜ ìˆìœ¼ë©° ì´í›„ í˜¸ì¶œì€ ëŸ°íƒ€ì„ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.\ní•˜ì§€ë§Œ persistent gradient tapeë¥¼ ìƒì„±í•˜ë©´ ë™ì¼í•œ ê³„ì‚°ì„ í†µí•´ ì—¬ëŸ¬ ë¯¸ë¶„ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në¨¼ì € GradientTapeë¥¼ ì‚¬ìš©í•´ ë¯¸ë¶„ì„ ê³„ì‚°í•˜ê³  ê³„ì‚°ì„ ìœ„í•´ ì•¡ì„¸ìŠ¤í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ê°„ë‹¨í•œ í•¨ìˆ˜ \\(y=x^2\\)ë¥¼ ì •ì˜í•˜ê³  ë¯¸ë¶„ì„ ê³„ì‚°í•©ë‹ˆë‹¤:\n### Gradient computation with GradientTape ###\n\n# y = x^2\n# Example: x = 3.0\nx = tf.Variable(3.0)\n\n# Initiate the gradient tape\nwith tf.GradientTape() as tape:\n  # Define the function\n  y = x * x\n# Access the gradient -- derivative of y with respect to x\ndy_dx = tape.gradient(y, x)\n\nprint(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\në‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨í•  ë•Œì—ëŠ” loss í•¨ìˆ˜ë¥¼ ìµœì†Œí™” í•˜ê¸°ìœ„í•´ SGDë¥¼ í™œìš©í•˜ì—¬ ë¯¸ë¶„í•©ë‹ˆë‹¤.\në‹¤ìŒ ì˜ˆì‹œì—ì„œëŠ” Loss í•¨ìˆ˜ë¥¼ SGDë¥¼ í™œìš©í•˜ì—¬ ìµœì†Œí™”í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\\(L = (x- x_f)^2\\)\n### Function minimization with automatic differentiation and SGD ###\n\n# Initialize a random value for our initial x\nx = tf.Variable([tf.random.normal([1])])\nprint(\"Initializing x={}\".format(x.numpy()))\n\nlearning_rate = 1e-2 # learning rate for SGD\nhistory = []\n# Define the target value\nx_f = 4\n\n# We will run SGD for a number of iterations. At each iteration, we compute the loss, \n#   compute the derivative of the loss with respect to x, and perform the SGD update.\nfor i in range(500):\n  with tf.GradientTape() as tape:\n    loss = (x-x_f)**2\n\n  # loss minimization using gradient tape\n  grad = tape.gradient(loss, x) # compute the derivative of the loss with respect to x\n  new_x = x - learning_rate*grad # sgd update\n  x.assign(new_x) # update the value of x\n  history.append(x.numpy()[0])\n\n# Plot the evolution of x as we optimize towards x_f!\nplt.plot(history)\nplt.plot([0, 500],[x_f,x_f])\nplt.legend(('Predicted', 'True'))\nplt.xlabel('Iteration')\nplt.ylabel('x value')"
  },
  {
    "objectID": "posts/k-means/index.html",
    "href": "posts/k-means/index.html",
    "title": "K-means clustering",
    "section": "",
    "text": "image of kmeans\n\n\n\nK-means ì•Œê³ ë¦¬ì¦˜ì€..\nCentroid Based Clustering ê¸°ë²• ì¤‘ í•˜ë‚˜ë¡œ, ë°ì´í„° í¬ì¸íŠ¸ë“¤ì„ Kê°œì˜ êµ°ì§‘ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë¹„ì§€ë„ í•™ìŠµ(unsupervised learning) ë°©ë²•ì…ë‹ˆë‹¤. K-means ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ê±°ì³ ë°ì´í„°ë¥¼ êµ°ì§‘í™”í•©ë‹ˆë‹¤.\n\nì´ˆê¸°í™”: Kê°œì˜ ì´ˆê¸° ì¤‘ì‹¬ì (centroid)ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ê±°ë‚˜, ë°ì´í„° í¬ì¸íŠ¸ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.\ní• ë‹¹: ê° ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì ì— í• ë‹¹í•˜ì—¬, Kê°œì˜ êµ°ì§‘ì„ ìƒì„±í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬(Euclidean distance)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê±°ë¦¬ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.\nì—…ë°ì´íŠ¸: ê° êµ°ì§‘ì˜ ì¤‘ì‹¬ì ì„ ìƒˆë¡­ê²Œ ê³„ì‚°í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ì¤‘ì‹¬ì ì€ í•´ë‹¹ êµ°ì§‘ì— ì†í•œ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì˜ í‰ê·  ìœ„ì¹˜ì…ë‹ˆë‹¤.\nìˆ˜ë ´ ì—¬ë¶€ í™•ì¸: ì¤‘ì‹¬ì ì˜ ìœ„ì¹˜ê°€ ë” ì´ìƒ ë³€í•˜ì§€ ì•Šê±°ë‚˜, ë¯¸ë¦¬ ì •í•œ ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ 2ë‹¨ê³„ì™€ 3ë‹¨ê³„ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤.\n\nK-means ì•Œê³ ë¦¬ì¦˜ì€ ê°„ë‹¨í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›Œ ë„ë¦¬ ì‚¬ìš©ë˜ì§€ë§Œ, ëª‡ ê°€ì§€ ë‹¨ì ë„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ë‹¨ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\nKê°’ì„ ë¯¸ë¦¬ ì„¤ì •í•´ì•¼ í•˜ëŠ”ë°, ì´ëŠ” ìµœì ì˜ êµ°ì§‘ ê°œìˆ˜ë¥¼ ì•Œê¸° ì–´ë ¤ìš´ ê²½ìš° ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ˆê¸° ì¤‘ì‹¬ì  ì„ íƒì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´, ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ ìµœì ì˜ ê²°ê³¼ë¥¼ ì°¾ì•„ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\nêµ°ì§‘ì˜ ëª¨ì–‘ì´ ì›í˜•ì´ ì•„ë‹Œ ê²½ìš°ë‚˜ êµ°ì§‘ í¬ê¸°ê°€ ë‹¤ë¥¸ ê²½ìš°, ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì´ëŸ¬í•œ ë‹¨ì ì—ë„ ë¶ˆêµ¬í•˜ê³  K-means ì•Œê³ ë¦¬ì¦˜ì€ êµ°ì§‘í™”ì— ì‚¬ìš©ë˜ëŠ” ëŒ€í‘œì ì¸ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ë„ë¦¬ ì ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\nlibrary(cluster)\n\niris_data &lt;- iris[, -5]\n\n# Apply K-means (K=3)\nset.seed(42)\nkmeans_result &lt;- kmeans(iris_data, centers = 3, nstart = 20)\n\ntable(iris$Species, kmeans_result$cluster)\nclusplot(iris_data, kmeans_result$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)\n              1  2  3\n  setosa     50  0  0\n  versicolor  0 48  2\n  virginica   0 14 36"
  },
  {
    "objectID": "posts/kelly-criterion/index.html",
    "href": "posts/kelly-criterion/index.html",
    "title": "ì¼ˆë¦¬ ê¸°ì¤€",
    "section": "",
    "text": "ì¼ˆë¦¬ ê¸°ì¤€ì´ë€..\nì¼ˆë¦¬ ê¸°ì¤€(Kelly Criterion)ì€ íˆ¬ìì™€ ë„ë°•ì—ì„œ ìµœì ì˜ ë°°ë‹¹ì„ ê²°ì •í•˜ê¸° ìœ„í•œ ê³µì‹ì…ë‹ˆë‹¤. ì´ ê³µì‹ì€ 1956ë…„ì— John Larry Kelly Jr.ì— ì˜í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ì¼ˆë¦¬ ê¸°ì¤€ì˜ ëª©í‘œëŠ” ì¥ê¸°ì ì¸ ìë³¸ ê°€ì¹˜ ì¦ê°€ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°°ë‹¹ê¸ˆì„ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤.\nì¼ˆë¦¬ ê³µì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\\[ f* = \\frac{p}{a} - \\frac{q}{b} \\]\nì—¬ê¸°ì„œ:\n\\(f*\\)ëŠ” ìµœì ì˜ ë°°ë‹¹ ë¹„ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n\\(a\\)ëŠ” ìˆœì†í•´ë¥ ì…ë‹ˆë‹¤.\n\\(b\\)ëŠ” ìˆœì´ìµë¥ ì…ë‹ˆë‹¤.\n\\(p\\)ëŠ” ì´ê¸¸ í™•ë¥ ì…ë‹ˆë‹¤.\n\\(q\\)ëŠ” ì§ˆ í™•ë¥ ì´ë©°, \\((1 - p)\\)ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì¼ˆë¦¬ ê¸°ì¤€ì€ íˆ¬ììê°€ ì–¼ë§ˆë‚˜ ë§ì€ ëˆì„ íˆ¬ìí•˜ê±°ë‚˜ ì–¼ë§ˆë‚˜ ë§ì€ ëˆì„ ë°°íŒ…í•´ì•¼ í•˜ëŠ”ì§€ ê²°ì •í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ì´ ê³µì‹ì„ ì‚¬ìš©í•˜ë©´ ì¥ê¸°ì ìœ¼ë¡œ ìˆ˜ìµì„ ê·¹ëŒ€í™”í•˜ê³  ë¦¬ìŠ¤í¬ë¥¼ ìµœì†Œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nì¼ˆë¦¬ ê¸°ì¤€ ìœ ë„\n\\[ r = (1+fb)^p \\times (1-fa)^q \\]\nìœ„ì˜ ì‹ì— ë¡œê·¸ë¥¼ ì·¨í•˜ë©´,\n\\[ logr = p log(1+fb) + q log(1 - fa) \\]\nìœ„ì˜ ì‹ì„ fì— ëŒ€í•´ ë¯¸ë¶„í•˜ë©´,\n\\[ 0 = \\frac{pb}{(1+fb)} - \\frac{qa}{(1-fa)}\\]\n\\[ pb \\cdot (1-fa) = qa \\cdot (1+fb) \\]\nì´ë¥¼ ì •ë¦¬í•˜ë©´, í•˜ê¸°ì™€ ê°™ì´ ì¼ˆë¦¬ ê¸°ì¤€ì‹ì´ ë‚˜ì˜µë‹ˆë‹¤.\n\\[ f* = \\frac{p}{a} - \\frac{q}{b} \\]\n\n\nì˜ˆì‹œ\nìŠ¹ë¥ ì´ 60%ì¸ ê²Œì„ì—ì„œ ì´ê¸°ë©´ ë‚´ê¸° ê¸ˆì•¡ë§Œí¼ ë²Œê³ , ì§€ë©´ ë‚´ê¸° ê¸ˆì•¡ ëª¨ë‘ë¥¼ ìƒëŠ” ê²Œì„ì„ ê°€ì •í•´ë´…ì‹œë‹¤.\nì¼ˆë¦¬ ê¸°ì¤€ì— ë”°ë¥´ë©´, ì „ì²´ ê¸ˆì•¡ì˜ 20%ì„ íˆ¬ì í•˜ì˜€ì„ ë•Œ ìˆ˜ìµì„ ê·¹ëŒ€í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nê·¸ë ‡ë‹¤ë©´, 1000ì›ì˜ ì´ˆê¸°ìê¸ˆìœ¼ë¡œ 1000ë²ˆ ê²Œì„í–ˆì„ ë•Œ, ì „ì²´ ê¸ˆì•¡ì˜ 10% ~ 100%ê¹Œì§€ íˆ¬ìí•˜ì˜€ì„ ê²½ìš°ì˜ ê¸°ëŒ€ ìˆ˜ìµì„ ì‹œë®¬ë ˆì´ì…˜í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\nset.seed(42)\n\nnum_games &lt;- 1000\nbet_ratio &lt;- seq(0.1,1,by = 0.1)\nresult &lt;- numeric(10)\n\nfor(r in 1:10){\nratio &lt;- bet_ratio[r]\nmoney &lt;- 1000\nfor(i in 1:num_games){\n    random_number &lt;- runif(1)\n    if(random_number &lt;0.6) {\n        money &lt;- money + (money*ratio)\n    } else {\n        money &lt;- money - (money*ratio)\n    }\n}\n\nresult[r] &lt;- money\n}\n\nprint(result)\n\n [1]  3.444046e+11  3.703831e+11  4.379182e+05  3.782560e-06  1.354241e-20\n [6]  5.060881e-32  7.664663e-65 8.607885e-111 6.451605e-228  0.000000e+00\n\n\nì‹œë®¬ë ˆì´ì…˜ì—ì„œë„ ë³¼ ìˆ˜ ìˆë“¯ì´, 20%ì˜ ë¹„ìœ¨ë¡œ ë°°íŒ…í•˜ì˜€ì„ ë•Œ ê°€ì¥ í° ìˆ˜ìµì„ ì–»ì„ ìˆ˜ ìˆìŒì„ ì•Œìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/kl-divergence/index.html",
    "href": "posts/kl-divergence/index.html",
    "title": "Kullback-Leibler Divergence ì‰½ê²Œ ì´í•´í•˜ê¸°",
    "section": "",
    "text": "ë³¸ ë¬¸ì„œë¥¼ ì½ê¸° ì•ì„œ, Entropyì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë¨¼ì € ì½ê³  ì˜¤ì‹œê¸¸ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤.\n\nKL Divergenceë€..\nKL Divergence(Kullback-Leibler Divergence)ëŠ” ì‹¤ì œ ëª¨ë¸ê³¼ ì˜ˆì¸¡ ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\nKL DivergenceëŠ” ë˜í•œ ì •ë³´ ì´ë¡ ì—ì„œ ìƒëŒ€ ì—”íŠ¸ë¡œí”¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.\n\n\nKL Divergenceì˜ ê°„ë‹¨í•œ ì˜ˆì‹œ\nìˆ˜ì‹ì„ ì‚´í´ë³´ê¸° ì•ì„œ, ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ë‘ í™˜ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•´ë³´ê² ìŠµë‹ˆë‹¤.\nì´ì „ Entropyë¬¸ì„œë¥¼ ì½ìœ¼ì…¨ë‹¤ë©´, Entropyì˜ ê³µì‹ì´ í•˜ê¸°ì™€ ê°™ìŒì„ ê¸°ì–µí•˜ì‹¤ ê²ƒì…ë‹ˆë‹¤. ë˜í•œ Entropyì˜ ì˜ë¯¸ê°€ ë†€ë¼ì›€ì˜ í‰ê· ì¸ ê²ƒë„ í•¨ê»˜ ê¸°ì–µí•˜ì‹¤ ê²ƒì…ë‹ˆë‹¤.\n\\[\n\\sum log(\\frac{1}{p(x)}) \\cdot p(x)\n\\]\ní•œë²ˆ 6ë©´ì²´ ì£¼ì‚¬ìœ„ë¥¼ ìƒê°í•´ë´…ì‹œë‹¤.\n6ë©´ì²´ ì£¼ì‚¬ìœ„ì˜ ì‹¤ì œ EntropyëŠ” \\(6 \\times log(\\frac{1}{1/6}) \\cdot 1/6 \\simeq 1.792\\) ì…ë‹ˆë‹¤.\ní•˜ì§€ë§Œ ì œê°€ ì‹¤í—˜ì„ ì˜ëª» ì„¤ê³„í•´ì„œ ì´ ì£¼ì‚¬ìœ„ì˜ í™•ë¥  ê°’ì„ ì˜ëª» ê³„ì‚°í–ˆë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤.\n1 ~ 6 ê¹Œì§€ì˜ í™•ë¥ ì€ (0.1,0.2,0.1,0.2,0.2,0.2) ì…ë‹ˆë‹¤. ì´ë¥¼ q(x)ë¼ê³  í•˜ê² ìŠµë‹ˆë‹¤.\nê·¸ë ‡ë‹¤ë©´, ì œê°€ ì˜ˆì¸¡í•œ ë†€ë¼ì›€ê³¼ ì‹¤ì œ í™•ë¥ ì„ í†µí•´ Entropyì˜ ê³„ì‚° ì—­ì‹œ ê°€ëŠ¥í•  ê²ƒì…ë‹ˆë‹¤. \\(\\sum log(\\frac{1}{q(x)}) \\cdot 1/6 \\simeq 1.84\\)\nì´ ë‘ ê°’ì˜ ì°¨, 0.048ì´ ë‘ ë¶„í¬ì˜ ì°¨, KL Divergenceì˜ ê°’ì…ë‹ˆë‹¤. ì´ì œ KL Divergenceì˜ ìˆ˜ì‹ì„ ë³´ë„ë¡ í•©ì‹œë‹¤.\n\n\nKL Divergence ìˆ˜ì‹\nìœ„í‚¤í”¼ë””ì•„ì˜ ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°ì˜ ê¸€ì— ë”°ë¥´ë©´ ì´ì‚° í™•ë¥ ì—ì„œì˜ ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°ì€ í•˜ê¸°ì™€ ê°™ìŠµë‹ˆë‹¤:\n\\[D_{KL}(P||Q) = \\sum_i p(i) log \\frac{P(i)}{Q(i)} \\]\nìš°ë¦¬ê°€ ìœ„ì—ì„œ ê³„ì‚°í–ˆë˜ ì‹ì„ ì¼ë°˜í™” í•œë‹¤ë©´, \\[ \\sum log(\\frac{1}{q(x)}) \\cdot p(x) - \\sum log(\\frac{1}{p(x)}) \\cdot p(x) \\]\np(x)ë¡œ ë¬¶ëŠ”ë‹¤ë©´,\n\\[ \\sum p(x)  (log(\\frac{1}{q(x)}) -  log(\\frac{1}{p(x)})) \\]\nlogë¥¼ ì •ë¦¬í•˜ë©´, ìš°ë¦¬ê°€ ì›í–ˆë˜ ì‹ì´ ë‚˜ì˜µë‹ˆë‹¤.\n\\[ \\sum p(x) log(\\frac{p(x)}{q(x)}) \\]"
  },
  {
    "objectID": "posts/knapsack/index.html",
    "href": "posts/knapsack/index.html",
    "title": "ë°°ë‚­ ë¬¸ì œ",
    "section": "",
    "text": "ê°œìš”\nortoolsë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ë‚­ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì˜ˆì œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\nì¼ë°˜ì ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ìƒìë¥¼ ë°°ì†¡ íŠ¸ëŸ­ì— íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\n\n\në¬¸ì œ ì„¤ì •\n850ì˜ ë¬´ê²Œë¥¼ ë‹´ì„ ìˆ˜ ìˆëŠ” ì»¨í…Œì´ë„ˆì™€ 50ê°€ì§€ í•­ëª©ì˜ ì•„ì´í…œë“¤ì´ ìˆìŠµë‹ˆë‹¤.\nì•„ì´í…œì€ ê°’ê³¼ ë¬´ê²Œë¡œ ì´ë£¨ì–´ì ¸ìˆìŠµë‹ˆë‹¤.\nvalues = [\n    360, 83, 59, 130, 431, 67, 230, 52, 93, 125, 670, 892, 600, 38, 48, 147,\n    78, 256, 63, 17, 120, 164, 432, 35, 92, 110, 22, 42, 50, 323, 514, 28,\n    87, 73, 78, 15, 26, 78, 210, 36, 85, 189, 274, 43, 33, 10, 19, 389, 276,\n    312\n]\nweights = [[\n    7, 0, 30, 22, 80, 94, 11, 81, 70, 64, 59, 18, 0, 36, 3, 8, 15, 42, 9, 0,\n    42, 47, 52, 32, 26, 48, 55, 6, 29, 84, 2, 4, 18, 56, 7, 29, 93, 44, 71,\n    3, 86, 66, 31, 65, 0, 79, 20, 65, 52, 13\n]]\ncapacities = [850]\n\n\në¬¸ì œ í•´ê²°\nfrom ortools.algorithms import pywrapknapsack_solver\n\nsolver = pywrapknapsack_solver.KnapsackSolver(\n    pywrapknapsack_solver.KnapsackSolver.\n    KNAPSACK_MULTIDIMENSION_BRANCH_AND_BOUND_SOLVER, 'KnapsackExample')\n\nsolver.Init(values, weights, capacities)\ncomputed_value = solver.Solve()\npacked_items = []\npacked_weights = []\ntotal_weight = 0\nprint('Total value =', computed_value)\nfor i in range(len(values)):\n    if solver.BestSolutionContains(i):\n        packed_items.append(i)\n        packed_weights.append(weights[0][i])\n        total_weight += weights[0][i]\n\nprint('Total weight:', total_weight)\nprint('Packed items:', packed_items)\nprint('Packed_weights:', packed_weights)\nTotal value = 7534\nTotal weight: 850\nPacked items: [0, 1, 3, 4, 6, 10, 11, 12, 14, 15, 16, 17, 18, 19, 21, 22, 24, 27, 28, 29, 30, 31,\n               32, 34, 38, 39, 41, 42, 44, 47, 48, 49]\nPacked_weights: [7, 0, 22, 80, 11, 59, 18, 0, 3, 8, 15, 42, 9, 0, 47, 52, 26, 6, 29, 84, 2, 4,\n                 18, 7, 71, 3, 66, 31, 0, 65, 52, 13]"
  },
  {
    "objectID": "posts/knapsack-n-bags/index.html",
    "href": "posts/knapsack-n-bags/index.html",
    "title": "ë°°ë‚­ ë¬¸ì œ (ì—¬ëŸ¬ ë°°ë‚­)",
    "section": "",
    "text": "ê°œìš”\nortoolsë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ ë°°ë‚­ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì˜ˆì œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n\n\në¬¸ì œ ì„¤ì •\ndata = {}\ndata['weights'] = [\n    48, 30, 42, 36, 36, 48, 42, 42, 36, 24, 30, 30, 42, 36, 36\n]\ndata['values'] = [\n    10, 30, 25, 50, 35, 30, 15, 40, 30, 35, 45, 10, 20, 30, 25\n]\nassert len(data['weights']) == len(data['values'])\ndata['num_items'] = len(data['weights'])\ndata['all_items'] = range(data['num_items'])\n\ndata['bin_capacities'] = [100, 100, 100, 100, 100]\ndata['num_bins'] = len(data['bin_capacities'])\ndata['all_bins'] = range(data['num_bins'])\n\n\në¬¸ì œ í•´ê²°\nsolver = pywraplp.Solver.CreateSolver('SCIP')\n\n# x[i, b] = 1 if item i is packed in bin b.\nx = {}\nfor i in data['all_items']:\n    for b in data['all_bins']:\n        x[i, b] = solver.BoolVar(f'x_{i}_{b}')\n        \n# Each item is assigned to at most one bin.\nfor i in data['all_items']:\n    solver.Add(sum(x[i, b] for b in data['all_bins']) &lt;= 1)\n\n# The amount packed in each bin cannot exceed its capacity.\nfor b in data['all_bins']:\n    solver.Add(\n        sum(x[i, b] * data['weights'][i]\n            for i in data['all_items']) &lt;= data['bin_capacities'][b])\n            \n# Maximize total value of packed items.\nobjective = solver.Objective()\nfor i in data['all_items']:\n    for b in data['all_bins']:\n        objective.SetCoefficient(x[i, b], data['values'][i])\nobjective.SetMaximization()\n\nstatus = solver.Solve()\nif status == pywraplp.Solver.OPTIMAL:\n    print(f'Total packed value: {objective.Value()}')\n    total_weight = 0\n    for b in data['all_bins']:\n        print(f'Bin {b}')\n        bin_weight = 0\n        bin_value = 0\n        for i in data['all_items']:\n            if x[i, b].solution_value() &gt; 0:\n                print(\n                    f\"Item {i} weight: {data['weights'][i]} value: {data['values'][i]}\"\n                )\n                bin_weight += data['weights'][i]\n                bin_value += data['values'][i]\n        print(f'Packed bin weight: {bin_weight}')\n        print(f'Packed bin value: {bin_value}\\n')\n        total_weight += bin_weight\n    print(f'Total packed weight: {total_weight}')\nelse:\n    print('The problem does not have an optimal solution.')\nTotal packed value: 395.0\nBin 0\nItem 4 weight: 36 value: 35\nItem 9 weight: 24 value: 35\nItem 14 weight: 36 value: 25\nPacked bin weight: 96\nPacked bin value: 95\n\nBin 1\nItem 1 weight: 30 value: 30\nItem 8 weight: 36 value: 30\nItem 10 weight: 30 value: 45\nPacked bin weight: 96\nPacked bin value: 105\n\nBin 2\nItem 2 weight: 42 value: 25\nItem 5 weight: 48 value: 30\nPacked bin weight: 90\nPacked bin value: 55\n\nBin 3\nItem 7 weight: 42 value: 40\nItem 13 weight: 36 value: 30\nPacked bin weight: 78\nPacked bin value: 70\n\nBin 4\nItem 3 weight: 36 value: 50\nItem 12 weight: 42 value: 20\nPacked bin weight: 78\nPacked bin value: 70\n\nTotal packed weight: 438"
  },
  {
    "objectID": "posts/linear-optimization/index.html",
    "href": "posts/linear-optimization/index.html",
    "title": "ìƒì‚°ê³„íš",
    "section": "",
    "text": "ê°œìš”\nortoolsë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ í˜• í”„ë¡œê·¸ë˜ë° ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì˜ˆì œë¡œ ìƒì‚° ê³„íš ìµœì í™” ë¬¸ì œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\nì´ ì˜ˆì œì—ì„œëŠ” ë‘ ê°€ì§€ ì œí’ˆì„ ìƒì‚°í•˜ëŠ” ê³µì¥ì´ ìˆìœ¼ë©°, ê° ì œí’ˆì—ëŠ” ì¼ì •í•œ ì´ìµì´ ìˆìŠµë‹ˆë‹¤.\nê³µì¥ì€ ì œí•œëœ ìì›ì„ ì‚¬ìš©í•˜ì—¬ ì´ìµì„ ìµœëŒ€í™”í•˜ë ¤ê³  í•©ë‹ˆë‹¤.\n\n\në¬¸ì œ ì„¤ì •\n\nì œí’ˆ Aì™€ ì œí’ˆ Bë¥¼ ìƒì‚°í•  ìˆ˜ ìˆìœ¼ë©°, ê°ê° $20, $40ì˜ ì´ìµì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì œí’ˆ AëŠ” ìì› 1ì„ 3 ë‹¨ìœ„, ìì› 2ë¥¼ 2 ë‹¨ìœ„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\nì œí’ˆ BëŠ” ìì› 1ì„ 2 ë‹¨ìœ„, ìì› 2ë¥¼ 5 ë‹¨ìœ„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\nê³µì¥ì€ ìì› 1ì„ 30 ë‹¨ìœ„, ìì› 2ë¥¼ 40 ë‹¨ìœ„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nëª©í‘œ: ì´ìµì„ ìµœëŒ€í™”í•˜ë©´ì„œ ì œí•œëœ ìì›ì„ ì‚¬ìš©í•˜ì—¬ ì œí’ˆ Aì™€ Bì˜ ìµœì  ìƒì‚°ëŸ‰ì„ ì°¾ìœ¼ì„¸ìš”.\n\n\në¬¸ì œ í•´ê²°\nfrom ortools.linear_solver import pywraplp\n\n# ì„ í˜• í”„ë¡œê·¸ë˜ë° ì†”ë²„ ìƒì„±\nsolver = pywraplp.Solver.CreateSolver('GLOP')\n\n# ë³€ìˆ˜ ìƒì„± (ì •ìˆ˜í˜•ì˜ ê²½ìš° IntVar)\nproduct_a = solver.NumVar(0, solver.infinity(), 'Product_A')\nproduct_b = solver.NumVar(0, solver.infinity(), 'Product_B')\n\n\n# ì œì•½ ì¡°ê±´ ì¶”ê°€\nconstraint1 = solver.Constraint(0, 30, 'Resource_1')\nconstraint1.SetCoefficient(product_a, 3)\nconstraint1.SetCoefficient(product_b, 2)\n\nconstraint2 = solver.Constraint(0, 40, 'Resource_2')\nconstraint2.SetCoefficient(product_a, 2)\nconstraint2.SetCoefficient(product_b, 5)\n\n# ëª©ì  í•¨ìˆ˜ ì •ì˜ ë° ìµœëŒ€í™”\nobjective = solver.Objective()\nobjective.SetCoefficient(product_a, 20)\nobjective.SetCoefficient(product_b, 40)\nobjective.SetMaximization()\n\n# ë¬¸ì œ í•´ê²°\nstatus = solver.Solve()\n\n# ê²°ê³¼ ì¶œë ¥\nif status == pywraplp.Solver.OPTIMAL:\n    print('Objective value =', objective.Value())\n    print('Product A =', product_a.solution_value())\n    print('Product B =', product_b.solution_value())\nelse:\n    print('The problem does not have an optimal solution.')\nObjective value = 345.4545454545455\nProduct A = 6.363636363636365\nProduct B = 5.454545454545454"
  },
  {
    "objectID": "posts/mit-6s191/index.html",
    "href": "posts/mit-6s191/index.html",
    "title": "2023 MIT 6.S191 Deep Learning",
    "section": "",
    "text": "2023ë…„ MITì—ì„œ ê°•ì˜ëœ 6.S191 ê°•ì¢Œì˜ Labì˜ì´í•´ë¥¼ ë•ê¸°ìœ„í•´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\nLabì— ëŒ€í•´ì„œëŠ” ì´ê³³ì—ì„œ í™•ì¸ê°€ëŠ¥í•˜ë©°, í•´ë‹¹ ê°•ì¢ŒëŠ” ì—¬ê¸°ì„œ í™•ì¸ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\nLab1-Part1 Intro to Tensorflow\nLab1-Part2 Music Generation\nLab2-Part1 MNIST\nLab2-Part2 Face Detection\nLab3-Part1 Introduction Caspa\nLab3-Part2 Bias and Uncertainty"
  },
  {
    "objectID": "posts/mnist/index.html",
    "href": "posts/mnist/index.html",
    "title": "2023 MIT 6.S191 03 MNIST",
    "section": "",
    "text": "2023ë…„ MITì—ì„œ ê°•ì˜ëœ 6.S191 ê°•ì¢Œì˜ Labì˜ì´í•´ë¥¼ ë•ê¸°ìœ„í•´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\nLabì— ëŒ€í•´ì„œëŠ” ì´ê³³ì—ì„œ í™•ì¸ê°€ëŠ¥í•˜ë©°, í•´ë‹¹ ê°•ì¢ŒëŠ” ì—¬ê¸°ì„œ í™•ì¸ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\nLab1-Part1 Intro to Tensorflow\nLab1-Part2 Music Generation\nLab2-Part1 MNIST\nLab2-Part2 Face Detection\nLab3-Part1 Introduction Caspa\nLab3-Part2 Bias and Uncertainty\n\nì´ë²ˆ ê³¼ì œì—ì„œëŠ” CNN ëª¨ë¸ì„ í†µí•´ ì†ìœ¼ë¡œ ì“°ì—¬ì§„ 0 ~ 9 ì˜ ê¸€ì”¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨í˜•ì„ ë§Œë“­ë‹ˆë‹¤.\n60,000 ê°œì˜ train ì´ë¯¸ì§€ì™€ 10,000 ê°œì˜ test ì´ë¯¸ì§€ë¡œ ì´ë£¨ì–´ì§„ MNIST ë°ì´í„° ì…‹ì„ í™œìš©í•  ê²ƒì…ë‹ˆë‹¤.\n# Import Tensorflow 2.0\n%tensorflow_version 2.x\nimport tensorflow as tf \n\n!pip install mitdeeplearning\nimport mitdeeplearning as mdl\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom tqdm import tqdm\n\n# Check that we are using a GPU, if not switch runtimes\n#   using Runtime &gt; Change Runtime Type &gt; GPU\nassert len(tf.config.list_physical_devices('GPU')) &gt; 0\n\n3.1 MNIST ë°ì´í„°\në°ì´í„°ë¥¼ ë‹¤ìš´ë°›ê³ , ì–´ë–»ê²Œ ì´ë£¨ì–´ì ¸ìˆëŠ”ì§€ í™•ì¸í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\nmnist = tf.keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n# convert dim from (n_imgs x 28 x 28) to (n_imgs x 28 x 28 x 1) to make channel\ntrain_images = (np.expand_dims(train_images, axis=-1)/255.).astype(np.float32)\ntrain_labels = (train_labels).astype(np.int64)\ntest_images = (np.expand_dims(test_images, axis=-1)/255.).astype(np.float32)\ntest_labels = (test_labels).astype(np.int64)\nplt.figure(figsize=(10,10))\nrandom_inds = np.random.choice(60000,36)\nfor i in range(36):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image_ind = random_inds[i]\n    plt.imshow(np.squeeze(train_images[image_ind]), cmap=plt.cm.binary)\n    plt.xlabel(train_labels[image_ind])\n\n\n\n3.2 MLP êµ¬ì¡°ì˜ ëª¨ë¸\nCNN ëª¨ë¸ì„ ë§Œë“¤ê¸° ì•ì„œ MLP êµ¬ì¡°ì˜ ëª¨ë¸ì„ ë¨¼ì € ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤.\ní•˜ê¸°ì˜ ì´ë¯¸ì§€ì™€ ê°™ì´ ì´ë¯¸ì§€ëŠ” ì´ 784 (28 x 28)ì˜ í”½ì…€ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\nì´ 784ê°œì˜ inputì„ 128ê°œì˜ hidden layerë¡œ í†µê³¼ì‹œí‚¨ ë’¤ 10ê°œ (0 ~ 9)ì˜ output layerë¡œ ì¶œë ¥í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\ndef build_fc_model():\n  fc_model = tf.keras.Sequential([\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(128, activation = 'relu'),\n      tf.keras.layers.Dense(10, activation = 'softmax')\n      \n  ])\n  return fc_model\n\nmodel = build_fc_model()\n\n# configure loss and opimizer\nmodel.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-1), \n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n\nBATCH_SIZE = 64\nEPOCHS = 5\n\n# train model\nmodel.fit(train_images, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS)           \nEpoch 1/5\n938/938 [==============================] - 8s 3ms/step - loss: 0.3753 - accuracy: 0.8948\nEpoch 2/5\n938/938 [==============================] - 3s 3ms/step - loss: 0.1977 - accuracy: 0.9432\nEpoch 3/5\n938/938 [==============================] - 3s 3ms/step - loss: 0.1493 - accuracy: 0.9573\nEpoch 4/5\n938/938 [==============================] - 3s 3ms/step - loss: 0.1200 - accuracy: 0.9654\nEpoch 5/5\n938/938 [==============================] - 3s 3ms/step - loss: 0.1012 - accuracy: 0.9715\n&lt;keras.callbacks.History at 0x7f5b20e0ae60&gt;\nëŒ€ëµ train ë°ì´í„°ì—ì„œ 97% ì •ë„ì˜ ì •í™•ë„ë¥¼ ë³´ì´ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ì œ test ë°ì´í„°ì—ì„œì˜ ê²°ê³¼ë¥¼ í™•ì¸í•´ë´…ì‹œë‹¤.\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('Test accuracy:', test_acc)\n313/313 [==============================] - 1s 3ms/step - loss: 0.1048 - accuracy: 0.9696\nTest accuracy: 0.9696000218391418\ntest ë°ì´í„°ì—ì„œì˜ ê²°ê³¼ê°€ ì•½ê°„ ë‚®ìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ train ë°ì´í„°ì™€ test ë°ì´í„°ì—ì„œ ì°¨ì´ê°€ ë‚  ë•Œ, overfittingë˜ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.\nì´ì œ CNN ëª¨ë¸ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤.\n\n\n3.3 CNN êµ¬ì¡°ì˜ ëª¨ë¸\ní•˜ê¸°ì™€ ê°™ì€ êµ¬ì¡°ì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤.\n\ndef build_cnn_model():\n    cnn_model = tf.keras.Sequential([\n\n        tf.keras.layers.Conv2D(32, (3, 3), padding = 'same', activation = tf.nn.relu), \n        tf.keras.layers.MaxPool2D(pool_size = (2, 2)),\n\n        tf.keras.layers.Conv2D(64, (3, 3), padding = 'same', activation = tf.nn.relu),\n        tf.keras.layers.MaxPool2D(pool_size = (2, 2)),\n\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n\n        # TODO: Define the last Dense layer to output the classification \n        # probabilities. Pay attention to the activation needed a probability\n        # output\n        tf.keras.layers.Dense(10, activation = tf.nn.softmax)\n    ])\n    \n    return cnn_model\n  \ncnn_model = build_cnn_model()\n# Initialize the model by passing some data through\ncnn_model.predict(train_images[[0]])\n# Print the summary of the layers in the model.\nprint(cnn_model.summary())\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 28, 28, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n 2D)                                                             \n                                                                 \n flatten_1 (Flatten)         (None, 3136)              0         \n                                                                 \n dense_2 (Dense)             (None, 128)               401536    \n                                                                 \n dense_3 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 421,642\nTrainable params: 421,642\nNon-trainable params: 0\n_________________________________________________________________\ncnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\ncnn_model.fit(train_images, train_labels, batch_size = BATCH_SIZE, epochs = EPOCHS)\nEpoch 1/5\n938/938 [==============================] - 8s 5ms/step - loss: 0.1493 - accuracy: 0.9546\nEpoch 2/5\n938/938 [==============================] - 5s 5ms/step - loss: 0.0447 - accuracy: 0.9860\nEpoch 3/5\n938/938 [==============================] - 7s 8ms/step - loss: 0.0297 - accuracy: 0.9905\nEpoch 4/5\n938/938 [==============================] - 5s 5ms/step - loss: 0.0217 - accuracy: 0.9930\nEpoch 5/5\n938/938 [==============================] - 4s 4ms/step - loss: 0.0165 - accuracy: 0.9948\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\n\nprint('Test accuracy:', test_acc)\n313/313 [==============================] - 1s 2ms/step - loss: 0.1048 - accuracy: 0.9696\nTest accuracy: 0.9696000218391418\nì´ì œ CNN ëª¨ë¸ì„ í†µí•´ ì˜ˆì¸¡ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.\npredictions = cnn_model.predict(test_images)\npredictions[0]\narray([4.6900855e-10, 5.6617080e-09, 3.1779661e-09, 3.1454579e-08,\n       4.5877988e-13, 7.0875771e-11, 3.0195982e-18, 9.9999988e-01,\n       5.5570687e-11, 1.1235415e-07], dtype=float32)\n10ê°œì˜ output layerë¡œ ëª¨ë¸ì„ ì •ì˜í•˜ì˜€ê¸°ì—, 10ê°œì˜ ìš”ì†Œë¥¼ í¬í•¨í•œ arrayê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\nsoftmaxëŠ” ì—¬ëŸ¬ê°œì˜ ì¶œë ¥ê°’ì„ í™•ë¥ ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì–´ ì£¼ê¸°ì—, np.argmaxë¥¼ í†µí•˜ì—¬ ì–´ë–¤ ê°’ì˜ í™•ë¥ ì´ ê°€ì¥ í°ì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nprediction = np.argmax(predictions[0])\n\nprint(prediction)\n7\nì •ë§ë¡œ 7ì´ ë§ëŠ”ì§€ í™•ì¸í•´ ë´…ì‹œë‹¤.\nprint(\"Label of this digit is:\", test_labels[0])\nplt.imshow(test_images[0,:,:,0], cmap=plt.cm.binary)\n\nì¢€ ë” ë‹¤ì–‘í•œ ê²°ê³¼ë“¤ì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\n\n\n1.4 GradientTapeë¥¼ ì‚¬ìš©í•œ CNN\n1.3ì—ì„œì˜ ì˜ˆì‹œì™€ ê°™ì´ ëª¨ë¸ì˜ optimizerì™€ lossë¥¼ ì‚¬ìš©í•  ê²½ìš° ë§¤ìš° í¸ë¦¬í•˜ê³  ì§ê´€ì ì´ì§€ë§Œ, ëª¨ë¸ì— ëŒ€í•œ í†µì œë ¥ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.\nì´ì— ëŒ€í•œ ëŒ€ì•ˆìœ¼ë¡œ tf.GradientTape.gradientë¥¼ í™œìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.\n### ì´ì „ì˜ ë°©ì‹\ncnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\ncnn_model.fit(train_images, train_labels, batch_size = BATCH_SIZE, epochs = EPOCHS)\n### ìƒˆë¡œìš´ ë°©ì‹\ncnn_model = build_cnn_model()\n\nbatch_size = 12\nloss_history = mdl.util.LossHistory(smoothing_factor=0.95) # to record the evolution of the loss\nplotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss', scale='semilogy')\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-2) # define our optimizer\n\nif hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n\nfor idx in tqdm(range(0, train_images.shape[0], batch_size)):\n  # First grab a batch of training data and convert the input images to tensors\n  (images, labels) = (train_images[idx:idx+batch_size], train_labels[idx:idx+batch_size])\n  images = tf.convert_to_tensor(images, dtype=tf.float32)\n\n  # GradientTape to record differentiation operations\n  with tf.GradientTape() as tape:\n    logits = cnn_model(images)\n    loss_value = tf.keras.backend.sparse_categorical_crossentropy(labels, logits)\n\n  loss_history.append(loss_value.numpy().mean()) # append the loss to the loss_history record\n  plotter.plot(loss_history.get())\n\n  # Backpropagation\n  grads = tape.gradient(loss_value, cnn_model.trainable_variables)\n  optimizer.apply_gradients(zip(grads, cnn_model.trainable_variables))"
  },
  {
    "objectID": "posts/music-generation/index.html",
    "href": "posts/music-generation/index.html",
    "title": "2023 MIT 6.S191 02 Music Generation with RNNs",
    "section": "",
    "text": "2023ë…„ MITì—ì„œ ê°•ì˜ëœ 6.S191 ê°•ì¢Œì˜ Labì˜ì´í•´ë¥¼ ë•ê¸°ìœ„í•´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\nLabì— ëŒ€í•´ì„œëŠ” ì´ê³³ì—ì„œ í™•ì¸ê°€ëŠ¥í•˜ë©°, í•´ë‹¹ ê°•ì¢ŒëŠ” ì—¬ê¸°ì„œ í™•ì¸ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\nLab1-Part1 Intro to Tensorflow\nLab1-Part2 Music Generation\nLab2-Part1 MNIST\nLab2-Part2 Face Detection\nLab3-Part1 Introduction Caspa\nLab3-Part2 Bias and Uncertainty\n\n\n2.1 Dependencies\nìš°ì„ , ë³¸ ê³¼ì œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤\n# Import Tensorflow 2.0\n%tensorflow_version 2.x\nimport tensorflow as tf \n\n# Download and import the MIT Introduction to Deep Learning package\n!pip install mitdeeplearning\nimport mitdeeplearning as mdl\n\n# Import all remaining packages\nimport numpy as np\nimport os\nimport time\nimport functools\nfrom IPython import display as ipythondisplay\nfrom tqdm import tqdm\n!apt-get install abcmidi timidity &gt; /dev/null 2&gt;&1\n\n# Check that we are using a GPU, if not switch runtimes\n#   using Runtime &gt; Change Runtime Type &gt; GPU\nassert len(tf.config.list_physical_devices('GPU')) &gt; 0\n\n\n2.2 ë°ì´í„°\nABC ê¸°ë³´ë²•ìœ¼ë¡œ êµ¬ì„±ëœ ì•„ì¼ëœë“œ í¬í¬ ìŒì•…ì˜ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ë³¸ ê³¼ì œë¥¼ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.\n# Download the dataset\nsongs = mdl.lab1.load_training_data()\n\n# Print one of the songs to inspect it in greater detail!\nexample_song = songs[0]\nprint(\"\\nExample song: \")\nprint(example_song)\nFound 817 songs in text\n\nExample song: \nX:1\nT:Alexander's\nZ: id:dc-hornpipe-1\nM:C|\nL:1/8\nK:D Major\n(3ABc|dAFA DFAd|fdcd FAdf|gfge fefd|(3efe (3dcB A2 (3ABc|!\ndAFA DFAd|fdcd FAdf|gfge fefd|(3efe dc d2:|!\nAG|FAdA FAdA|GBdB GBdB|Acec Acec|dfaf gecA|!\nFAdA FAdA|GBdB GBdB|Aceg fefd|(3efe dc d2:|!\nABC ê¸°ë³´ë²•ì„ ìŒì•…íŒŒì¼ë¡œ ë³€í™˜ í›„ ì‹¤í–‰ì‹œì¼œ ë´…ì‹œë‹¤.\nmdl.lab1.play_song(example_song)\n\n\n\ní•œê°€ì§€ ìœ ë…í•´ì•¼í•  ë¶€ë¶„ì€ ì´ ë°ì´í„°ê°€ ë‹¨ìˆœíˆ ìŒì•…ì˜ ë…¸íŠ¸ë§Œì„ í¬í•¨í•œ ê²ƒì´ ì•„ë‹ˆë¼ ë…¸ë˜ ì œëª©, í‚¤, í…œí¬ê¹Œì§€ í¬í•¨í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë‹¤ì–‘í•œ ë¬¸ìì—´ë“¤ì´ í•™ìŠµì— ìˆì–´ì„œ ì–´ë– í•œ ì˜í–¥ì„ ë¯¸ì¹ ê¹Œìš”? ì´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ í‘œí˜„í•˜ê¸° ìœ„í•œ ê¸°ë²•ë„ ì´ë²ˆ ê³¼ì œì—ì„œ í•¨ê»˜ ì•Œì•„ë´…ì‹œë‹¤.\n# Join our list of song strings into a single string containing all songs\nsongs_joined = \"\\n\\n\".join(songs) \n\n# Find all unique characters in the joined string\nvocab = sorted(set(songs_joined))\nprint(\"There are\", len(vocab), \"unique characters in the dataset\")\nThere are 83 unique characters in the dataset\n\n\n2.3 ë°ì´í„° í”„ë¦¬ í”„ë¡œì„¸ì‹±\nìš°ë¦¬ëŠ” RNNì„ í›ˆë ¨ì‹œì¼œ ABC ê¸°ë³´ë²•ì˜ íŒ¨í„´ì„ í•™ìŠµí•˜ê³ ì í•©ë‹ˆë‹¤.\nê·¸ë¦¬ê³  ì´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ìƒˆë¡œìš´ ìŒì•…ì„ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤.\nì´ë¥¼ ê³ ë ¤í•œë‹¤ë©´, ìš°ë¦¬ê°€ ëª¨ë¸ì—ì„œ ì–»ê³ ì í•˜ëŠ” ê²ƒì€, ë¬¸ì ì—´ì´ ì£¼ì–´ì¡Œì„ ë•Œ ë‹¤ìŒ ë¬¸ìê°€ ë¬´ì—‡ì¸ê°€ì— ëŒ€í•œ ê²ƒì…ë‹ˆë‹¤.\nRNN ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ì•ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ í‘œí˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.\nì´ë¥¼ ìœ„í•´ ë‘ ê°œì˜ lookup tableì„ ë§Œë“­ë‹ˆë‹¤.\ní•˜ë‚˜ëŠ” 83ê°œì˜ characterë¥¼ numberë¡œ, ë‹¤ë¥¸ í•˜ë‚˜ëŠ” numberë¥¼ ë‹¤ì‹œ characterë¡œ ë³€í™˜í•´ì£¼ëŠ” í…Œì´ë¸”ì…ë‹ˆë‹¤.\n### Define numerical representation of text ###\n\n# Create a mapping from character to unique index.\n# For example, to get the index of the character \"d\", \n#   we can evaluate `char2idx[\"d\"]`.  \nchar2idx = {u:i for i, u in enumerate(vocab)}\n\n# Create a mapping from indices to characters. This is\n#   the inverse of char2idx and allows us to convert back\n#   from unique index to the character in our vocabulary.\nidx2char = np.array(vocab)\nì´ì œ ê° ë¬¸ìë“¤ì€ í•˜ê¸°ì™€ ê°™ì´ ìˆ«ìì™€ ë§¤ì¹­ë©ë‹ˆë‹¤.\n{\n  '\\n':   0,\n  ' ' :   1,\n  '!' :   2,\n  '\"' :   3,\n  '#' :   4,\n  \"'\" :   5,\n  '(' :   6,\n  ')' :   7,\n  ',' :   8,\n  '-' :   9,\n  '.' :  10,\n  '/' :  11,\n  '0' :  12,\n  '1' :  13,\n  '2' :  14,\n  '3' :  15,\n  '4' :  16,\n  '5' :  17,\n  '6' :  18,\n  '7' :  19,\n  ...\n}\nì´ì œ ê° ë…¸ë˜ë¥¼ ìˆ«ìë¡œ ë³€í™˜ì‹œì¼œë´…ì‹œë‹¤.\n### Vectorize the songs string ###\ndef vectorize_string(string):\n  result = []\n  for c in string:\n    result.append(char2idx[c])\n  result = np.array(result)\n  return result  \n\nvectorized_songs = vectorize_string(songs_joined)\n'X:1\\nT:Alex' ---- characters mapped to int ----&gt; [49 22 13  0 45 22 26 67 60 79]\në°ì´í„° ì…‹ì„ ìˆ«ìë¡œ ë³€ê²½í•˜ì˜€ìœ¼ë‹ˆ, training exampleê³¼ targetì„ ì •í•´ë´…ì‹œë‹¤.\nìš°ë¦¬ëŠ” RNNì— seq_length ë§Œí¼ì˜ inputì„ ë„£ê³  seq_length + 1 ê¹Œì§€ì˜ ìˆ«ìë¥¼ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.\nì˜ˆë¥¼ ë“¤ì–´ seq_length ê°€ 4ì¼ ë•Œ â€œhellâ€ì´ë¼ëŠ” ë‹¨ì–´ê°€ inputì´ ë˜ë©´, â€œelloâ€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n### Batch definition to create training examples ###\n\ndef get_batch(vectorized_songs, seq_length, batch_size):\n  # the length of the vectorized songs string\n  n = vectorized_songs.shape[0] - 1\n  # randomly choose the starting indices for the examples in the training batch\n  idx = np.random.choice(n-seq_length, batch_size)\n\n  input_batch = [vectorized_songs[i : i+seq_length] for i in idx]\n  output_batch = [vectorized_songs[i+1 : i+seq_length+1] for i in idx]\n\n\n  # x_batch, y_batch provide the true inputs and targets for network training\n  x_batch = np.reshape(input_batch, [batch_size, seq_length])\n  y_batch = np.reshape(output_batch, [batch_size, seq_length])\n  return x_batch, y_batch\n\n\n2.4 RNN ëª¨ë¸\nì´ì œ RNN ëª¨ë¸ì„ í›ˆë ¨í•  ëª¨ë“  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\nRNNì„ ì´ìš©í•´ ABC ê¸°ë³´ë²•ì„ í›ˆë ¨í•˜ì—¬ ë…¸ë˜ë¥¼ ìƒì„±í•´ë³´ë„ë¡ í•©ì‹œë‹¤.\nìš°ë¦¬ê°€ ì‚¬ìš©í•  Layerë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\ntf.keras.layers.Embedding: ê° ë¬¸ìì˜ ìˆ«ìë¥¼ ì°¨ì›ì´ ìˆëŠ” ë²¡í„°ì— ë§¤í•‘í•˜ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ì¡°íšŒ í…Œì´ë¸”ë¡œ êµ¬ì„±ëœ ì…ë ¥ ë ˆì´ì–´ì…ë‹ˆë‹¤.\ntf.keras.layers.LSTM: í¬ê¸°ê°€ ìˆëŠ” LSTM ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤.\ntf.keras.layers.Dense: ì¶œë ¥ì´ ìˆëŠ” ì¶œë ¥ ë ˆì´ì–´ì…ë‹ˆë‹¤.\n\n\ndef LSTM(rnn_units): \n  return tf.keras.layers.LSTM(\n    rnn_units, \n    return_sequences=True, \n    recurrent_initializer='glorot_uniform',\n    recurrent_activation='sigmoid',\n    stateful=True,\n  )\n\n### Defining the RNN Model ###\n\n'''TODO: Add LSTM and Dense layers to define the RNN model using the Sequential API.'''\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    # Layer 1: Embedding layer to transform indices into dense vectors \n    #   of a fixed embedding size\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n\n    # Layer 2: LSTM with `rnn_units` number of units. \n    # TODO: Call the LSTM function defined above to add this layer.\n    LSTM(rnn_units),\n\n    # Layer 3: Dense (fully-connected) layer that transforms the LSTM output\n    #   into the vocabulary size. \n    # TODO: Add the Dense layer.\n    tf.keras.layers.Dense(vocab_size)\n  ])\n\n  return model\n\n# Build a simple model with default hyperparameters. You will get the \n#   chance to change these later.\nmodel = build_model(len(vocab), embedding_dim=256, rnn_units=1024, batch_size=32)\nmodel.summary()\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (32, None, 256)           21248     \n                                                                 \n lstm (LSTM)                 (32, None, 1024)          5246976   \n                                                                 \n dense (Dense)               (32, None, 83)            85075     \n                                                                 \n=================================================================\nTotal params: 5,353,299\nTrainable params: 5,353,299\nNon-trainable params: 0\n_________________________________________________________________\nx, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\npred = model(x)\nprint(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\nprint(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")\nInput shape:       (32, 100)  # (batch_size, sequence_length)\nPrediction shape:  (32, 100, 83) # (batch_size, sequence_length, vocab_size)\nì´ì œ ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ Loss í•¨ìˆ˜ë¥¼ ì •ì˜í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ multi labelì„ ì˜ˆì¸¡í•˜ëŠ” classification ëª¨ë¸ì…ë‹ˆë‹¤.\nì´ëŸ° ê²½ìš° tensorflowì—ì„œ sparase_categorical_crossentropyë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì ì ˆí•©ë‹ˆë‹¤.\n### Defining the loss function ###\n\ndef compute_loss(labels, logits):\n  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n  return loss\n\nexample_batch_loss = compute_loss(y, pred)\n\nprint(\"Prediction shape: \", pred.shape, \" # (batch_size, sequence_length, vocab_size)\") \nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())\nëª¨ë¸ í•™ìŠµì„ ìœ„í•œ Hyperparameterë¥¼ ì„¸íŒ…í•´ ë´…ì‹œë‹¤.\n### Hyperparameter setting and optimization ###\n\n# Optimization parameters:\nnum_training_iterations = 2000  # Increase this to train longer\nbatch_size = 4  # Experiment between 1 and 64\nseq_length = 100  # Experiment between 50 and 500\nlearning_rate = 5e-3  # Experiment between 1e-5 and 1e-1\n\n# Model parameters: \nvocab_size = len(vocab)\nembedding_dim = 256 \nrnn_units = 1024  # Experiment between 1 and 2048\n\n# Checkpoint location: \ncheckpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")\nì´ì œ trainì„ ì‹œì‘í•´ë´…ì‹œë‹¤. ì´ë²ˆ ê³¼ì œì—ì„œëŠ” Adam ì˜µí‹°ë§ˆì´ì €ì™€ tf.GradientTapeë¥¼ í™œìš©í•©ë‹ˆë‹¤.\n### Define optimizer and training operation ###\n\n\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate)\n\n@tf.function\ndef train_step(x, y): \n  # Use tf.GradientTape()\n  with tf.GradientTape() as tape:\n  \n    y_hat = model(x)\n  \n    loss = compute_loss(y,y_hat)\n\n  # Now, compute the gradients \n  '''TODO: complete the function call for gradient computation. \n      Remember that we want the gradient of the loss with respect all \n      of the model parameters. \n      HINT: use `model.trainable_variables` to get a list of all model\n      parameters.'''\n  grads = tape.gradient(loss, model.trainable_variables)\n  \n  # Apply the gradients to the optimizer so it can update the model accordingly\n  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n  return loss\n\n##################\n# Begin training!#\n##################\n\nhistory = []\nplotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\nif hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n\nfor iter in tqdm(range(num_training_iterations)):\n\n  # Grab a batch and propagate it through the network\n  x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\n  loss = train_step(x_batch, y_batch)\n\n  # Update the progress bar\n  history.append(loss.numpy().mean())\n  plotter.plot(history)\n\n  # Update the model with the changed weights!\n  if iter % 100 == 0:     \n    model.save_weights(checkpoint_prefix)\n    \n# Save the trained model and the weights\nmodel.save_weights(checkpoint_prefix)\n\nì´ì œ ìƒˆë¡œìš´ ìŒì•…ì„ ë§Œë“¤ì–´ë³´ë„ë¡ í•©ì‹œë‹¤. ìš°ì„  batch_sizeë¥¼ 1ë¡œ ì¤„ì—¬ ìƒˆë¡œìš´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1) # TODO\n\n# Restore the model weights for the last checkpoint after training\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel.build(tf.TensorShape([1, None]))\n\nmodel.summary()\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_4 (Embedding)     (1, None, 256)            21248     \n                                                                 \n lstm_4 (LSTM)               (1, None, 1024)           5246976   \n                                                                 \n dense_4 (Dense)             (1, None, 83)             85075     \n                                                                 \n=================================================================\nTotal params: 5,353,299\nTrainable params: 5,353,299\nNon-trainable params: 0\n_________________________________________________________________\nì´ì œ predictionì„ í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\n### Prediction of a generated song ###\n\ndef generate_text(model, start_string, generation_length=1000):\n  # Evaluation step (generating ABC text using the learned RNN model)\n\n  # convert the start string to numbers (vectorize)\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Here batch size == 1\n  model.reset_states()\n  tqdm._instances.clear()\n\n  for i in tqdm(range(generation_length)):\n      # evaluate the inputs and generate the next character predictions\n      predictions = model(input_eval)\n      \n      # Remove the batch dimension\n      predictions = tf.squeeze(predictions, 0)\n      \n      # use a multinomial distribution to sample'''\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n      \n      # Pass the prediction along with the previous hidden state\n      #   as the next inputs to the model\n      input_eval = tf.expand_dims([predicted_id], 0)\n      \n      # add the predicted character to the generated text!\n      text_generated.append(idx2char[predicted_id]) \n    \n  return (start_string + ''.join(text_generated))\n\ngenerated_text = generate_text(model, start_string=\"X\", generation_length=1000)\ngenerated_songs = mdl.lab1.extract_song_snippet(generated_text)"
  },
  {
    "objectID": "posts/mutual-information/index.html",
    "href": "posts/mutual-information/index.html",
    "title": "ìƒí˜¸ ì •ë³´ëŸ‰ (Mutual Informaion)",
    "section": "",
    "text": "ìƒí˜¸ ì •ë³´ëŸ‰ì€..\nìƒí˜¸ ì •ë³´ëŸ‰(mutual information, MI)ì€ ì •ë³´ ì´ë¡ (Information Theory)ì—ì„œ ë‘ í™•ë¥  ë³€ìˆ˜ ê°„ì˜ ì˜ì¡´ì„±ì„ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ìƒí˜¸ ì •ë³´ëŸ‰ì€ ë‘ í™•ë¥  ë³€ìˆ˜ê°€ ì„œë¡œ ì–¼ë§ˆë‚˜ ë§ì€ ì •ë³´ë¥¼ ê³µìœ í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ë©°, ê·¸ ê°’ì´ í´ìˆ˜ë¡ ë‘ ë³€ìˆ˜ ì‚¬ì´ì˜ ì˜ì¡´ì„±ì´ ë†’ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒí˜¸ ì •ë³´ëŸ‰ì€ ì—”íŠ¸ë¡œí”¼ì™€ ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìˆ˜ì‹ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.\në‘ í™•ë¥  ë³€ìˆ˜ Xì™€ Yê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ìƒí˜¸ ì •ë³´ëŸ‰ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:\n\\[\nMI(X; Y) = \\displaystyle \\sum_{x \\in X }\\sum_{y \\in Y}p(x,y)log[\\frac{p(x,y)}{p(x),p(y)}]\n\\]\n\n\nR-squaredì™€ì˜ ì°¨ì´\nìƒí˜¸ ì •ë³´ëŸ‰(mutual information, MI)ê³¼ ê²°ì •ê³„ìˆ˜(R-squared)ëŠ” ë‘ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ì¸¡ì •í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í†µê³„ì ì¸ ì§€í‘œì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ ê°€ì •ê³¼ ê³„ì‚° ë°©ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ê°ê° ë‹¤ë¥¸ ì¸¡ë©´ì„ ê°•ì¡°í•©ë‹ˆë‹¤.\n\nMIëŠ” ë‘ ë³€ìˆ˜ ê°„ì˜ ì¼ë°˜ì ì¸ ì˜ì¡´ì„±ì„ ì¸¡ì •í•˜ëŠ” ë°˜ë©´, R-squaredëŠ” ë‘ ë³€ìˆ˜ ê°„ì˜ ì„ í˜• ê´€ê³„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. MIëŠ” ì„ í˜•, ë¹„ì„ í˜•, ëª¨ë…¸í† ë‹‰ ë“± ëª¨ë“  ì¢…ë¥˜ì˜ ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì§€ë§Œ, R-squaredëŠ” ì„ í˜• ê´€ê³„ì—ë§Œ êµ­í•œë©ë‹ˆë‹¤.\nMIëŠ” ì–‘ìˆ˜ ë˜ëŠ” 0ì˜ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆìœ¼ë©°, ê°’ì´ í´ìˆ˜ë¡ ë‘ ë³€ìˆ˜ê°€ ë§ì€ ì •ë³´ë¥¼ ê³µìœ í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. R-squaredì˜ ê°’ì€ 0ê³¼ 1 ì‚¬ì´ì— ìˆìœ¼ë©°, ê°’ì´ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì„ í˜• ê´€ê³„ê°€ ê°•í•˜ë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/pca/index.html",
    "href": "posts/pca/index.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "ì¢Œì¸¡ 3D ëª¨ë¸ì„ 2ì°¨ì›ìœ¼ë¡œ ì˜®ê¸´ ëª¨ìŠµ.\n\nPCAëŠ”..\nì£¼ì„±ë¶„ ë¶„ì„(PCA, Principal Component Analysis)ì€ ê³ ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ì €ì°¨ì›ì˜ ë°ì´í„°ë¡œ ì¶•ì†Œí•˜ê±°ë‚˜ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í†µê³„ì  ê¸°ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë°ì´í„°ì˜ ë¶„ì‚°(variance)ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œ ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ ì¢Œí‘œê³„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. PCAëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤.\n\në°ì´í„°ì˜ ê³µë¶„ì‚° í–‰ë ¬(covariance matrix)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\nê³µë¶„ì‚° í–‰ë ¬ì˜ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\nê³ ìœ ê°’ì´ í° ìˆœì„œëŒ€ë¡œ í•´ë‹¹ ê³ ìœ ë²¡í„°ë¥¼ ì •ë ¬í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì •ë ¬ëœ ê³ ìœ ë²¡í„°ê°€ ì£¼ì„±ë¶„(principal components)ì´ë©°, ë°ì´í„°ì˜ ë¶„ì‚°ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nì›í•˜ëŠ” ì°¨ì›ì˜ ìˆ˜ë§Œí¼ ì£¼ì„±ë¶„ì„ ì„ íƒí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 3ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ë ¤ë©´ ê°€ì¥ í° ë‘ ê°œì˜ ê³ ìœ ê°’ì— í•´ë‹¹í•˜ëŠ” ê³ ìœ ë²¡í„°ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\nì„ íƒí•œ ì£¼ì„±ë¶„ì— ë°ì´í„°ë¥¼ íˆ¬ì˜í•˜ì—¬ ì €ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ì–»ìŠµë‹ˆë‹¤.\n\nPCAì˜ ì£¼ìš” ëª©ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\nì°¨ì› ì¶•ì†Œ: ê³ ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ì €ì°¨ì›ì˜ ë°ì´í„°ë¡œ ë³€í™˜í•˜ì—¬ ë°ì´í„°ì˜ ë³µì¡ì„±ì„ ì¤„ì´ê³  ê³„ì‚° ë¹„ìš©ì„ ì ˆê°í•©ë‹ˆë‹¤.\nì‹œê°í™”: ê³ ì°¨ì›ì˜ ë°ì´í„°ë¥¼ 2D ë˜ëŠ” 3Dë¡œ í‘œí˜„í•˜ì—¬ ë°ì´í„°ì˜ íŒ¨í„´, í´ëŸ¬ìŠ¤í„° ë˜ëŠ” ì´ìƒì¹˜ë¥¼ ì‰½ê²Œ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\në…¸ì´ì¦ˆ ì œê±°: ë°ì´í„°ì˜ ì£¼ìš” ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ì—¬ ë°ì´í„°ë¥¼ ê¹¨ë—í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.\níŠ¹ì„± ì„ íƒ ë° ì¶”ì¶œ: ë°ì´í„°ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±ì„ ì„ íƒí•˜ê±°ë‚˜ ìƒˆë¡œìš´ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ì—¬ ë°ì´í„°ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nPCAëŠ” ë°ì´í„° ì „ì²˜ë¦¬, ì‹œê°í™”, ê¸°ê³„ í•™ìŠµ ë° íŒ¨í„´ ì¸ì‹ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì£¼ì„±ë¶„ ë¶„ì„ì—ëŠ” ëª‡ ê°€ì§€ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, PCAëŠ” ì„ í˜•ì ì¸ ê´€ê³„ë¥¼ ê°€ì •í•˜ê¸° ë•Œë¬¸ì— ë¹„ì„ í˜•ì ì¸ ë°ì´í„° êµ¬ì¡°ë¥¼ ì œëŒ€ë¡œ í‘œí˜„í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²½ìš°ì—ëŠ” ì»¤ë„ PCA, t-SNE, UMAP ë“± ë‹¤ë¥¸ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nPCA ì˜ˆì œ\n# Load required library\nlibrary(Rvcg)\nlibrary(rgl)\nlibrary(tidyverse)\n\n# Load 3D Dataset\nstl_mesh &lt;- vcgImport(\"D:/Utah_teapot_(solid).stl\")\n\n# Convert stl file to df\nvertices &lt;- cbind(stl_mesh$vb[1,],stl_mesh$vb[2,],stl_mesh$vb[3,])\n\n# Compress data in 2D Using PCA\npca_result &lt;- prcomp(vertices, center = TRUE, scale. = TRUE)\nreduced_data &lt;- pca_result$x[, 1:2]\nreduced_df &lt;- data.frame(reduced_data)\ncolnames(reduced_df) &lt;- c(\"PC1\", \"PC2\")\n\n# Visualize 2D data\n\nggplot(reduced_df, aes(x = PC2, y = PC1)) +\n  geom_point() +\n  theme_minimal() +\n  ggtitle(\"Teapot 3D Data Reduced to 2D using PCA\")\n\n\nPCA from Scratch\n# ì˜ˆì œ ë°ì´í„° ìƒì„±\nset.seed(42)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100, sd = 0.5)\nz &lt;- -x + y + rnorm(100, sd = 0.5)\ndata &lt;- data.frame(x, y, z)\n\n# 1. ë°ì´í„°ì˜ ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚°\ndata_cov &lt;- cov(data)\n\n# 2. ê³µë¶„ì‚° í–‰ë ¬ì˜ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„° ì°¾ê¸°\neigen_result &lt;- eigen(data_cov)\n\n# ê³ ìœ ê°’\neigen_values &lt;- eigen_result$values\n# ê³ ìœ ë²¡í„°\neigen_vectors &lt;- eigen_result$vectors\n\n# 3. ì›í•˜ëŠ” ì°¨ì›ì˜ ìˆ˜ë§Œí¼ ì£¼ì„±ë¶„ ì„ íƒ (ì—¬ê¸°ì„œëŠ” 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ)\nnum_dimensions &lt;- 2\nselected_eigen_vectors &lt;- sorted_eigen_vectors[, 1:num_dimensions]\n\n# 4. ì„ íƒí•œ ì£¼ì„±ë¶„ì— ë°ì´í„°ë¥¼ íˆ¬ì˜í•˜ì—¬ ì €ì°¨ì›ì˜ ë°ì´í„° ì–»ê¸°\ndata_centered &lt;- scale(data, center = TRUE, scale = FALSE)\nreduced_data &lt;- data_centered %*% selected_eigen_vectors\n\n# ê²°ê³¼ ì¶œë ¥\nreduced_data &lt;- data.frame(reduced_data)\ncolnames(reduced_data) &lt;- c(\"PC1\", \"PC2\")\nprint(reduced_data)"
  },
  {
    "objectID": "posts/random-forest/index.html",
    "href": "posts/random-forest/index.html",
    "title": "Random Forest",
    "section": "",
    "text": "Random ForestëŠ”..\nëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ë°°ê¹…(Bagging)ì˜ í•œ í˜•íƒœë¡œì„œ, ì—¬ëŸ¬ ê°œì˜ ê²°ì • íŠ¸ë¦¬ë¥¼ ì¡°í•©í•˜ì—¬ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” ì•™ìƒë¸” ê¸°ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê²°ì • íŠ¸ë¦¬ì˜ ê°€ì¥ í° ë¬¸ì œì  ì¤‘ í•˜ë‚˜ì¸ ê³¼ì í•©(overfitting)ì„ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ê³ , ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\nëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\nì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ì„ í†µí•´ ì—¬ëŸ¬ ê°œì˜ ìƒ˜í”Œì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ë•Œ, ì¤‘ë³µ í—ˆìš©ìœ¼ë¡œ ì›ë³¸ ë°ì´í„°ì…‹ê³¼ ê°™ì€ í¬ê¸°ì˜ ìƒ˜í”Œì„ ë§Œë“­ë‹ˆë‹¤.\nê° ìƒ˜í”Œë¡œë¶€í„° ê²°ì • íŠ¸ë¦¬ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ íŠ¹ì„±(feature)ì˜ ë¶€ë¶„ì§‘í•©ì„ ì‚¬ìš©í•´ ë…¸ë“œë¥¼ ë¶„í• í•˜ëŠ” ìµœì ì˜ ë¶„í• ì„ ì°¾ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬´ì‘ìœ„ì„±ì€ ê²°ì • íŠ¸ë¦¬ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë‚®ì¶”ê³ , ë‹¤ì–‘ì„±ì„ ë†’ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.\ní…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ë•Œ, ê° ê²°ì • íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ì„ ëª¨ì•„ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤. ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš° ë‹¤ìˆ˜ê²° íˆ¬í‘œ(Majority Voting) ë°©ì‹ì„ ì‚¬ìš©í•´ ìµœì¢… ì˜ˆì¸¡ í´ë˜ìŠ¤ë¥¼ ê²°ì •í•˜ë©°, íšŒê·€ ë¬¸ì œì˜ ê²½ìš° ê° íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ê°’ì˜ í‰ê· ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n\n\nFashion MNIST ë°ì´í„°ë¥¼ í™œìš©í•œ Random Forest ì˜ˆì œ\n [ì¶œì²˜] : Fashion MNIST | Kaggle\nlibrary(tidymodels)\n\n## Load Dataset\nfmnist &lt;- arrow::read_feather(\"fashion-mnist.feather\")\nfmnist$label &lt;- as.factor(fmnist$label)\n\nset.seed(42)\nsplit_index &lt;- initial_split(fmnist,3/4,label)\ntrain_tb &lt;- training(split_index)\ntest_tb &lt;- testing(split_index)\n\n## Make workflow\n\nfmnist_recipe &lt;- recipe(label ~ ., data = train_tb) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; \n  prep()\n\nfmnist_rf &lt;- rand_forest(trees = 100, mtry = sqrt(784)) |&gt; \n  set_engine(\"ranger\", importance = \"impurity\") |&gt; \n  set_mode(\"classification\")\n\nfmnist_wf &lt;- workflow() |&gt;\n  add_recipe(fmnist_recipe) |&gt; \n  add_model(fmnist_rf)\n\nfmnist_wf_trained &lt;- fmnist_wf |&gt; fit(train_tb)\n\n# Predict with test dat\npredictions &lt;- predict(fmnist_wf_trained, test_tb) |&gt; \n  bind_cols(test_tb |&gt;  select(label))\n\naccuracy(predictions, truth = label, estimate = .pred_class)\n# A tibble: 1 Ã— 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.882"
  },
  {
    "objectID": "posts/regularization/index.html",
    "href": "posts/regularization/index.html",
    "title": "Regularization (ì •ê·œí™” ê¸°ë²•)",
    "section": "",
    "text": "ëª¨ë¸ì˜ ì •ê·œí™”ë€â€¦\nì •ê·œí™” ê¸°ë²•ì€ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì¤„ì´ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì •ê·œí™” ê¸°ë²•ìœ¼ë¡œëŠ” ë¦¿ì§€ íšŒê·€(Ridge Regression), ë¼ì˜ íšŒê·€(Lasso Regression), ê·¸ë¦¬ê³  ì—˜ë¼ìŠ¤í‹±ë„· íšŒê·€(Elastic Net Regression)ê°€ ìˆìŠµë‹ˆë‹¤.\n\në¦¿ì§€ íšŒê·€ (Ridge Regression) - L2 ì •ê·œí™”: ë¦¿ì§€ íšŒê·€ëŠ” ì„ í˜• íšŒê·€ì— L2 ì •ê·œí™”ë¥¼ ì¶”ê°€í•œ ê²ƒì…ë‹ˆë‹¤. L2 ì •ê·œí™”ëŠ” íšŒê·€ ê³„ìˆ˜ì˜ ì œê³±í•©ì— ë¹„ë¡€í•˜ëŠ” íŒ¨ë„í‹°ë¥¼ ì†ì‹¤ í•¨ìˆ˜ì— ì¶”ê°€í•©ë‹ˆë‹¤. ì´ íŒ¨ë„í‹°ëŠ” ëª¨ë¸ì˜ ê³„ìˆ˜ë¥¼ ì‘ê²Œ ë§Œë“¤ì–´ ê³¼ì í•©ì„ ë°©ì§€í•˜ë©°, ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„°ì¸ ëŒë‹¤(lambda)ëŠ” ì •ê·œí™” í•­ì˜ ê°•ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.\në¼ì˜ íšŒê·€ (Lasso Regression) - L1 ì •ê·œí™”: ë¼ì˜ íšŒê·€ëŠ” ì„ í˜• íšŒê·€ì— L1 ì •ê·œí™”ë¥¼ ì¶”ê°€í•œ ê²ƒì…ë‹ˆë‹¤. L1 ì •ê·œí™”ëŠ” íšŒê·€ ê³„ìˆ˜ì˜ ì ˆëŒ“ê°’ì˜ í•©ì— ë¹„ë¡€í•˜ëŠ” íŒ¨ë„í‹°ë¥¼ ì†ì‹¤ í•¨ìˆ˜ì— ì¶”ê°€í•©ë‹ˆë‹¤. ë¼ì˜ íšŒê·€ëŠ” ê³„ìˆ˜ë¥¼ ì •í™•íˆ 0ìœ¼ë¡œ ë§Œë“¤ì–´ í¬ì†Œí•œ ëª¨ë¸ì„ ìƒì„±í•˜ë©°, ì´ë¥¼ í†µí•´ ë³€ìˆ˜ ì„ íƒì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ë¼ì˜ íšŒê·€ëŠ” ë¦¿ì§€ íšŒê·€ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ëŒë‹¤(lambda) í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™” í•­ì˜ ê°•ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.\nì—˜ë¼ìŠ¤í‹±ë„· íšŒê·€ (Elastic Net Regression) - L1ê³¼ L2 ì •ê·œí™”ì˜ ì¡°í•©: ì—˜ë¼ìŠ¤í‹±ë„· íšŒê·€ëŠ” L1 ì •ê·œí™”ì™€ L2 ì •ê·œí™”ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë¼ì˜ íšŒê·€ì˜ ë³€ìˆ˜ ì„ íƒ ê¸°ëŠ¥ê³¼ ë¦¿ì§€ íšŒê·€ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ëª¨ë‘ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—˜ë¼ìŠ¤í‹±ë„· íšŒê·€ì—ì„œëŠ” ëŒë‹¤(lambda)ì™€ ì•ŒíŒŒ(alpha) ë‘ ê°€ì§€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™” í•­ì˜ ê°•ë„ì™€ L1, L2 ì •ê·œí™”ì˜ ë¹„ìœ¨ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.\n\n [ì¶œì²˜] Lasso and Ridge Regression in Python Tutorial | DataCamp\n\n\nì ì ˆí•œ ì •ê·œí™” ê¸°ë²•ì˜ ì„ íƒë²•\n\nìƒê´€ ê´€ê³„ê°€ ë†’ì€ íŠ¹ì„±ì´ ë§ì€ ê²½ìš°: ì—˜ë¼ìŠ¤í‹±ë„· íšŒê·€ê°€ ë” ì í•©í•œ ì„ íƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. L1 ì •ê·œí™”ëŠ” ë³€ìˆ˜ ì„ íƒì— ë„ì›€ì´ ë˜ì§€ë§Œ, ìƒê´€ ê´€ê³„ê°€ ë†’ì€ íŠ¹ì„±ë“¤ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•˜ê³  ë‹¤ë¥¸ íŠ¹ì„±ë“¤ì„ ì œì™¸í•  ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²½ìš°, L1ê³¼ L2 ì •ê·œí™”ì˜ ì¡°í•©ì¸ ì—˜ë¼ìŠ¤í‹±ë„· íšŒê·€ê°€ ë” ì ì ˆí•œ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\níŠ¹ì„± ìˆ˜ê°€ ê´€ì¸¡ì¹˜ ìˆ˜ë³´ë‹¤ ë§ì€ ê²½ìš°: ë¦¿ì§€ íšŒê·€ê°€ ì í•©í•œ ì„ íƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¼ì˜ íšŒê·€ëŠ” ìµœëŒ€ ê´€ì¸¡ì¹˜ ìˆ˜ë§Œí¼ì˜ ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ë¯€ë¡œ, ì´ ê²½ìš°ì—ëŠ” ë¦¿ì§€ íšŒê·€ê°€ ë” ë‚˜ì€ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\níŠ¹ì„± ì„ íƒì´ ì¤‘ìš”í•œ ê²½ìš°: ë¼ì˜ íšŒê·€ê°€ ì í•©í•œ ì„ íƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. L1 ì •ê·œí™”ëŠ” ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ë³€ìˆ˜ ì„ íƒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ë¼ì˜ íšŒê·€ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë” ê°„ë‹¨í•œ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/t-sne/index.html",
    "href": "posts/t-sne/index.html",
    "title": "t-sne",
    "section": "",
    "text": "t-SNEë€â€¦\nt-SNE(t-Distributed Stochastic Neighbor Embedding)ëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì› ê³µê°„(ì£¼ë¡œ 2ì°¨ì› ë˜ëŠ” 3ì°¨ì›)ìœ¼ë¡œ ì‹œê°í™”í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì…ë‹ˆë‹¤. t-SNEëŠ” ì›ë³¸ ê³ ì°¨ì› ë°ì´í„°ì—ì„œì˜ ë°ì´í„° í¬ì¸íŠ¸ ê°„ ìœ ì‚¬ë„ì™€ ì¶•ì†Œëœ ì €ì°¨ì› ê³µê°„ì—ì„œì˜ ë°ì´í„° í¬ì¸íŠ¸ ê°„ ìœ ì‚¬ë„ë¥¼ ë¹„ìŠ·í•˜ê²Œ ìœ ì§€í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì´ ê¸°ë²•ì€ íŠ¹íˆ ë°ì´í„°ì˜ êµ°ì§‘ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ëŠ” ë° íš¨ê³¼ì ì´ë¼ê³  ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.\nt-SNEëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤:\n\nìœ ì‚¬ë„ ê³„ì‚°: ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ê° ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ë•Œ, ê°€ìš°ì‹œì•ˆ ì»¤ë„ì„ ì‚¬ìš©í•˜ì—¬ ì¡°ê±´ë¶€ í™•ë¥ ì„ êµ¬í•©ë‹ˆë‹¤. ì´ ì¡°ê±´ë¶€ í™•ë¥ ì€ í•œ ë°ì´í„° í¬ì¸íŠ¸ê°€ ë‹¤ë¥¸ ë°ì´í„° í¬ì¸íŠ¸ì™€ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ì…ë‹ˆë‹¤.\nì €ì°¨ì› ë§µí•‘: ì´ˆê¸°ì— ë¬´ì‘ìœ„ë¡œ ì„¤ì •ëœ ì €ì°¨ì› ê³µê°„ì—ì„œì˜ ë°ì´í„° í¬ì¸íŠ¸ ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. t-SNEëŠ” ì´ ë‹¨ê³„ì—ì„œ t-ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ëŠ” ì›ë³¸ ê³ ì°¨ì› ë°ì´í„°ì—ì„œì˜ êµ°ì§‘ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œì˜ ë§µí•‘ì„ ë” ì‰½ê²Œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\nìµœì í™”: ê³ ì°¨ì› ë°ì´í„°ì—ì„œì˜ ìœ ì‚¬ë„ì™€ ì €ì°¨ì› ê³µê°„ì—ì„œì˜ ìœ ì‚¬ë„ê°€ ìµœëŒ€í•œ ë¹„ìŠ·í•´ì§€ë„ë¡, ì €ì°¨ì› ê³µê°„ì˜ ë°ì´í„° í¬ì¸íŠ¸ ìœ„ì¹˜ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤. ì´ ìµœì í™” ê³¼ì •ì€ ê·¸ë˜ë””ì–¸íŠ¸ ë””ì„¼íŠ¸(Gradient Descent)ì™€ ê°™ì€ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ë©ë‹ˆë‹¤.\n\nt-SNEëŠ” ì°¨ì› ì¶•ì†Œ ê²°ê³¼ë¥¼ í†µí•´ ê³ ì°¨ì› ë°ì´í„°ì—ì„œì˜ êµ°ì§‘ êµ¬ì¡°ì™€ íŒ¨í„´ì„ ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰½ê²Œ í•´ì¤ë‹ˆë‹¤. í•˜ì§€ë§Œ ê³„ì‚° ë³µì¡ë„ê°€ ë†’ì•„ í° ë°ì´í„°ì…‹ì— ì ìš©í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆê³ , ìµœì í™” ê³¼ì •ì˜ ë¬´ì‘ìœ„ì„±ìœ¼ë¡œ ì¸í•´ ê²°ê³¼ì˜ ì¬í˜„ì„±ì´ ë‚®ì„ ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.\nlibrary(reticulate)\nlibrary(tidyverse)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.manifold import TSNE\n\n# Load the MNIST dataset\nmnist = datasets.fetch_openml('mnist_784')\nX, y = mnist.data, mnist.target\n\n# Select a subset of the dataset to reduce computation time\nn_samples = 5000\nX_sample = X[:n_samples]\ny_sample = y[:n_samples]\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_sample)\n\ny_sample\ny_sample = np.asarray(y_sample)\nX_tsne &lt;- py$X_tsne\ny_sample &lt;- py$y_sample\n\ntsne_df &lt;- tibble(x1 = X_tsne[,1], x2 = X_tsne[,2], y = y_sample)\n\nhead(tsne_df)\n# A tibble: 6 Ã— 3\n    x1     x2  y        \n1  17.1   9.26  5        \n2  59.9   24.4  0        \n3  14.0  -53.1  4        \n4 -62.3   12.5  1        \n5  -3.57 -26.4  9        \n6 -32.6   61.9  2\n# Draw Plot\ntsne_df |&gt;\n  ggplot(aes(x = x1, y = x2, color = y)) +\n  geom_jitter() +\n  ggtitle(\"Result of t-sne on MNIST\")+\n  ggthemes::theme_fivethirtyeight()"
  },
  {
    "objectID": "posts/umap/index.html",
    "href": "posts/umap/index.html",
    "title": "UMAP",
    "section": "",
    "text": "UMAPì´ë€â€¦\nUMAP (Uniform Manifold Approximation and Projection)ì€ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ì¶•ì†Œí•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. UMAPì€ t-SNEì™€ ìœ ì‚¬í•˜ê²Œ ë°ì´í„°ì˜ êµ°ì§‘ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ê±°ë‚˜ ì°¨ì› ì¶•ì†Œí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ UMAPì€ ê¸°í•˜í•™ì  íŠ¹ì„±ì„ ë” ì˜ ë³´ì¡´í•˜ê³ , ê³„ì‚° íš¨ìœ¨ì„±ì´ ë” ë†’ì•„ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ë„ ì ìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\nUMAP ì•Œê³ ë¦¬ì¦˜ì˜ ì£¼ìš” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\nê³ ì°¨ì› ë°ì´í„°ì˜ ì§€ì—­ êµ¬ì¡° íŒŒì•…: ê° ë°ì´í„° í¬ì¸íŠ¸ì˜ ê·¼ì²˜ ì´ì›ƒì„ ì°¾ì•„ ë°ì´í„°ì˜ ì§€ì—­ì  êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ k-ìµœê·¼ì ‘ ì´ì›ƒ(KNN) ì•Œê³ ë¦¬ì¦˜ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\nì§€ì—­ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê³ ì°¨ì› ë°ì´í„°ì˜ ê·¸ë˜í”„ ìƒì„±: ê° ë°ì´í„° í¬ì¸íŠ¸ì™€ ê·¸ ì´ì›ƒ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ ì°¨ì› ë°ì´í„°ì˜ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ë•Œ, ê±°ë¦¬ ì¸¡ì •ì—ëŠ” ì£¼ë¡œ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ë‚˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•˜ë©°, ê°€ì¤‘ì¹˜ëŠ” ë©€ë¦¬ ë–¨ì–´ì§„ í¬ì¸íŠ¸ì— ëŒ€í•´ ë” ë‚®ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í• ë‹¹ë©ë‹ˆë‹¤.\nì €ì°¨ì› ì„ë² ë”©ì„ ìœ„í•œ ê·¸ë˜í”„ ìƒì„±: ì €ì°¨ì› ê³µê°„ì—ì„œë„ ì›ë³¸ ê³ ì°¨ì› ë°ì´í„°ì˜ ì§€ì—­ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë ¤ê³  ë…¸ë ¥í•˜ë©°, ê·¸ë˜í”„ ê¸°ë°˜ ìµœì í™”ë¥¼ í†µí•´ ê³ ì°¨ì› ê·¸ë˜í”„ì™€ ì €ì°¨ì› ê·¸ë˜í”„ ê°„ì˜ ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.\nìµœì í™”: ê·¸ë˜ë””ì–¸íŠ¸ ë””ì„¼íŠ¸(Gradient Descent)ì™€ ê°™ì€ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì €ì°¨ì› ì„ë² ë”© ê³µê°„ì˜ ë°ì´í„° í¬ì¸íŠ¸ ìœ„ì¹˜ë¥¼ ì¡°ì •í•˜ë©´ì„œ, ê³ ì°¨ì› ê·¸ë˜í”„ì™€ ì €ì°¨ì› ê·¸ë˜í”„ ê°„ì˜ ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•˜ëŠ” ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n\nUMAPì€ t-SNEì— ë¹„í•´ ê¸°í•˜í•™ì  íŠ¹ì„±ì„ ë” ì˜ ë³´ì¡´í•˜ê³ , ê³„ì‚° íš¨ìœ¨ì„±ì´ ë†’ì•„ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ë„ ì ìš©í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, UMAPì€ ì°¨ì› ì¶•ì†Œ ê²°ê³¼ë¥¼ í†µí•´ ê³ ì°¨ì› ë°ì´í„°ì—ì„œì˜ êµ°ì§‘ êµ¬ì¡°ì™€ íŒ¨í„´ì„ ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰½ê²Œ í•´ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ UMAPì€ ë§ì€ ë°ì´í„° ê³¼í•™ìë“¤ì´ ì„ í˜¸í•˜ëŠ” ì°¨ì› ì¶•ì†Œ ê¸°ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\nlibrary(reticulate)\nlibrary(tidyverse)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport umap\n\n# Load the MNIST dataset\nmnist = datasets.fetch_openml('mnist_784')\nX, y = mnist.data, mnist.target\n\n# Select a subset of the dataset to reduce computation time\nn_samples = 5000\nX_sample = X[:n_samples]\ny_sample = y[:n_samples]\n\n# Apply UMAP\numap_reducer = umap.UMAP(n_components=2, random_state=42)\nX_umap = umap_reducer.fit_transform(X_sample)\ny_sample = np.asarray(y_sample)\nX_umap &lt;- py$X_umap\ny_sample &lt;- py$y_sample\n\numap_df &lt;- tibble(x1 = X_umap[,1], x2 = X_umap[,2], y = y_sample)\n\nhead(umap_df)\n# A tibble: 6 Ã— 3\n    x1    x2   y        \n1  4.05  7.81  5        \n2  0.744 3.75  0        \n3 10.9   0.124 4        \n4 11.9   7.83  1        \n5 11.6   2.26  9        \n6  8.29  5.54  2\n# Draw Plot\numap_df |&gt;\n  ggplot(aes(x = x1, y = x2, color = y)) +\n  geom_jitter() +\n  ggtitle(\"Result of UMAP on MNIST\")+\n  ggthemes::theme_fivethirtyeight()"
  },
  {
    "objectID": "posts/word-embedding/index.html",
    "href": "posts/word-embedding/index.html",
    "title": "Word Embeddingê³¼ Word2Vec",
    "section": "",
    "text": "Word Embeddingì€..\në‹¨ì–´ ì„ë² ë”©ì€ ë‹¨ì–´ë“¤ì„ ê³ ì°¨ì› ë²¡í„° ê³µê°„ì— ë§¤í•‘í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ ë³€í™˜ëœ ë²¡í„°ëŠ” ë‹¨ì–´ ê°„ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ë°˜ì˜í•˜ë„ë¡ í•©ë‹ˆë‹¤. ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ ë‹¨ì–´ë“¤ì€ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ë†“ì´ê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì€ ë‹¨ì–´ ê°„ì˜ ìœ ì‚¬ì„±ì„ íŒŒì•…í•˜ê³  ë¬¸ì¥ì´ë‚˜ ë¬¸ì„œì˜ ì˜ë¯¸ë¥¼ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, â€œì™• - ë‚¨ì + ì—¬ì = ì—¬ì™•â€ê³¼ ê°™ì€ ê´€ê³„ë¥¼ ë²¡í„° ì—°ì‚°ì„ í†µí•´ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nìƒê¸° ì˜ˆì‹œì˜ ì‹œê°í™”\nê¸°ì´ˆì ì¸ Word Embeddingì˜ í•™ìŠµì€ í•œ ë¬¸ì¥ì˜ ë‹¤ìŒ ë‹¨ì–´(í† í°)ì„ ì˜ˆì¸¡í•˜ëŠ” Neural Network(ì´í•˜ NN)ì„ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ â€œì‚¬ê³¼/ëŠ”â€ ì´ë¼ëŠ” í† í°ì„ inputìœ¼ë¡œ ë„£ìœ¼ë©´ â€œë¹¨ê°›ë‹¤â€ë¼ëŠ” í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ í•™ìŠµëœ NNì—ì„œ ê° ë‹¨ì–´ì— í• ë‹¹ëœ ê° hidden layerë“¤ì— ëŒ€í•œ ê°€ì¤‘ë“¤ì´ ë‹¨ì–´ë“¤ì„ ë²¡í„° ê³µê°„ì— ë§¤í•‘í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ìœ„ì˜ ì™•/ë‚¨ì/ì—¬ì™•/ì—¬ìì˜ ì˜ˆì‹œë¥¼ ì¼ë°˜í™”í•  ê²½ìš°, ê° ë‹¨ì–´ë“¤ì´ 2ê°œì˜ hidden layerì— ì—°ê²°ë˜ì–´ ìˆê³ , ê° layerëŠ” ì„±ë³„/ì§ìœ„ë¥¼ ëœ»í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nWord2Vecì€..\nWord2Vecì€ êµ¬ê¸€ ì—°êµ¬ìë“¤ì´ ê°œë°œí•œ ë‹¨ì–´ ì„ë² ë”© ê¸°ë²•ì…ë‹ˆë‹¤. Word2Vecì€ í° í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ë‹¨ì–´ë“¤ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì‹ ê²½ë§ ê¸°ë°˜ ëª¨ë¸ì…ë‹ˆë‹¤. Word2Vecì—ëŠ” ì£¼ë¡œ ë‘ ê°€ì§€ í•™ìŠµ ë°©ë²•ì´ ì‚¬ìš©ë©ë‹ˆë‹¤:\n\nCBOW (Continuous Bag of Words): ì´ ë°©ë²•ì€ ì£¼ë³€ ë‹¨ì–´ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì¦‰, ì£¼ë³€ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. â€œì‚¬ê³¼ëŠ” ë¹¨ê°›ë‹¤â€ ì˜ˆì‹œì—ì„œ â€œì‚¬ê³¼â€ì™€ â€œë¹¨ê°›ë‹¤â€ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ â€œëŠ”â€ì„ ì¶œë ¥ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\nSkip-gram: ì´ ë°©ë²•ì€ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ë³€ ë‹¨ì–´ë“¤ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì£¼ë¡œ í° ë°ì´í„°ì…‹ì— ì í•©í•˜ë©°, í¬ì†Œí•œ ë‹¨ì–´ë“¤ì— ëŒ€í•´ì„œë„ ë” ë‚˜ì€ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ì˜ ì˜ˆì‹œë¥¼ í™œìš©í•˜ìë©´, ì—¬ê¸°ì„œëŠ” â€œëŠ”â€ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , â€œì‚¬ê³¼â€ì™€ â€œë¹¨ê°›ë‹¤â€ë¥¼ ì¶œë ¥ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n\n\n\nì˜ˆì œ: Seoul - Korea + Japan = ?\nlibrary(tidyverse)\nglove_6B_50d &lt;- read_table(\"C:/Users/tranq/Desktop/glove.6B.50d.txt\", \n                           col_names = FALSE)\n\nword_embeddings &lt;- glove_6B_50d[, 1]\nvector_embeddings &lt;- as.matrix(glove_6B_50d[, -1])\n\nget_word_vector &lt;- function(word) {\n  word_index &lt;- which(word_embeddings == word)\n  if (length(word_index) == 0) {\n    return(NULL)\n  }\n  return(vector_embeddings[word_index, ])\n}\n\nfind_closest_word &lt;- function(result_vector, n = 5) {\n\n  similarity_scores &lt;- vector_embeddings %*% result_vector / (sqrt(rowSums(vector_embeddings^2)) * sqrt(sum(result_vector^2)))\n  closest_indices &lt;- order(similarity_scores, decreasing = TRUE)[1:n]\n  return(word_embeddings[closest_indices,1])\n}\n\nseoul &lt;- get_word_vector(\"seoul\")\nkorea &lt;- get_word_vector(\"korea\")\njapan &lt;- get_word_vector(\"japan\")\n\nresult_vector &lt;- soeul - korea + japan\n\nclosest_word &lt;- find_closest_word(result_vector, n = 5)\nprint(closest_word)\n# A tibble: 5 Ã— 1\n  X1      \n  &lt;chr&gt;   \n1 tokyo   \n2 osaka   \n3 japan   \n4 shanghai\n5 seoul"
  },
  {
    "objectID": "posts/zifs-law/index.html",
    "href": "posts/zifs-law/index.html",
    "title": "Zipfâ€™s Law (ë‹¨ì–´ ì‚¬ìš©ì˜ ë¶„í¬)",
    "section": "",
    "text": "Zipfâ€™s LawëŠ”..\nZipfì˜ ë²•ì¹™(Zipfâ€™s Law)ì€ ì–¸ì–´í•™ê³¼ ì •ë³´ ì´ë¡ ì—ì„œ ê´€ì°°ë˜ëŠ” ê²½í—˜ì ì¸ ë²•ì¹™ìœ¼ë¡œ, ì£¼ì–´ì§„ ë§ë­‰ì¹˜(corpus)ì—ì„œ ë‹¨ì–´ì˜ ì‚¬ìš© ë¹ˆë„ì™€ ìˆœìœ„ ì‚¬ì´ì— íŠ¹ì •í•œ ê´€ê³„ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ê¸°ìˆ í•©ë‹ˆë‹¤. ì´ ë²•ì¹™ì€ ë¯¸êµ­ì˜ ì–¸ì–´í•™ì ì¡°ì§€ ìº¥ìŠ¤ë¦¬ ì§€í”„(George Kingsley Zipf)ì— ì˜í•´ 1930ë…„ëŒ€ì— ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\nZipfì˜ ë²•ì¹™ì— ë”°ë¥´ë©´, ë§ë­‰ì¹˜ì—ì„œ ê° ë‹¨ì–´ì˜ ì‚¬ìš© ë¹ˆë„ëŠ” ê·¸ ë‹¨ì–´ì˜ ìˆœìœ„ì— ë°˜ë¹„ë¡€í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§í•´, ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ì˜ ë¹ˆë„ëŠ” ë‘ ë²ˆì§¸ë¡œ ë¹ˆë²ˆí•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ì˜ ë¹ˆë„ë³´ë‹¤ ëŒ€ëµ ë‘ ë°° ë§ìœ¼ë©°, ì„¸ ë²ˆì§¸ë¡œ ë¹ˆë²ˆí•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ì˜ ë¹ˆë„ë³´ë‹¤ ëŒ€ëµ ì„¸ ë°° ë§ì€ ì‹ì…ë‹ˆë‹¤.\në¸Œë¼ìš´ ëŒ€í•™êµ í˜„ëŒ€ ë¯¸êµ­ ì˜ì–´ í‘œì¤€ ë§ë­‰ì¹˜ì˜ ê²½ìš°, ê°€ì¥ ì‚¬ìš© ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ëŠ” ì˜ì–´Â ì •ê´€ì‚¬ Â â€œtheâ€ì´ë©° ì „ì²´ ë¬¸ì„œì—ì„œ 7%ì˜ ë¹ˆë„(ì•½ ë°±ë§Œ ê°œ ë‚¨ì§“ì˜ ì „ì²´ ì‚¬ìš© ë‹¨ì–´ ì¤‘ 69,971íšŒ)ë¥¼ ì°¨ì§€í•œë‹¤. ë‘ ë²ˆì§¸ë¡œ ì‚¬ìš© ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ëŠ” â€œofâ€ë¡œ ì•½ 3.5% ë‚¨ì§“(36,411íšŒ)í•œ ë¹ˆë„ë¥¼ ì°¨ì§€í•˜ë©°, ì„¸ ë²ˆì§¸ë¡œ ì‚¬ìš© ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ëŠ” â€œandâ€(28,852íšŒ)ë¡œ, ì§€í”„ì˜ ë²•ì¹™ì— ì •í™•íˆ ë“¤ì–´ ë§ëŠ”ë‹¤. ì•½ 135ê°œ í•­ëª©ì˜ ì–´íœ˜ë§Œìœ¼ë¡œ ë¸Œë¼ìš´ ëŒ€í•™ ë§ë­‰ì¹˜ì˜ ì ˆë°˜ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\nZipfì˜ ë²•ì¹™ì€ ì—¬ëŸ¬ ì–¸ì–´ì™€ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ì—ì„œ ì¼ê´€ë˜ê²Œ ê´€ì°°ë˜ëŠ” í˜„ìƒìœ¼ë¡œ, ì¸ê°„ ì–¸ì–´ì™€ ì •ë³´ ì²˜ë¦¬ì˜ ê¸°ë³¸ ì›ë¦¬ë¥¼ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ì´ ë²•ì¹™ì˜ ì •í™•í•œ ì›ì¸ì€ ì•„ì§ ëª…í™•í•˜ê²Œ ë°í˜€ì§€ì§€ ì•Šì•˜ì§€ë§Œ, ìì—° ì–¸ì–´ê°€ ìµœì í™”ëœ ì •ë³´ ì „ë‹¬ ë°©ì‹ì„ ë”°ë¥´ëŠ” ê²°ê³¼ë¼ëŠ” ì„¤ëª…ì´ ì œì•ˆë˜ê³  ìˆìŠµë‹ˆë‹¤.\n\n\nZipfâ€™s Law ì˜ˆì œ (ì œì¸ ì˜¤ìŠ¤í‹´ ì†Œì„¤)\n\nì´ì„±ê³¼ ê°ì„± / ì˜¤ë§Œê³¼ í¸ê²¬ / ë§¨ìŠ¤í•„ë“œ íŒŒí¬ / ì— ë§ˆ / ë…¸ìƒê±° ì‚¬ì› / ì„¤ë“ ëª¨ë‘ Zipfâ€™s lawë¥¼ ë§Œì¡±í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n# Load Library\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(janeaustenr)\n\nword_count &lt;- austen_books() |&gt;\n  unnest_tokens(word, text) |&gt;\n  count(book, word, sort = TRUE)\n\ntotal_word &lt;- word_count |&gt; group_by(book) |&gt; summarise(total = sum(n))\n\nword_count &lt;- word_count |&gt; left_join(total_word, by = \"book\")\n\nhead(word_count)\n\nggplot(word_count, aes(n/total, fill = book)) +\n  geom_histogram(show.legend = FALSE) +\n  xlim(NA, 0.0009) +\n  facet_wrap(~book, ncol = 2, scales = \"free_y\")\n# A tibble: 6 Ã— 4\n  book           word      n  total\n  &lt;fct&gt;          &lt;chr&gt; &lt;int&gt;  &lt;int&gt;\n1 Mansfield Park the    6206 160460\n2 Mansfield Park to     5475 160460\n3 Mansfield Park and    5438 160460\n4 Emma           to     5239 160996\n5 Emma           the    5201 160996\n6 Emma           and    4896 160996"
  }
]